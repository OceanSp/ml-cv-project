{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHnVupBBn9eR"
      },
      "source": [
        "# Detectron2 Beginner's Tutorial\n",
        "\n",
        "<img src=\"https://dl.fbaipublicfiles.com/detectron2/Detectron2-Logo-Horz.png\" width=\"500\">\n",
        "\n",
        "Welcome to detectron2! This is the official colab tutorial of detectron2. Here, we will go through some basics usage of detectron2, including the following:\n",
        "* Run inference on images or videos, with an existing detectron2 model\n",
        "* Train a detectron2 model on a new dataset\n",
        "\n",
        "You can make a copy of this tutorial by \"File -> Open in playground mode\" and make changes there. __DO NOT__ request access to this tutorial.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM54r6jlKTII"
      },
      "source": [
        "# Install detectron2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FsePPpwZSmqt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyyaml==5.1\n",
            "  Downloading PyYAML-5.1.tar.gz (274 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.2/274.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m \u001b[31m[37 lines of output]\u001b[0m\n",
            "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
            "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
            "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
            "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-7ei6ycn8/pyyaml_9cb640e3af2e4c11acd83e972d74aec7/setup.py\", line 291, in <module>\n",
            "  \u001b[31m   \u001b[0m     setup(\n",
            "  \u001b[31m   \u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/core.py\", line 185, in setup\n",
            "  \u001b[31m   \u001b[0m     return run_commands(dist)\n",
            "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^\n",
            "  \u001b[31m   \u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/core.py\", line 201, in run_commands\n",
            "  \u001b[31m   \u001b[0m     dist.run_commands()\n",
            "  \u001b[31m   \u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/dist.py\", line 969, in run_commands\n",
            "  \u001b[31m   \u001b[0m     self.run_command(cmd)\n",
            "  \u001b[31m   \u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/setuptools/dist.py\", line 963, in run_command\n",
            "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
            "  \u001b[31m   \u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
            "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
            "  \u001b[31m   \u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/setuptools/command/egg_info.py\", line 321, in run\n",
            "  \u001b[31m   \u001b[0m     self.find_sources()\n",
            "  \u001b[31m   \u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/setuptools/command/egg_info.py\", line 329, in find_sources\n",
            "  \u001b[31m   \u001b[0m     mm.run()\n",
            "  \u001b[31m   \u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/setuptools/command/egg_info.py\", line 551, in run\n",
            "  \u001b[31m   \u001b[0m     self.add_defaults()\n",
            "  \u001b[31m   \u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/setuptools/command/egg_info.py\", line 589, in add_defaults\n",
            "  \u001b[31m   \u001b[0m     sdist.add_defaults(self)\n",
            "  \u001b[31m   \u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/setuptools/command/sdist.py\", line 112, in add_defaults\n",
            "  \u001b[31m   \u001b[0m     super().add_defaults()\n",
            "  \u001b[31m   \u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/command/sdist.py\", line 251, in add_defaults\n",
            "  \u001b[31m   \u001b[0m     self._add_defaults_ext()\n",
            "  \u001b[31m   \u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/command/sdist.py\", line 336, in _add_defaults_ext\n",
            "  \u001b[31m   \u001b[0m     self.filelist.extend(build_ext.get_source_files())\n",
            "  \u001b[31m   \u001b[0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-7ei6ycn8/pyyaml_9cb640e3af2e4c11acd83e972d74aec7/setup.py\", line 199, in get_source_files\n",
            "  \u001b[31m   \u001b[0m     self.cython_sources(ext.sources, ext)\n",
            "  \u001b[31m   \u001b[0m     ^^^^^^^^^^^^^^^^^^^\n",
            "  \u001b[31m   \u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py\", line 107, in __getattr__\n",
            "  \u001b[31m   \u001b[0m     raise AttributeError(attr)\n",
            "  \u001b[31m   \u001b[0m AttributeError: cython_sources\n",
            "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "\u001b[?25hfatal: destination path 'detectron2' already exists and is not an empty directory.\n",
            "Ignoring dataclasses: markers 'python_version < \"3.7\"' don't match your environment\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.11/dist-packages (9.5.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.3)\n",
            "Collecting pycocotools>=2.0.2\n",
            "  Downloading pycocotools-2.0.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.11/dist-packages (2.4.0)\n",
            "Collecting yacs>=0.1.8\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (2.2.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.11/dist-packages (4.66.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.15.1)\n",
            "Collecting fvcore<0.1.6,>=0.1.5\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting iopath<0.1.10,>=0.1.7\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl.metadata (370 bytes)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.1 in /usr/local/lib/python3.11/dist-packages (2.3.0)\n",
            "Collecting hydra-core>=1.1\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting black\n",
            "  Downloading black-24.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.2/79.2 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (23.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy<2,>=1.20 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.26.3)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: PyYAML in /usr/lib/python3/dist-packages (from yacs>=0.1.8) (5.4.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (2.1.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (2.26.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.5.2)\n",
            "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (4.23.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (69.0.3)\n",
            "Requirement already satisfied: six>1.9 in /usr/lib/python3/dist-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.0.1)\n",
            "Collecting portalocker (from iopath<0.1.10,>=0.1.7)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf<2.4,>=2.1) (4.9.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from black) (8.1.7)\n",
            "Collecting mypy-extensions>=0.4.3 (from black)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pathspec>=0.9.0 (from black)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.11/dist-packages (from black) (4.1.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard) (2020.6.20)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.4)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/lib/python3/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.0)\n",
            "Downloading pycocotools-2.0.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (458 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m458.7/458.7 kB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading black-24.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: fvcore\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61406 sha256=efd67e8cb999446c54e0fe12647f5346fc11cfaa5fe353be2751af2329f50509\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/71/95/3b8fde5c65c6e4a806e0867c1651dcc71a1cb2f3430e8f355f\n",
            "Successfully built fvcore\n",
            "Installing collected packages: yacs, portalocker, pathspec, mypy-extensions, iopath, hydra-core, black, pycocotools, fvcore\n",
            "Successfully installed black-24.10.0 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.0.0 pathspec-0.12.1 portalocker-3.1.1 pycocotools-2.0.8 yacs-0.1.8\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!python -m pip install pyyaml==5.1\n",
        "import sys, os, distutils.core\n",
        "# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities (e.g. compiled operators).\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n",
        "!git clone 'https://github.com/facebookresearch/detectron2'\n",
        "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
        "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
        "sys.path.insert(0, os.path.abspath('./detectron2'))\n",
        "\n",
        "# Properly install detectron2. (Please do not install twice in both ways)\n",
        "# !python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d288Z2mF5dC",
        "outputId": "c47c5426-64d6-4632-f868-e2f14dfe39be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Mon_Oct_24_19:12:58_PDT_2022\n",
            "Cuda compilation tools, release 12.0, V12.0.76\n",
            "Build cuda_12.0.r12.0/compiler.31968024_0\n",
            "torch:  2.1 ; cuda:  cu121\n",
            "detectron2: 0.6\n"
          ]
        }
      ],
      "source": [
        "import torch, detectron2\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "print(\"detectron2:\", detectron2.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZyAvNCJMmvFF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "matplotlib data path: /usr/local/lib/python3.11/dist-packages/matplotlib/mpl-data\n",
            "CONFIGDIR=/root/.config/matplotlib\n",
            "interactive is False\n",
            "platform is linux\n",
            "CACHEDIR=/root/.cache/matplotlib\n",
            "font search path [PosixPath('/usr/local/lib/python3.11/dist-packages/matplotlib/mpl-data/fonts/ttf'), PosixPath('/usr/local/lib/python3.11/dist-packages/matplotlib/mpl-data/fonts/afm'), PosixPath('/usr/local/lib/python3.11/dist-packages/matplotlib/mpl-data/fonts/pdfcorefonts')]\n",
            "generated new fontManager\n"
          ]
        }
      ],
      "source": [
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2bjrfb2LDeo"
      },
      "source": [
        "# Train on a custom dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjbUIhSxUdm_"
      },
      "source": [
        "In this section, we show how to train an existing detectron2 model on a custom dataset in a new format.\n",
        "\n",
        "We use [the balloon segmentation dataset](https://github.com/matterport/Mask_RCNN/tree/master/samples/balloon)\n",
        "which only has one class: balloon.\n",
        "We'll train a balloon segmentation model from an existing model pre-trained on COCO dataset, available in detectron2's model zoo.\n",
        "\n",
        "Note that COCO dataset does not have the \"balloon\" category. We'll be able to recognize this new class in a few minutes.\n",
        "\n",
        "## Prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting kagglehub\n",
            "  Downloading kagglehub-0.3.6-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (23.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->kagglehub) (3.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->kagglehub) (2020.6.20)\n",
            "Downloading kagglehub-0.3.6-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.9/51.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kagglehub\n",
            "Successfully installed kagglehub-0.3.6\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install kagglehub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Qg7zSVOulkb",
        "outputId": "b3c3e5b1-44f0-4402-bd63-076af70ef442"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting new HTTPS connection (1): www.kaggle.com:443\n",
            "https://www.kaggle.com:443 \"GET /api/v1/datasets/view/jaidalmotra/weed-detection HTTP/1.1\" 200 None\n",
            "Starting new HTTPS connection (1): www.kaggle.com:443\n",
            "https://www.kaggle.com:443 \"GET /api/v1/datasets/download/jaidalmotra/weed-detection?dataset_version_number=1 HTTP/1.1\" 302 0\n",
            "Starting new HTTPS connection (1): storage.googleapis.com:443\n",
            "https://storage.googleapis.com:443 \"GET /kaggle-data-sets/3851613/6675836/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20250111%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250111T010134Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=4d2a5542d8e7e120b876fbabe451d6817313d9a646500193aa8869227a1626cbb4d9422329ff8b926e74d16138af1295338819afea86fd0ffe191af44b6592699a3dd5afd3987089f1fb728f47ec9c9d1f605d3a3ce6c2ed275a6aea4a51b2ba03ebf80506adf10da9159b8fb4d641a05ee7ba9bfeaad38a0c1e0d98d7c2d1acdfd2866ea9b1105de98f32ee66693af5a9c94d6d5b651970310bf772f280e4fd15e2f12fc4b7a438cc01e8f3ad4f10ce7282877ada2a01e3b213a45d23e992a27a0e44bf65aa45b2e992b3a3fe8b866c3f1e6ca1ce33fd807a563b6aceae1de5668a63fda1e544a301175bb92fe3b1a1521f86d1f1193f2ebf214272f95c8fff HTTP/1.1\" 200 163743041\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/jaidalmotra/weed-detection?dataset_version_number=1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 156M/156M [00:01<00:00, 131MB/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"jaidalmotra/weed-detection\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "# Define paths for train and test data\n",
        "train_folder = os.path.join(path, 'train')\n",
        "test_folder = os.path.join(path, 'test')\n",
        "annotations_train = os.path.join(train_folder, '_annotations.coco.json')\n",
        "annotations_test = os.path.join(test_folder, '_annotations.coco.json')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVJoOm6LVJwW"
      },
      "source": [
        "Register the balloon dataset to detectron2, following the [detectron2 custom dataset tutorial](https://detectron2.readthedocs.io/tutorials/datasets.html).\n",
        "Here, the dataset is in its custom format, therefore we write a function to parse it and prepare it into detectron2's standard format. User should write such a function when using a dataset in custom format. See the tutorial for more details.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PIbAM2pv-urF"
      },
      "outputs": [],
      "source": [
        "# if your dataset is in COCO format, this cell can be replaced by the following three lines:\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "register_coco_instances(\"weed_train\", {}, annotations_train, train_folder)\n",
        "register_coco_instances(\"weed_val\", {}, annotations_test, test_folder)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ljbWTX0Wi8E"
      },
      "source": [
        "To verify the dataset is in correct format, let's visualize the annotations of randomly selected samples in the training set:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlqXIXXhW8dA"
      },
      "source": [
        "## Train!\n",
        "\n",
        "Now, let's fine-tune a COCO-pretrained R50-FPN Mask R-CNN model on the balloon dataset. It takes ~2 minutes to train 300 iterations on a P100 GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import detectron2\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.data import build_detection_test_loader\n",
        "from detectron2.evaluation import COCOEvaluator\n",
        "\n",
        "class MyTrainer(DefaultTrainer):\n",
        "    @classmethod\n",
        "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
        "        if output_folder is None:\n",
        "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\", dataset_name)\n",
        "        return COCOEvaluator(\n",
        "            dataset_name=dataset_name,\n",
        "            tasks=[\"bbox\"],   # or None to infer automatically\n",
        "            distributed=True,\n",
        "            output_dir=output_folder,\n",
        "            max_dets_per_image=100,\n",
        "            use_fast_impl=True,      # optional, depends on your needs\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7unkuuiqLdqd",
        "outputId": "ba1716cd-3f3b-401d-bae5-8fbbd2199d9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[01/11 01:21:41 d2.engine.defaults]: \u001b[0mModel:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): StandardROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc_relu1): ReLU()\n",
            "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (fc_relu2): ReLU()\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=3, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:21:41 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:21:41 d2.data.datasets.coco]: \u001b[0mLoaded 1661 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/train/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:21:41 d2.data.build]: \u001b[0mRemoved 6 images with no usable annotations. 1655 images left.\n",
            "\u001b[32m[01/11 01:21:41 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "\u001b[32m[01/11 01:21:41 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
            "\u001b[32m[01/11 01:21:41 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:21:41 d2.data.common]: \u001b[0mSerializing 1655 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:21:41 d2.data.common]: \u001b[0mSerialized dataset takes 0.63 MiB\n",
            "\u001b[32m[01/11 01:21:41 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=8\n",
            "\u001b[32m[01/11 01:21:41 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl ...\n",
            "[Checkpointer] Loading from /root/.torch/iopath_cache/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl ...\n",
            "Reading a file from 'Detectron2 Model Zoo'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (3, 1024) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (3,) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (8, 1024) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (8,) in the model! You might want to double check if this is expected.\n",
            "Some model parameters or buffers are not found in the checkpoint:\n",
            "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
            "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[01/11 01:21:41 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:21:50 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:21:50 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:21:50 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:21:50 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:21:50 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:21:50 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:21:50 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:21:50 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:21:51 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0015 s/iter. Inference: 0.0488 s/iter. Eval: 0.0006 s/iter. Total: 0.0508 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:21:56 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0018 s/iter. Inference: 0.0488 s/iter. Eval: 0.0004 s/iter. Total: 0.0510 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:22:01 d2.evaluation.evaluator]: \u001b[0mInference done 209/245. Dataloading: 0.0018 s/iter. Inference: 0.0485 s/iter. Eval: 0.0004 s/iter. Total: 0.0508 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 01:22:03 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.357196 (0.051488 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:22:03 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048556 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:22:03 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:22:03 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:22:03 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.51s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.21s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.001\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.004\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.002\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.001\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.002\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.003\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.005\n",
            "\u001b[32m[01/11 01:22:04 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.102 | 0.350  | 0.000  | 0.000 | 0.000 | 0.158 |\n",
            "\u001b[32m[01/11 01:22:04 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP    |\n",
            "|:------------|:-----|:---------------|:------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 0.102 |\n",
            "\u001b[32m[01/11 01:22:04 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:22:04 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:22:04 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:22:04 d2.evaluation.testing]: \u001b[0mcopypaste: 0.1023,0.3501,0.0000,0.0000,0.0000,0.1577\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:22:12 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:22:12 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:22:12 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:22:12 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:22:12 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:22:12 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:22:12 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:22:12 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:22:13 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0477 s/iter. Eval: 0.0003 s/iter. Total: 0.0492 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:22:18 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0022 s/iter. Inference: 0.0492 s/iter. Eval: 0.0005 s/iter. Total: 0.0520 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:22:23 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0019 s/iter. Inference: 0.0496 s/iter. Eval: 0.0005 s/iter. Total: 0.0520 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:22:25 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.588880 (0.052454 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:22:25 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049450 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:22:25 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:22:25 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:22:25 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.002\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.006\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.003\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.001\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.004\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.006\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.002\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.009\n",
            "\u001b[32m[01/11 01:22:25 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.158 | 0.572  | 0.000  | 0.000 | 0.007 | 0.317 |\n",
            "\u001b[32m[01/11 01:22:25 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP    |\n",
            "|:------------|:-----|:---------------|:------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 0.158 |\n",
            "\u001b[32m[01/11 01:22:25 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:22:25 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:22:25 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:22:25 d2.evaluation.testing]: \u001b[0mcopypaste: 0.1579,0.5723,0.0000,0.0000,0.0074,0.3168\n",
            "\u001b[32m[01/11 01:22:25 d2.utils.events]: \u001b[0m eta: 1:04:02  iter: 19  total_loss: 1.835  loss_cls: 0.8822  loss_box_reg: 0.5944  loss_rpn_cls: 0.3376  loss_rpn_loc: 0.02782    time: 0.7665  last_time: 0.7717  data_time: 0.0570  last_data_time: 0.0287   lr: 4.9953e-06  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:22:34 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:22:34 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:22:34 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:22:34 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:22:34 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:22:34 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:22:34 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:22:34 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:22:35 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0504 s/iter. Eval: 0.0004 s/iter. Total: 0.0518 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:22:40 d2.evaluation.evaluator]: \u001b[0mInference done 112/245. Dataloading: 0.0014 s/iter. Inference: 0.0480 s/iter. Eval: 0.0004 s/iter. Total: 0.0498 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:22:45 d2.evaluation.evaluator]: \u001b[0mInference done 210/245. Dataloading: 0.0016 s/iter. Inference: 0.0484 s/iter. Eval: 0.0004 s/iter. Total: 0.0504 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 01:22:47 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.759656 (0.053165 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:22:47 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048464 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:22:48 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:22:48 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:22:48 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.33s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.001\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.006\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.002\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.001\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.016\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.024\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.007\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.036\n",
            "\u001b[32m[01/11 01:22:48 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.111 | 0.640  | 0.002  | 0.000 | 0.015 | 0.198 |\n",
            "\u001b[32m[01/11 01:22:48 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP    |\n",
            "|:------------|:-----|:---------------|:------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 0.111 |\n",
            "\u001b[32m[01/11 01:22:48 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:22:48 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:22:48 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:22:48 d2.evaluation.testing]: \u001b[0mcopypaste: 0.1106,0.6405,0.0025,0.0000,0.0152,0.1981\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:22:56 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:22:56 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:22:56 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:22:56 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:22:56 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:22:56 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:22:56 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:22:56 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:22:57 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0482 s/iter. Eval: 0.0004 s/iter. Total: 0.0496 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:23:02 d2.evaluation.evaluator]: \u001b[0mInference done 111/245. Dataloading: 0.0014 s/iter. Inference: 0.0485 s/iter. Eval: 0.0004 s/iter. Total: 0.0504 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:23:07 d2.evaluation.evaluator]: \u001b[0mInference done 209/245. Dataloading: 0.0017 s/iter. Inference: 0.0487 s/iter. Eval: 0.0004 s/iter. Total: 0.0509 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 01:23:09 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.351476 (0.051464 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:23:09 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048624 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:23:09 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:23:09 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:23:09 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.59s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.09s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.002\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.012\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.004\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.003\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.026\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.069\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.027\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.099\n",
            "\u001b[32m[01/11 01:23:10 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.211 | 1.178  | 0.008  | 0.000 | 0.049 | 0.422 |\n",
            "\u001b[32m[01/11 01:23:10 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP    |\n",
            "|:------------|:-----|:---------------|:------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 0.211 |\n",
            "\u001b[32m[01/11 01:23:10 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:23:10 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:23:10 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:23:10 d2.evaluation.testing]: \u001b[0mcopypaste: 0.2107,1.1782,0.0081,0.0000,0.0490,0.4224\n",
            "\u001b[32m[01/11 01:23:10 d2.utils.events]: \u001b[0m eta: 1:03:24  iter: 39  total_loss: 1.878  loss_cls: 0.7999  loss_box_reg: 0.6183  loss_rpn_cls: 0.4051  loss_rpn_loc: 0.0304    time: 0.7752  last_time: 0.7610  data_time: 0.0330  last_data_time: 0.0224   lr: 9.9902e-06  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:23:18 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:23:18 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:23:18 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:23:18 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:23:18 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:23:18 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:23:18 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:23:18 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:23:19 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0484 s/iter. Eval: 0.0004 s/iter. Total: 0.0499 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:23:24 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0022 s/iter. Inference: 0.0486 s/iter. Eval: 0.0004 s/iter. Total: 0.0513 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:23:29 d2.evaluation.evaluator]: \u001b[0mInference done 200/245. Dataloading: 0.0018 s/iter. Inference: 0.0486 s/iter. Eval: 0.0027 s/iter. Total: 0.0532 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:23:32 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.822409 (0.053427 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:23:32 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048683 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:23:32 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:23:32 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:23:32 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.94s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.12s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.003\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.018\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.001\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.007\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.036\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.130\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.067\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.175\n",
            "\u001b[32m[01/11 01:23:33 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.315 | 1.797  | 0.013  | 0.000 | 0.101 | 0.718 |\n",
            "\u001b[32m[01/11 01:23:33 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP    |\n",
            "|:------------|:-----|:---------------|:------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 0.315 |\n",
            "\u001b[32m[01/11 01:23:33 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:23:33 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:23:33 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:23:33 d2.evaluation.testing]: \u001b[0mcopypaste: 0.3151,1.7973,0.0130,0.0000,0.1011,0.7181\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:23:41 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:23:41 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:23:41 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:23:41 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:23:41 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:23:41 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:23:41 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:23:41 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:23:42 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0482 s/iter. Eval: 0.0004 s/iter. Total: 0.0496 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:23:47 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0017 s/iter. Inference: 0.0492 s/iter. Eval: 0.0004 s/iter. Total: 0.0515 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:23:52 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0018 s/iter. Inference: 0.0492 s/iter. Eval: 0.0004 s/iter. Total: 0.0515 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:23:54 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.600539 (0.052502 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:23:54 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049579 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:23:54 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:23:55 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:23:55 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.17s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.15s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.005\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.023\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.001\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.012\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.006\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.051\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.182\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.100\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.242\n",
            "\u001b[32m[01/11 01:23:56 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.475 | 2.253  | 0.022  | 0.000 | 0.132 | 1.180 |\n",
            "\u001b[32m[01/11 01:23:56 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP    |\n",
            "|:------------|:-----|:---------------|:------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 0.475 |\n",
            "\u001b[32m[01/11 01:23:56 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:23:56 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:23:56 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:23:56 d2.evaluation.testing]: \u001b[0mcopypaste: 0.4750,2.2532,0.0218,0.0000,0.1325,1.1795\n",
            "\u001b[32m[01/11 01:23:56 d2.utils.events]: \u001b[0m eta: 1:03:10  iter: 59  total_loss: 1.636  loss_cls: 0.6645  loss_box_reg: 0.6329  loss_rpn_cls: 0.1954  loss_rpn_loc: 0.01608    time: 0.7786  last_time: 0.7643  data_time: 0.0325  last_data_time: 0.0210   lr: 1.4985e-05  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:24:04 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:24:04 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:24:04 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:24:04 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:24:04 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:24:04 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:24:04 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:24:04 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:24:06 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0485 s/iter. Eval: 0.0004 s/iter. Total: 0.0499 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:24:11 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0014 s/iter. Inference: 0.0489 s/iter. Eval: 0.0004 s/iter. Total: 0.0508 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:24:16 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0014 s/iter. Inference: 0.0491 s/iter. Eval: 0.0004 s/iter. Total: 0.0509 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 01:24:18 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.484160 (0.052017 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:24:18 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049439 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:24:18 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:24:18 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:24:18 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.15s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.006\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.029\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.002\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.016\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.008\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.066\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.204\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.122\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.264\n",
            "\u001b[32m[01/11 01:24:20 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.604 | 2.855  | 0.031  | 0.000 | 0.166 | 1.606 |\n",
            "\u001b[32m[01/11 01:24:20 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP    |\n",
            "|:------------|:-----|:---------------|:------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 0.604 |\n",
            "\u001b[32m[01/11 01:24:20 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:24:20 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:24:20 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:24:20 d2.evaluation.testing]: \u001b[0mcopypaste: 0.6039,2.8550,0.0311,0.0000,0.1662,1.6058\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:24:27 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:24:27 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:24:27 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:24:27 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:24:27 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:24:27 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:24:27 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:24:27 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:24:28 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0032 s/iter. Inference: 0.0478 s/iter. Eval: 0.0004 s/iter. Total: 0.0514 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:24:33 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0026 s/iter. Inference: 0.0498 s/iter. Eval: 0.0004 s/iter. Total: 0.0529 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:24:38 d2.evaluation.evaluator]: \u001b[0mInference done 199/245. Dataloading: 0.0022 s/iter. Inference: 0.0507 s/iter. Eval: 0.0004 s/iter. Total: 0.0534 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:24:41 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.809853 (0.053374 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:24:41 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050253 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:24:41 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:24:41 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:24:41 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.42s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.20s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.008\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.035\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.002\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.021\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.009\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.079\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.233\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.135\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.305\n",
            "\u001b[32m[01/11 01:24:43 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.775 | 3.478  | 0.043  | 0.000 | 0.186 | 2.113 |\n",
            "\u001b[32m[01/11 01:24:43 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP    |\n",
            "|:------------|:-----|:---------------|:------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 0.775 |\n",
            "\u001b[32m[01/11 01:24:43 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:24:43 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:24:43 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:24:43 d2.evaluation.testing]: \u001b[0mcopypaste: 0.7747,3.4776,0.0431,0.0000,0.1861,2.1128\n",
            "\u001b[32m[01/11 01:24:43 d2.utils.events]: \u001b[0m eta: 1:02:56  iter: 79  total_loss: 1.646  loss_cls: 0.6125  loss_box_reg: 0.7279  loss_rpn_cls: 0.2544  loss_rpn_loc: 0.03194    time: 0.7760  last_time: 0.7645  data_time: 0.0319  last_data_time: 0.0186   lr: 1.998e-05  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:24:51 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:24:51 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:24:51 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:24:51 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:24:51 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:24:51 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:24:51 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:24:51 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:24:53 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0542 s/iter. Eval: 0.0004 s/iter. Total: 0.0555 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:24:58 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0019 s/iter. Inference: 0.0498 s/iter. Eval: 0.0004 s/iter. Total: 0.0521 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:25:03 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0017 s/iter. Inference: 0.0504 s/iter. Eval: 0.0004 s/iter. Total: 0.0525 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:25:05 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.710382 (0.052960 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:25:05 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050190 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:25:05 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:25:05 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:25:05 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.17s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.010\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.044\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.002\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.025\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.091\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.237\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.152\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.303\n",
            "\u001b[32m[01/11 01:25:07 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.958 | 4.443  | 0.044  | 0.000 | 0.230 | 2.494 |\n",
            "\u001b[32m[01/11 01:25:07 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP    |\n",
            "|:------------|:-----|:---------------|:------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 0.958 |\n",
            "\u001b[32m[01/11 01:25:07 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:25:07 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:25:07 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:25:07 d2.evaluation.testing]: \u001b[0mcopypaste: 0.9578,4.4427,0.0445,0.0000,0.2301,2.4944\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:25:14 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:25:14 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:25:14 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:25:14 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:25:14 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:25:14 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:25:14 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:25:14 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:25:15 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0043 s/iter. Inference: 0.0493 s/iter. Eval: 0.0004 s/iter. Total: 0.0541 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:25:21 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0026 s/iter. Inference: 0.0495 s/iter. Eval: 0.0004 s/iter. Total: 0.0526 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:25:26 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0024 s/iter. Inference: 0.0490 s/iter. Eval: 0.0004 s/iter. Total: 0.0518 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:25:28 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.594288 (0.052476 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:25:28 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049091 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:25:28 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:25:28 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:25:28 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.40s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.16s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.012\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.052\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.001\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.003\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.030\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.013\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.107\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.258\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.326\n",
            "\u001b[32m[01/11 01:25:30 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 1.204 | 5.226  | 0.091  | 0.000 | 0.277 | 3.002 |\n",
            "\u001b[32m[01/11 01:25:30 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP    |\n",
            "|:------------|:-----|:---------------|:------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 1.204 |\n",
            "\u001b[32m[01/11 01:25:30 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:25:30 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:25:30 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:25:30 d2.evaluation.testing]: \u001b[0mcopypaste: 1.2041,5.2265,0.0912,0.0000,0.2771,3.0018\n",
            "\u001b[32m[01/11 01:25:30 d2.utils.events]: \u001b[0m eta: 1:02:40  iter: 99  total_loss: 1.468  loss_cls: 0.5591  loss_box_reg: 0.7045  loss_rpn_cls: 0.1321  loss_rpn_loc: 0.01591    time: 0.7755  last_time: 0.7943  data_time: 0.0349  last_data_time: 0.0370   lr: 2.4975e-05  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:25:38 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:25:38 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:25:38 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:25:38 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:25:38 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:25:38 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:25:38 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:25:38 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:25:39 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0495 s/iter. Eval: 0.0004 s/iter. Total: 0.0510 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:25:44 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0015 s/iter. Inference: 0.0493 s/iter. Eval: 0.0004 s/iter. Total: 0.0512 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:25:49 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0014 s/iter. Inference: 0.0495 s/iter. Eval: 0.0004 s/iter. Total: 0.0514 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:25:52 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.511168 (0.052130 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:25:52 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049521 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:25:52 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:25:52 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:25:52 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.17s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.015\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.066\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.001\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.004\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.035\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.021\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.123\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.280\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.190\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.350\n",
            "\u001b[32m[01/11 01:25:53 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 1.506 | 6.591  | 0.131  | 0.000 | 0.361 | 3.459 |\n",
            "\u001b[32m[01/11 01:25:53 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP    |\n",
            "|:------------|:-----|:---------------|:------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 1.506 |\n",
            "\u001b[32m[01/11 01:25:53 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:25:53 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:25:53 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:25:53 d2.evaluation.testing]: \u001b[0mcopypaste: 1.5061,6.5912,0.1307,0.0000,0.3612,3.4593\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:26:01 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:26:01 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:26:01 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:26:01 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:26:01 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:26:01 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:26:01 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:26:01 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:26:02 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0484 s/iter. Eval: 0.0004 s/iter. Total: 0.0497 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:26:07 d2.evaluation.evaluator]: \u001b[0mInference done 105/245. Dataloading: 0.0018 s/iter. Inference: 0.0508 s/iter. Eval: 0.0004 s/iter. Total: 0.0531 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:26:12 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0018 s/iter. Inference: 0.0500 s/iter. Eval: 0.0004 s/iter. Total: 0.0523 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:26:15 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:13.174609 (0.054894 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:26:15 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049940 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:26:15 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:26:15 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:26:15 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.15s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.019\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.084\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.001\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.004\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.041\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.023\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.137\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.278\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.194\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.344\n",
            "\u001b[32m[01/11 01:26:17 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 1.934 | 8.362  | 0.127  | 0.000 | 0.433 | 4.093 |\n",
            "\u001b[32m[01/11 01:26:17 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP    |\n",
            "|:------------|:-----|:---------------|:------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 1.934 |\n",
            "\u001b[32m[01/11 01:26:17 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:26:17 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:26:17 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:26:17 d2.evaluation.testing]: \u001b[0mcopypaste: 1.9337,8.3616,0.1270,0.0000,0.4327,4.0929\n",
            "\u001b[32m[01/11 01:26:17 d2.utils.events]: \u001b[0m eta: 1:02:26  iter: 119  total_loss: 1.475  loss_cls: 0.5246  loss_box_reg: 0.7461  loss_rpn_cls: 0.127  loss_rpn_loc: 0.02243    time: 0.7758  last_time: 0.7679  data_time: 0.0334  last_data_time: 0.0245   lr: 2.997e-05  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:26:25 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:26:25 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:26:25 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:26:25 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:26:25 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:26:25 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:26:25 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:26:25 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:26:26 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0037 s/iter. Inference: 0.0475 s/iter. Eval: 0.0004 s/iter. Total: 0.0516 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:26:31 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0016 s/iter. Inference: 0.0488 s/iter. Eval: 0.0004 s/iter. Total: 0.0509 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:26:36 d2.evaluation.evaluator]: \u001b[0mInference done 209/245. Dataloading: 0.0016 s/iter. Inference: 0.0488 s/iter. Eval: 0.0004 s/iter. Total: 0.0509 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 01:26:38 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.369955 (0.051541 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:26:38 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048719 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:26:38 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:26:38 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:26:38 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.29s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.16s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.023\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.108\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.002\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.006\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.044\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.029\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.145\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.285\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.205\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.350\n",
            "\u001b[32m[01/11 01:26:40 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 2.335 | 10.813 | 0.162  | 0.000 | 0.622 | 4.408 |\n",
            "\u001b[32m[01/11 01:26:40 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP    |\n",
            "|:------------|:-----|:---------------|:------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 2.335 |\n",
            "\u001b[32m[01/11 01:26:40 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:26:40 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:26:40 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:26:40 d2.evaluation.testing]: \u001b[0mcopypaste: 2.3348,10.8135,0.1622,0.0000,0.6216,4.4076\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:26:48 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:26:48 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:26:48 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:26:48 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:26:48 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:26:48 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:26:48 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:26:48 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:26:49 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0486 s/iter. Eval: 0.0004 s/iter. Total: 0.0500 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:26:54 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0015 s/iter. Inference: 0.0500 s/iter. Eval: 0.0004 s/iter. Total: 0.0520 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:26:59 d2.evaluation.evaluator]: \u001b[0mInference done 197/245. Dataloading: 0.0016 s/iter. Inference: 0.0493 s/iter. Eval: 0.0027 s/iter. Total: 0.0538 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:27:01 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.949056 (0.053954 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:27:01 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049347 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:27:01 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:27:01 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:27:01 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.15s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.028\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.124\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.002\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.008\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.051\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.031\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.154\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.305\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.217\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.375\n",
            "\u001b[32m[01/11 01:27:03 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 2.817 | 12.386 | 0.213  | 0.000 | 0.767 | 5.093 |\n",
            "\u001b[32m[01/11 01:27:03 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP    |\n",
            "|:------------|:-----|:---------------|:------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 2.817 |\n",
            "\u001b[32m[01/11 01:27:03 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:27:03 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:27:03 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:27:03 d2.evaluation.testing]: \u001b[0mcopypaste: 2.8172,12.3857,0.2135,0.0000,0.7669,5.0930\n",
            "\u001b[32m[01/11 01:27:03 d2.utils.events]: \u001b[0m eta: 1:02:10  iter: 139  total_loss: 1.42  loss_cls: 0.4968  loss_box_reg: 0.7813  loss_rpn_cls: 0.1181  loss_rpn_loc: 0.02672    time: 0.7759  last_time: 0.7790  data_time: 0.0321  last_data_time: 0.0275   lr: 3.4965e-05  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:27:11 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:27:11 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:27:11 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:27:11 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:27:11 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:27:11 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:27:11 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:27:11 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:27:12 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0481 s/iter. Eval: 0.0004 s/iter. Total: 0.0496 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:27:17 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0016 s/iter. Inference: 0.0492 s/iter. Eval: 0.0004 s/iter. Total: 0.0512 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:27:23 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0020 s/iter. Inference: 0.0490 s/iter. Eval: 0.0004 s/iter. Total: 0.0515 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:27:25 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.490532 (0.052044 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:27:25 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048956 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:27:25 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:27:25 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:27:25 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.04s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.15s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.032\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.144\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.002\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.010\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.054\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.035\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.158\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.303\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.221\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.370\n",
            "\u001b[32m[01/11 01:27:26 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 3.228 | 14.367 | 0.220  | 0.000 | 0.975 | 5.425 |\n",
            "\u001b[32m[01/11 01:27:26 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP    |\n",
            "|:------------|:-----|:---------------|:------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 3.228 |\n",
            "\u001b[32m[01/11 01:27:26 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:27:26 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:27:26 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:27:26 d2.evaluation.testing]: \u001b[0mcopypaste: 3.2283,14.3666,0.2197,0.0000,0.9750,5.4252\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:27:34 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:27:34 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:27:34 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:27:34 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:27:34 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:27:34 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:27:34 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:27:34 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:27:35 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0026 s/iter. Inference: 0.0529 s/iter. Eval: 0.0007 s/iter. Total: 0.0562 s/iter. ETA=0:00:13\n",
            "\u001b[32m[01/11 01:27:40 d2.evaluation.evaluator]: \u001b[0mInference done 101/245. Dataloading: 0.0018 s/iter. Inference: 0.0489 s/iter. Eval: 0.0051 s/iter. Total: 0.0559 s/iter. ETA=0:00:08\n",
            "\u001b[32m[01/11 01:27:45 d2.evaluation.evaluator]: \u001b[0mInference done 198/245. Dataloading: 0.0020 s/iter. Inference: 0.0491 s/iter. Eval: 0.0027 s/iter. Total: 0.0539 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:27:48 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.908887 (0.053787 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:27:48 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048839 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:27:48 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:27:48 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:27:48 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.29s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.16s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.038\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.161\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.003\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.014\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.060\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.036\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.179\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.315\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.029\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.245\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.373\n",
            "\u001b[32m[01/11 01:27:50 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 3.832 | 16.127 | 0.253  | 0.001 | 1.358 | 5.953 |\n",
            "\u001b[32m[01/11 01:27:50 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP    |\n",
            "|:------------|:-----|:---------------|:------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 3.832 |\n",
            "\u001b[32m[01/11 01:27:50 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:27:50 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:27:50 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:27:50 d2.evaluation.testing]: \u001b[0mcopypaste: 3.8324,16.1269,0.2529,0.0006,1.3576,5.9529\n",
            "\u001b[32m[01/11 01:27:50 d2.utils.events]: \u001b[0m eta: 1:01:55  iter: 159  total_loss: 1.384  loss_cls: 0.4434  loss_box_reg: 0.7089  loss_rpn_cls: 0.1628  loss_rpn_loc: 0.03564    time: 0.7760  last_time: 0.7811  data_time: 0.0370  last_data_time: 0.0337   lr: 3.996e-05  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:27:58 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:27:58 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:27:58 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:27:58 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:27:58 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:27:58 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:27:58 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:27:58 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:27:59 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0020 s/iter. Inference: 0.0500 s/iter. Eval: 0.0004 s/iter. Total: 0.0524 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:28:04 d2.evaluation.evaluator]: \u001b[0mInference done 111/245. Dataloading: 0.0018 s/iter. Inference: 0.0482 s/iter. Eval: 0.0004 s/iter. Total: 0.0506 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:28:09 d2.evaluation.evaluator]: \u001b[0mInference done 211/245. Dataloading: 0.0017 s/iter. Inference: 0.0483 s/iter. Eval: 0.0004 s/iter. Total: 0.0505 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 01:28:11 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.319293 (0.051330 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:28:11 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048354 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:28:11 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:28:11 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:28:11 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.43s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.16s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.046\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.196\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.002\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.018\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.068\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.040\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.193\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.321\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.255\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.379\n",
            "\u001b[32m[01/11 01:28:13 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 4.629 | 19.582 | 0.230  | 0.000 | 1.842 | 6.814 |\n",
            "\u001b[32m[01/11 01:28:13 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP    |\n",
            "|:------------|:-----|:---------------|:------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 4.629 |\n",
            "\u001b[32m[01/11 01:28:13 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:28:13 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:28:13 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:28:13 d2.evaluation.testing]: \u001b[0mcopypaste: 4.6289,19.5824,0.2299,0.0003,1.8421,6.8143\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:28:21 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:28:21 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:28:21 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:28:21 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:28:21 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:28:21 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:28:21 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:28:21 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:28:22 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0007 s/iter. Inference: 0.0521 s/iter. Eval: 0.0004 s/iter. Total: 0.0531 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:28:27 d2.evaluation.evaluator]: \u001b[0mInference done 102/245. Dataloading: 0.0016 s/iter. Inference: 0.0529 s/iter. Eval: 0.0004 s/iter. Total: 0.0550 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:28:32 d2.evaluation.evaluator]: \u001b[0mInference done 197/245. Dataloading: 0.0017 s/iter. Inference: 0.0518 s/iter. Eval: 0.0004 s/iter. Total: 0.0541 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:28:35 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:13.044727 (0.054353 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:28:35 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.051443 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:28:35 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:28:35 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:28:35 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.16s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.054\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.225\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.003\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.023\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.076\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.048\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.202\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.326\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.255\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.386\n",
            "\u001b[32m[01/11 01:28:37 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 5.376 | 22.510 | 0.267  | 0.000 | 2.264 | 7.612 |\n",
            "\u001b[32m[01/11 01:28:37 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP    |\n",
            "|:------------|:-----|:---------------|:------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 5.376 |\n",
            "\u001b[32m[01/11 01:28:37 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:28:37 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:28:37 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:28:37 d2.evaluation.testing]: \u001b[0mcopypaste: 5.3756,22.5098,0.2673,0.0003,2.2645,7.6125\n",
            "\u001b[32m[01/11 01:28:37 d2.utils.events]: \u001b[0m eta: 1:01:41  iter: 179  total_loss: 1.304  loss_cls: 0.4235  loss_box_reg: 0.7617  loss_rpn_cls: 0.03804  loss_rpn_loc: 0.01721    time: 0.7763  last_time: 0.7642  data_time: 0.0357  last_data_time: 0.0238   lr: 4.4955e-05  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:28:45 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:28:45 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:28:45 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:28:45 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:28:45 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:28:45 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:28:45 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:28:45 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:28:46 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0472 s/iter. Eval: 0.0004 s/iter. Total: 0.0489 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:28:51 d2.evaluation.evaluator]: \u001b[0mInference done 111/245. Dataloading: 0.0014 s/iter. Inference: 0.0481 s/iter. Eval: 0.0004 s/iter. Total: 0.0499 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:28:56 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0014 s/iter. Inference: 0.0496 s/iter. Eval: 0.0004 s/iter. Total: 0.0515 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:28:58 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.450594 (0.051877 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:28:58 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049282 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:28:58 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:28:58 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:28:58 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.44s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.29s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.16s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.062\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.254\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.003\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.028\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.086\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.055\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.212\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.331\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.264\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.389\n",
            "\u001b[32m[01/11 01:29:00 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 6.219 | 25.403 | 0.328  | 0.000 | 2.772 | 8.589 |\n",
            "\u001b[32m[01/11 01:29:00 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP    |\n",
            "|:------------|:-----|:---------------|:------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 6.219 |\n",
            "\u001b[32m[01/11 01:29:00 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:29:00 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:29:00 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:29:00 d2.evaluation.testing]: \u001b[0mcopypaste: 6.2187,25.4035,0.3279,0.0004,2.7724,8.5890\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:29:08 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:29:08 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:29:08 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:29:08 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:29:08 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:29:08 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:29:08 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:29:08 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:29:09 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0015 s/iter. Inference: 0.0505 s/iter. Eval: 0.0004 s/iter. Total: 0.0523 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:29:14 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0019 s/iter. Inference: 0.0500 s/iter. Eval: 0.0004 s/iter. Total: 0.0524 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:29:19 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0019 s/iter. Inference: 0.0490 s/iter. Eval: 0.0004 s/iter. Total: 0.0514 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 01:29:21 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.491112 (0.052046 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:29:21 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048952 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:29:21 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:29:21 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:29:22 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.15s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.071\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.293\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.004\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.036\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.096\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.057\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.226\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.342\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.275\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.401\n",
            "\u001b[32m[01/11 01:29:23 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 7.136 | 29.304 | 0.357  | 0.000 | 3.618 | 9.608 |\n",
            "\u001b[32m[01/11 01:29:23 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP    |\n",
            "|:------------|:-----|:---------------|:------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 7.136 |\n",
            "\u001b[32m[01/11 01:29:23 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:29:23 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:29:23 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:29:23 d2.evaluation.testing]: \u001b[0mcopypaste: 7.1356,29.3038,0.3571,0.0004,3.6183,9.6077\n",
            "\u001b[32m[01/11 01:29:23 d2.utils.events]: \u001b[0m eta: 1:01:27  iter: 199  total_loss: 1.353  loss_cls: 0.3996  loss_box_reg: 0.7645  loss_rpn_cls: 0.1054  loss_rpn_loc: 0.04147    time: 0.7768  last_time: 0.7706  data_time: 0.0339  last_data_time: 0.0248   lr: 4.995e-05  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:29:31 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:29:31 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:29:31 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:29:31 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:29:31 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:29:31 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:29:31 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:29:31 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:29:32 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0448 s/iter. Eval: 0.0004 s/iter. Total: 0.0462 s/iter. ETA=0:00:10\n",
            "\u001b[32m[01/11 01:29:37 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0017 s/iter. Inference: 0.0489 s/iter. Eval: 0.0004 s/iter. Total: 0.0511 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:29:42 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0015 s/iter. Inference: 0.0488 s/iter. Eval: 0.0004 s/iter. Total: 0.0508 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 01:29:44 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.459670 (0.051915 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:29:44 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049222 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:29:45 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:29:45 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:29:45 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.47s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.16s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.081\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.318\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.005\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.043\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.108\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.059\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.237\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.349\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.293\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.402\n",
            "\u001b[32m[01/11 01:29:47 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:------:|\n",
            "| 8.075 | 31.765 | 0.474  | 0.000 | 4.254 | 10.825 |\n",
            "\u001b[32m[01/11 01:29:47 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP    |\n",
            "|:------------|:-----|:---------------|:------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 8.075 |\n",
            "\u001b[32m[01/11 01:29:47 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:29:47 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:29:47 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:29:47 d2.evaluation.testing]: \u001b[0mcopypaste: 8.0750,31.7653,0.4736,0.0004,4.2537,10.8252\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:29:54 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:29:54 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:29:54 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:29:54 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:29:54 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:29:54 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:29:54 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:29:54 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:29:55 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0036 s/iter. Inference: 0.0489 s/iter. Eval: 0.0004 s/iter. Total: 0.0529 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:30:00 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0018 s/iter. Inference: 0.0495 s/iter. Eval: 0.0004 s/iter. Total: 0.0518 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:30:05 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0017 s/iter. Inference: 0.0493 s/iter. Eval: 0.0004 s/iter. Total: 0.0514 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:30:08 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.506117 (0.052109 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:30:08 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049287 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:30:08 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:30:08 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:30:08 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.15s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.083\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.332\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.005\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.047\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.110\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.062\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.240\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.351\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.287\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.408\n",
            "\u001b[32m[01/11 01:30:09 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:------:|\n",
            "| 8.346 | 33.152 | 0.487  | 0.000 | 4.698 | 11.032 |\n",
            "\u001b[32m[01/11 01:30:09 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP    |\n",
            "|:------------|:-----|:---------------|:------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 8.346 |\n",
            "\u001b[32m[01/11 01:30:09 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:30:09 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:30:09 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:30:09 d2.evaluation.testing]: \u001b[0mcopypaste: 8.3464,33.1519,0.4867,0.0004,4.6983,11.0324\n",
            "\u001b[32m[01/11 01:30:09 d2.utils.events]: \u001b[0m eta: 1:01:12  iter: 219  total_loss: 1.184  loss_cls: 0.3553  loss_box_reg: 0.7608  loss_rpn_cls: 0.03154  loss_rpn_loc: 0.01847    time: 0.7764  last_time: 0.7670  data_time: 0.0345  last_data_time: 0.0245   lr: 5.4945e-05  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:30:18 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:30:18 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:30:18 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:30:18 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:30:18 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:30:18 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:30:18 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:30:18 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:30:19 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0528 s/iter. Eval: 0.0004 s/iter. Total: 0.0542 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:30:24 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0015 s/iter. Inference: 0.0494 s/iter. Eval: 0.0004 s/iter. Total: 0.0513 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:30:29 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0017 s/iter. Inference: 0.0491 s/iter. Eval: 0.0004 s/iter. Total: 0.0513 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 01:30:31 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.527519 (0.052198 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:30:31 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049316 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:30:31 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:30:31 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:30:31 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.45s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.16s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.089\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.351\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.005\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.056\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.116\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.061\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.242\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.351\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.288\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.408\n",
            "\u001b[32m[01/11 01:30:33 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:------:|\n",
            "| 8.918 | 35.058 | 0.505  | 0.000 | 5.615 | 11.614 |\n",
            "\u001b[32m[01/11 01:30:33 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP    |\n",
            "|:------------|:-----|:---------------|:------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 8.918 |\n",
            "\u001b[32m[01/11 01:30:33 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:30:33 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:30:33 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:30:33 d2.evaluation.testing]: \u001b[0mcopypaste: 8.9183,35.0583,0.5049,0.0005,5.6153,11.6141\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:30:41 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:30:41 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:30:41 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:30:41 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:30:41 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:30:41 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:30:41 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:30:41 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:30:42 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0504 s/iter. Eval: 0.0006 s/iter. Total: 0.0520 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:30:47 d2.evaluation.evaluator]: \u001b[0mInference done 105/245. Dataloading: 0.0021 s/iter. Inference: 0.0505 s/iter. Eval: 0.0004 s/iter. Total: 0.0531 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:30:52 d2.evaluation.evaluator]: \u001b[0mInference done 201/245. Dataloading: 0.0018 s/iter. Inference: 0.0505 s/iter. Eval: 0.0004 s/iter. Total: 0.0528 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:30:54 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.809339 (0.053372 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:30:54 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050474 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:30:54 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:30:54 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:30:55 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.16s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.101\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.383\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.007\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.064\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.131\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.070\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.255\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.358\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.304\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.410\n",
            "\u001b[32m[01/11 01:30:56 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
            "| 10.055 | 38.346 | 0.653  | 0.000 | 6.409 | 13.095 |\n",
            "\u001b[32m[01/11 01:30:56 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 10.055 |\n",
            "\u001b[32m[01/11 01:30:56 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:30:56 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:30:56 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:30:56 d2.evaluation.testing]: \u001b[0mcopypaste: 10.0554,38.3459,0.6528,0.0005,6.4093,13.0948\n",
            "\u001b[32m[01/11 01:30:56 d2.utils.events]: \u001b[0m eta: 1:01:00  iter: 239  total_loss: 1.185  loss_cls: 0.3354  loss_box_reg: 0.7864  loss_rpn_cls: 0.02419  loss_rpn_loc: 0.01555    time: 0.7770  last_time: 0.6566  data_time: 0.0370  last_data_time: 0.0271   lr: 5.994e-05  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:31:04 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:31:04 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:31:04 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:31:04 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:31:04 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:31:04 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:31:04 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:31:04 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:31:05 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0008 s/iter. Inference: 0.0495 s/iter. Eval: 0.0004 s/iter. Total: 0.0508 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:31:10 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0018 s/iter. Inference: 0.0498 s/iter. Eval: 0.0004 s/iter. Total: 0.0521 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:31:15 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0016 s/iter. Inference: 0.0494 s/iter. Eval: 0.0004 s/iter. Total: 0.0515 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:31:17 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.431100 (0.051796 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:31:17 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049045 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:31:18 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:31:18 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:31:18 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.47s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.16s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.108\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.415\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.008\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.066\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.142\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.074\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.261\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.365\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.321\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.411\n",
            "\u001b[32m[01/11 01:31:20 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
            "| 10.835 | 41.491 | 0.784  | 0.000 | 6.557 | 14.205 |\n",
            "\u001b[32m[01/11 01:31:20 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 10.835 |\n",
            "\u001b[32m[01/11 01:31:20 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:31:20 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:31:20 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:31:20 d2.evaluation.testing]: \u001b[0mcopypaste: 10.8353,41.4909,0.7844,0.0005,6.5574,14.2051\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:31:27 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:31:27 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:31:27 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:31:27 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:31:27 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:31:27 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:31:27 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:31:27 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:31:28 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0024 s/iter. Inference: 0.0507 s/iter. Eval: 0.0004 s/iter. Total: 0.0535 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:31:33 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0028 s/iter. Inference: 0.0484 s/iter. Eval: 0.0004 s/iter. Total: 0.0517 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:31:38 d2.evaluation.evaluator]: \u001b[0mInference done 202/245. Dataloading: 0.0024 s/iter. Inference: 0.0497 s/iter. Eval: 0.0004 s/iter. Total: 0.0525 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:31:41 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.653933 (0.052725 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:31:41 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049292 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:31:41 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:31:41 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:31:41 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.15s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.120\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.438\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.010\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.078\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.155\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.081\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.267\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.373\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.339\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.412\n",
            "\u001b[32m[01/11 01:31:42 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
            "| 11.995 | 43.770 | 0.990  | 0.000 | 7.766 | 15.507 |\n",
            "\u001b[32m[01/11 01:31:42 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 11.995 |\n",
            "\u001b[32m[01/11 01:31:42 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:31:42 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:31:42 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:31:42 d2.evaluation.testing]: \u001b[0mcopypaste: 11.9952,43.7701,0.9905,0.0005,7.7662,15.5066\n",
            "\u001b[32m[01/11 01:31:42 d2.utils.events]: \u001b[0m eta: 1:00:45  iter: 259  total_loss: 1.13  loss_cls: 0.2987  loss_box_reg: 0.7638  loss_rpn_cls: 0.02777  loss_rpn_loc: 0.01821    time: 0.7767  last_time: 0.7588  data_time: 0.0311  last_data_time: 0.0151   lr: 6.4935e-05  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:31:51 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:31:51 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:31:51 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:31:51 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:31:51 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:31:51 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:31:51 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:31:51 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:31:52 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0479 s/iter. Eval: 0.0004 s/iter. Total: 0.0494 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:31:57 d2.evaluation.evaluator]: \u001b[0mInference done 111/245. Dataloading: 0.0013 s/iter. Inference: 0.0485 s/iter. Eval: 0.0004 s/iter. Total: 0.0503 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:32:02 d2.evaluation.evaluator]: \u001b[0mInference done 210/245. Dataloading: 0.0014 s/iter. Inference: 0.0486 s/iter. Eval: 0.0004 s/iter. Total: 0.0505 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 01:32:04 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.285706 (0.051190 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:32:04 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048532 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:32:04 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:32:04 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:32:04 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.45s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.20s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.15s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.133\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.468\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.011\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.091\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.086\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.279\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.374\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.339\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.415\n",
            "\u001b[32m[01/11 01:32:06 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
            "| 13.254 | 46.808 | 1.112  | 0.001 | 9.078 | 16.837 |\n",
            "\u001b[32m[01/11 01:32:06 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 13.254 |\n",
            "\u001b[32m[01/11 01:32:06 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:32:06 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:32:06 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:32:06 d2.evaluation.testing]: \u001b[0mcopypaste: 13.2540,46.8083,1.1124,0.0006,9.0778,16.8372\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:32:13 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:32:13 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:32:13 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:32:13 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:32:13 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:32:13 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:32:13 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:32:13 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:32:15 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0007 s/iter. Inference: 0.0485 s/iter. Eval: 0.0004 s/iter. Total: 0.0496 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:32:20 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0017 s/iter. Inference: 0.0496 s/iter. Eval: 0.0004 s/iter. Total: 0.0518 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:32:25 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0016 s/iter. Inference: 0.0496 s/iter. Eval: 0.0004 s/iter. Total: 0.0517 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:32:27 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.562958 (0.052346 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:32:27 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049368 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:32:27 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:32:27 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:32:27 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.04s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.20s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.15s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.143\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.492\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.013\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.094\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.182\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.088\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.286\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.381\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.033\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.350\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.418\n",
            "\u001b[32m[01/11 01:32:29 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
            "| 14.300 | 49.152 | 1.308  | 0.001 | 9.406 | 18.227 |\n",
            "\u001b[32m[01/11 01:32:29 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 14.300 |\n",
            "\u001b[32m[01/11 01:32:29 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:32:29 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:32:29 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:32:29 d2.evaluation.testing]: \u001b[0mcopypaste: 14.2997,49.1522,1.3082,0.0014,9.4059,18.2268\n",
            "\u001b[32m[01/11 01:32:29 d2.utils.events]: \u001b[0m eta: 1:00:28  iter: 279  total_loss: 1.08  loss_cls: 0.2545  loss_box_reg: 0.6733  loss_rpn_cls: 0.06607  loss_rpn_loc: 0.03787    time: 0.7761  last_time: 0.6993  data_time: 0.0347  last_data_time: 0.0271   lr: 6.993e-05  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:32:37 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:32:37 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:32:37 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:32:37 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:32:37 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:32:37 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:32:37 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:32:37 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:32:38 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0013 s/iter. Inference: 0.0539 s/iter. Eval: 0.0004 s/iter. Total: 0.0555 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:32:43 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0015 s/iter. Inference: 0.0503 s/iter. Eval: 0.0004 s/iter. Total: 0.0524 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:32:48 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0018 s/iter. Inference: 0.0499 s/iter. Eval: 0.0004 s/iter. Total: 0.0522 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:32:50 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.701076 (0.052921 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:32:50 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049974 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:32:51 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:32:51 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:32:51 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.66s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.23s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.151\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.516\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.023\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.095\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.192\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.092\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.289\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.378\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.038\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.351\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.413\n",
            "\u001b[32m[01/11 01:32:53 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
            "| 15.052 | 51.636 | 2.286  | 0.002 | 9.512 | 19.186 |\n",
            "\u001b[32m[01/11 01:32:53 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 15.052 |\n",
            "\u001b[32m[01/11 01:32:53 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:32:53 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:32:53 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:32:53 d2.evaluation.testing]: \u001b[0mcopypaste: 15.0518,51.6364,2.2858,0.0017,9.5116,19.1863\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:33:01 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:33:01 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:33:01 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:33:01 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:33:01 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:33:01 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:33:01 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:33:01 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:33:02 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0497 s/iter. Eval: 0.0004 s/iter. Total: 0.0512 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:33:07 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0016 s/iter. Inference: 0.0491 s/iter. Eval: 0.0004 s/iter. Total: 0.0512 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:33:12 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0017 s/iter. Inference: 0.0490 s/iter. Eval: 0.0004 s/iter. Total: 0.0511 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 01:33:14 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.444466 (0.051852 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:33:14 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049011 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:33:14 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:33:14 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:33:14 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.04s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.12s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.15s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.154\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.532\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.025\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.096\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.197\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.091\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.295\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.381\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.038\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.348\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.419\n",
            "\u001b[32m[01/11 01:33:15 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
            "| 15.411 | 53.162 | 2.451  | 0.002 | 9.575 | 19.684 |\n",
            "\u001b[32m[01/11 01:33:15 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 15.411 |\n",
            "\u001b[32m[01/11 01:33:15 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:33:15 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:33:15 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:33:15 d2.evaluation.testing]: \u001b[0mcopypaste: 15.4110,53.1616,2.4506,0.0017,9.5752,19.6843\n",
            "\u001b[32m[01/11 01:33:15 d2.utils.events]: \u001b[0m eta: 1:00:14  iter: 299  total_loss: 0.9679  loss_cls: 0.2261  loss_box_reg: 0.6687  loss_rpn_cls: 0.04119  loss_rpn_loc: 0.02195    time: 0.7765  last_time: 0.7690  data_time: 0.0351  last_data_time: 0.0270   lr: 7.4925e-05  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:33:24 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:33:24 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:33:24 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:33:24 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:33:24 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:33:24 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:33:24 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:33:24 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:33:25 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0463 s/iter. Eval: 0.0003 s/iter. Total: 0.0476 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:33:30 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0017 s/iter. Inference: 0.0488 s/iter. Eval: 0.0004 s/iter. Total: 0.0510 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:33:35 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0017 s/iter. Inference: 0.0491 s/iter. Eval: 0.0004 s/iter. Total: 0.0512 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 01:33:37 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.440687 (0.051836 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:33:37 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049037 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:33:37 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:33:37 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:33:37 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.08s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.14s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.162\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.549\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.030\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.101\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.206\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.092\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.308\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.389\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.038\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.366\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.422\n",
            "\u001b[32m[01/11 01:33:38 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 16.215 | 54.903 | 2.980  | 0.002 | 10.148 | 20.602 |\n",
            "\u001b[32m[01/11 01:33:38 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 16.215 |\n",
            "\u001b[32m[01/11 01:33:38 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:33:38 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:33:38 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:33:38 d2.evaluation.testing]: \u001b[0mcopypaste: 16.2153,54.9034,2.9795,0.0019,10.1483,20.6016\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:33:46 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:33:46 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:33:46 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:33:46 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:33:46 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:33:46 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:33:46 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:33:46 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:33:47 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0478 s/iter. Eval: 0.0004 s/iter. Total: 0.0492 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:33:52 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0023 s/iter. Inference: 0.0485 s/iter. Eval: 0.0004 s/iter. Total: 0.0513 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:33:57 d2.evaluation.evaluator]: \u001b[0mInference done 199/245. Dataloading: 0.0024 s/iter. Inference: 0.0484 s/iter. Eval: 0.0026 s/iter. Total: 0.0534 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:33:59 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.849069 (0.053538 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:33:59 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048325 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:33:59 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:33:59 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:33:59 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.15s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.13s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.171\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.564\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.034\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.112\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.215\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.097\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.314\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.396\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.376\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.429\n",
            "\u001b[32m[01/11 01:34:01 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 17.137 | 56.376 | 3.382  | 0.000 | 11.184 | 21.550 |\n",
            "\u001b[32m[01/11 01:34:01 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 17.137 |\n",
            "\u001b[32m[01/11 01:34:01 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:34:01 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:34:01 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:34:01 d2.evaluation.testing]: \u001b[0mcopypaste: 17.1373,56.3759,3.3824,0.0003,11.1837,21.5498\n",
            "\u001b[32m[01/11 01:34:01 d2.utils.events]: \u001b[0m eta: 0:59:58  iter: 319  total_loss: 0.9381  loss_cls: 0.2095  loss_box_reg: 0.6842  loss_rpn_cls: 0.02498  loss_rpn_loc: 0.01361    time: 0.7758  last_time: 0.7681  data_time: 0.0328  last_data_time: 0.0234   lr: 7.992e-05  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:34:09 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:34:09 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:34:09 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:34:09 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:34:09 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:34:09 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:34:09 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:34:09 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:34:10 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0481 s/iter. Eval: 0.0004 s/iter. Total: 0.0497 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:34:15 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0017 s/iter. Inference: 0.0488 s/iter. Eval: 0.0004 s/iter. Total: 0.0509 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:34:20 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0019 s/iter. Inference: 0.0486 s/iter. Eval: 0.0004 s/iter. Total: 0.0509 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 01:34:22 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.559750 (0.052332 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:34:22 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049317 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:34:23 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:34:23 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:34:23 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.02s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.12s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.175\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.561\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.038\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.117\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.220\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.099\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.321\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.395\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.368\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.433\n",
            "\u001b[32m[01/11 01:34:24 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 17.544 | 56.103 | 3.819  | 0.000 | 11.706 | 22.028 |\n",
            "\u001b[32m[01/11 01:34:24 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 17.544 |\n",
            "\u001b[32m[01/11 01:34:24 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:34:24 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:34:24 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:34:24 d2.evaluation.testing]: \u001b[0mcopypaste: 17.5437,56.1027,3.8186,0.0000,11.7059,22.0277\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:34:32 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:34:32 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:34:32 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:34:32 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:34:32 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:34:32 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:34:32 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:34:32 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:34:33 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0487 s/iter. Eval: 0.0004 s/iter. Total: 0.0500 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:34:38 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0016 s/iter. Inference: 0.0490 s/iter. Eval: 0.0004 s/iter. Total: 0.0510 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:34:43 d2.evaluation.evaluator]: \u001b[0mInference done 210/245. Dataloading: 0.0015 s/iter. Inference: 0.0485 s/iter. Eval: 0.0004 s/iter. Total: 0.0504 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 01:34:44 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.224396 (0.050935 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:34:44 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048369 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:34:45 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:34:45 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:34:45 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.44s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.97s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.12s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.186\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.572\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.039\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.122\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.237\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.104\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.334\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.404\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.388\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.435\n",
            "\u001b[32m[01/11 01:34:46 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 18.579 | 57.244 | 3.898  | 0.000 | 12.225 | 23.654 |\n",
            "\u001b[32m[01/11 01:34:46 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 18.579 |\n",
            "\u001b[32m[01/11 01:34:46 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:34:46 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:34:46 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:34:46 d2.evaluation.testing]: \u001b[0mcopypaste: 18.5790,57.2442,3.8982,0.0000,12.2251,23.6539\n",
            "\u001b[32m[01/11 01:34:46 d2.utils.events]: \u001b[0m eta: 0:59:42  iter: 339  total_loss: 0.8811  loss_cls: 0.183  loss_box_reg: 0.6275  loss_rpn_cls: 0.02304  loss_rpn_loc: 0.01691    time: 0.7760  last_time: 0.7635  data_time: 0.0335  last_data_time: 0.0233   lr: 8.4915e-05  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:34:55 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:34:55 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:34:55 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:34:55 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:34:55 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:34:55 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:34:55 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:34:55 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:34:56 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0480 s/iter. Eval: 0.0003 s/iter. Total: 0.0493 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:35:01 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0014 s/iter. Inference: 0.0486 s/iter. Eval: 0.0004 s/iter. Total: 0.0504 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:35:06 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0015 s/iter. Inference: 0.0490 s/iter. Eval: 0.0004 s/iter. Total: 0.0509 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 01:35:08 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.342988 (0.051429 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:35:08 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048901 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:35:08 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:35:08 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:35:08 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.95s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.12s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.199\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.588\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.047\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.135\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.252\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.113\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.342\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.414\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.397\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.447\n",
            "\u001b[32m[01/11 01:35:09 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 19.865 | 58.846 | 4.738  | 0.000 | 13.487 | 25.152 |\n",
            "\u001b[32m[01/11 01:35:09 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 19.865 |\n",
            "\u001b[32m[01/11 01:35:09 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:35:09 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:35:09 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:35:09 d2.evaluation.testing]: \u001b[0mcopypaste: 19.8652,58.8464,4.7383,0.0000,13.4873,25.1519\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:35:17 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:35:17 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:35:17 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:35:17 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:35:17 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:35:17 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:35:17 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:35:17 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:35:18 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0479 s/iter. Eval: 0.0004 s/iter. Total: 0.0494 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:35:23 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0018 s/iter. Inference: 0.0492 s/iter. Eval: 0.0004 s/iter. Total: 0.0514 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:35:28 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0017 s/iter. Inference: 0.0490 s/iter. Eval: 0.0004 s/iter. Total: 0.0511 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 01:35:30 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.406909 (0.051695 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:35:30 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048922 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:35:30 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:35:30 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:35:30 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.91s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.11s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.210\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.601\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.059\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.135\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.269\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.117\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.353\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.421\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.399\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.457\n",
            "\u001b[32m[01/11 01:35:31 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 20.953 | 60.138 | 5.916  | 0.000 | 13.526 | 26.888 |\n",
            "\u001b[32m[01/11 01:35:31 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 20.953 |\n",
            "\u001b[32m[01/11 01:35:31 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:35:31 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:35:31 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:35:31 d2.evaluation.testing]: \u001b[0mcopypaste: 20.9529,60.1375,5.9159,0.0000,13.5258,26.8880\n",
            "\u001b[32m[01/11 01:35:31 d2.utils.events]: \u001b[0m eta: 0:59:25  iter: 359  total_loss: 0.8783  loss_cls: 0.1876  loss_box_reg: 0.5925  loss_rpn_cls: 0.03307  loss_rpn_loc: 0.02921    time: 0.7762  last_time: 0.7745  data_time: 0.0331  last_data_time: 0.0349   lr: 8.991e-05  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:35:39 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:35:39 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:35:39 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:35:39 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:35:39 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:35:39 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:35:39 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:35:39 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:35:40 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0031 s/iter. Inference: 0.0486 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:35:45 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0022 s/iter. Inference: 0.0500 s/iter. Eval: 0.0004 s/iter. Total: 0.0526 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:35:50 d2.evaluation.evaluator]: \u001b[0mInference done 195/245. Dataloading: 0.0019 s/iter. Inference: 0.0498 s/iter. Eval: 0.0027 s/iter. Total: 0.0545 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:35:53 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:13.075072 (0.054479 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:35:53 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049487 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:35:53 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:35:53 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:35:53 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.88s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.11s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.213\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.607\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.063\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.135\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.276\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.117\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.358\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.418\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.390\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.457\n",
            "\u001b[32m[01/11 01:35:54 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 21.315 | 60.727 | 6.327  | 0.000 | 13.472 | 27.616 |\n",
            "\u001b[32m[01/11 01:35:54 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 21.315 |\n",
            "\u001b[32m[01/11 01:35:54 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:35:54 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:35:54 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:35:54 d2.evaluation.testing]: \u001b[0mcopypaste: 21.3151,60.7269,6.3274,0.0000,13.4720,27.6164\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:36:02 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:36:02 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:36:02 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:36:02 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:36:02 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:36:02 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:36:02 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:36:02 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:36:03 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0473 s/iter. Eval: 0.0004 s/iter. Total: 0.0488 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:36:08 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0017 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:36:13 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0017 s/iter. Inference: 0.0500 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:36:15 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.622599 (0.052594 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:36:15 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049764 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:36:15 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:36:15 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:36:16 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.79s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.10s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.223\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.608\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.088\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.143\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.289\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.125\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.371\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.430\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.395\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.475\n",
            "\u001b[32m[01/11 01:36:16 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 22.282 | 60.793 | 8.820  | 0.000 | 14.284 | 28.906 |\n",
            "\u001b[32m[01/11 01:36:16 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 22.282 |\n",
            "\u001b[32m[01/11 01:36:16 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:36:16 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:36:16 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:36:16 d2.evaluation.testing]: \u001b[0mcopypaste: 22.2823,60.7925,8.8195,0.0000,14.2841,28.9058\n",
            "\u001b[32m[01/11 01:36:16 d2.utils.events]: \u001b[0m eta: 0:59:10  iter: 379  total_loss: 0.8301  loss_cls: 0.1659  loss_box_reg: 0.585  loss_rpn_cls: 0.03306  loss_rpn_loc: 0.01889    time: 0.7761  last_time: 0.7711  data_time: 0.0331  last_data_time: 0.0329   lr: 9.4905e-05  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:36:25 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:36:25 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:36:25 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:36:25 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:36:25 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:36:25 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:36:25 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:36:25 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:36:26 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0022 s/iter. Inference: 0.0482 s/iter. Eval: 0.0003 s/iter. Total: 0.0507 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:36:31 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0015 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:36:36 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0015 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:36:38 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.534103 (0.052225 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:36:38 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049590 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:36:38 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:36:38 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:36:38 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.79s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.10s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.229\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.610\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.100\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.139\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.310\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.130\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.385\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.440\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.396\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.491\n",
            "\u001b[32m[01/11 01:36:39 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 22.910 | 60.983 | 9.962  | 0.000 | 13.862 | 30.966 |\n",
            "\u001b[32m[01/11 01:36:39 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 22.910 |\n",
            "\u001b[32m[01/11 01:36:39 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:36:39 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:36:39 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:36:39 d2.evaluation.testing]: \u001b[0mcopypaste: 22.9103,60.9835,9.9616,0.0000,13.8624,30.9656\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:36:47 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:36:47 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:36:47 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:36:47 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:36:47 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:36:47 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:36:47 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:36:47 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:36:48 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0028 s/iter. Inference: 0.0480 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:36:53 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0022 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:36:58 d2.evaluation.evaluator]: \u001b[0mInference done 194/245. Dataloading: 0.0022 s/iter. Inference: 0.0521 s/iter. Eval: 0.0003 s/iter. Total: 0.0548 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:37:01 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:13.158026 (0.054825 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:37:01 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.051579 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:37:01 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:37:01 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:37:01 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.75s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.09s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.235\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.623\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.113\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.147\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.313\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.131\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.389\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.399\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.494\n",
            "\u001b[32m[01/11 01:37:02 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 23.521 | 62.289 | 11.300 | 0.000 | 14.700 | 31.338 |\n",
            "\u001b[32m[01/11 01:37:02 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 23.521 |\n",
            "\u001b[32m[01/11 01:37:02 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:37:02 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:37:02 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:37:02 d2.evaluation.testing]: \u001b[0mcopypaste: 23.5206,62.2888,11.3005,0.0000,14.7001,31.3376\n",
            "\u001b[32m[01/11 01:37:02 d2.utils.events]: \u001b[0m eta: 0:58:55  iter: 399  total_loss: 0.7815  loss_cls: 0.1672  loss_box_reg: 0.5367  loss_rpn_cls: 0.03197  loss_rpn_loc: 0.02027    time: 0.7759  last_time: 0.7005  data_time: 0.0345  last_data_time: 0.0245   lr: 9.99e-05  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:37:10 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:37:10 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:37:10 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:37:10 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:37:10 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:37:10 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:37:10 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:37:10 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:37:11 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0476 s/iter. Eval: 0.0003 s/iter. Total: 0.0490 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:37:16 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0015 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:37:21 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0016 s/iter. Inference: 0.0500 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:37:24 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.618105 (0.052575 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:37:24 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049944 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:37:24 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:37:24 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:37:24 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.70s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.09s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.231\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.614\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.108\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.149\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.305\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.129\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.387\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.445\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.407\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.491\n",
            "\u001b[32m[01/11 01:37:24 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 23.059 | 61.362 | 10.779 | 0.000 | 14.904 | 30.507 |\n",
            "\u001b[32m[01/11 01:37:24 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 23.059 |\n",
            "\u001b[32m[01/11 01:37:24 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:37:24 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:37:24 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:37:24 d2.evaluation.testing]: \u001b[0mcopypaste: 23.0589,61.3620,10.7792,0.0000,14.9045,30.5069\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:37:32 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:37:32 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:37:32 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:37:32 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:37:32 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:37:32 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:37:32 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:37:32 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:37:33 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0505 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:37:38 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0019 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:37:43 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0019 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:37:46 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.579536 (0.052415 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:37:46 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049472 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:37:46 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:37:46 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:37:46 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.68s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.08s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.229\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.614\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.105\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.144\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.314\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.130\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.385\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.440\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.402\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.486\n",
            "\u001b[32m[01/11 01:37:46 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 22.923 | 61.400 | 10.487 | 0.000 | 14.394 | 31.366 |\n",
            "\u001b[32m[01/11 01:37:46 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 22.923 |\n",
            "\u001b[32m[01/11 01:37:46 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:37:46 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:37:46 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:37:46 d2.evaluation.testing]: \u001b[0mcopypaste: 22.9235,61.4005,10.4866,0.0000,14.3943,31.3657\n",
            "\u001b[32m[01/11 01:37:46 d2.utils.events]: \u001b[0m eta: 0:58:42  iter: 419  total_loss: 0.8092  loss_cls: 0.1989  loss_box_reg: 0.5454  loss_rpn_cls: 0.03238  loss_rpn_loc: 0.02438    time: 0.7764  last_time: 0.7794  data_time: 0.0340  last_data_time: 0.0218   lr: 0.0001049  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:37:55 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:37:55 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:37:55 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:37:55 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:37:55 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:37:55 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:37:55 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:37:55 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:37:56 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0482 s/iter. Eval: 0.0003 s/iter. Total: 0.0494 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:38:01 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0015 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:38:06 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0016 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 01:38:08 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.413491 (0.051723 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:38:08 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048963 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:38:08 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:38:08 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:38:08 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.63s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.09s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.226\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.602\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.116\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.145\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.312\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.131\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.382\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.438\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.405\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.481\n",
            "\u001b[32m[01/11 01:38:09 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 22.642 | 60.166 | 11.560 | 0.000 | 14.512 | 31.226 |\n",
            "\u001b[32m[01/11 01:38:09 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 22.642 |\n",
            "\u001b[32m[01/11 01:38:09 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:38:09 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:38:09 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:38:09 d2.evaluation.testing]: \u001b[0mcopypaste: 22.6417,60.1665,11.5599,0.0000,14.5119,31.2260\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:38:16 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:38:16 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:38:16 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:38:16 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:38:16 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:38:16 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:38:16 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:38:17 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:38:18 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:38:23 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0021 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:38:28 d2.evaluation.evaluator]: \u001b[0mInference done 209/245. Dataloading: 0.0018 s/iter. Inference: 0.0486 s/iter. Eval: 0.0003 s/iter. Total: 0.0507 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 01:38:30 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.389310 (0.051622 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:38:30 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048594 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:38:30 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:38:30 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:38:30 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.60s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.08s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.229\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.607\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.113\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.150\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.316\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.131\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.389\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.450\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.417\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.493\n",
            "\u001b[32m[01/11 01:38:31 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 22.913 | 60.707 | 11.322 | 0.000 | 14.983 | 31.554 |\n",
            "\u001b[32m[01/11 01:38:31 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 22.913 |\n",
            "\u001b[32m[01/11 01:38:31 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:38:31 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:38:31 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:38:31 d2.evaluation.testing]: \u001b[0mcopypaste: 22.9133,60.7069,11.3217,0.0000,14.9826,31.5541\n",
            "\u001b[32m[01/11 01:38:31 d2.utils.events]: \u001b[0m eta: 0:58:25  iter: 439  total_loss: 0.7811  loss_cls: 0.1727  loss_box_reg: 0.5333  loss_rpn_cls: 0.02936  loss_rpn_loc: 0.02288    time: 0.7757  last_time: 0.7629  data_time: 0.0320  last_data_time: 0.0272   lr: 0.00010989  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:38:39 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:38:39 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:38:39 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:38:39 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:38:39 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:38:39 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:38:39 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:38:39 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:38:40 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0481 s/iter. Eval: 0.0003 s/iter. Total: 0.0496 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:38:45 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0018 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:38:50 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0016 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:38:52 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.649933 (0.052708 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:38:52 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049931 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:38:52 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:38:52 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:38:52 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.58s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.08s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.248\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.625\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.149\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.158\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.345\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.142\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.414\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.464\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.417\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.517\n",
            "\u001b[32m[01/11 01:38:53 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 24.806 | 62.473 | 14.879 | 0.000 | 15.790 | 34.499 |\n",
            "\u001b[32m[01/11 01:38:53 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 24.806 |\n",
            "\u001b[32m[01/11 01:38:53 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:38:53 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:38:53 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:38:53 d2.evaluation.testing]: \u001b[0mcopypaste: 24.8059,62.4731,14.8788,0.0000,15.7901,34.4994\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:39:01 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:39:01 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:39:01 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:39:01 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:39:01 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:39:01 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:39:01 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:39:01 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:39:02 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0512 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:39:07 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0017 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:39:12 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0017 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:39:14 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.557786 (0.052324 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:39:14 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049601 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:39:14 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:39:14 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:39:14 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.53s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.07s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.260\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.631\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.157\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.167\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.352\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.143\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.424\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.466\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.416\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.522\n",
            "\u001b[32m[01/11 01:39:14 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 25.978 | 63.105 | 15.712 | 0.000 | 16.702 | 35.181 |\n",
            "\u001b[32m[01/11 01:39:14 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 25.978 |\n",
            "\u001b[32m[01/11 01:39:14 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:39:14 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:39:14 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:39:14 d2.evaluation.testing]: \u001b[0mcopypaste: 25.9780,63.1051,15.7124,0.0000,16.7015,35.1811\n",
            "\u001b[32m[01/11 01:39:14 d2.utils.events]: \u001b[0m eta: 0:58:09  iter: 459  total_loss: 0.7195  loss_cls: 0.1745  loss_box_reg: 0.4878  loss_rpn_cls: 0.0369  loss_rpn_loc: 0.02225    time: 0.7749  last_time: 0.7877  data_time: 0.0299  last_data_time: 0.0256   lr: 0.00011489  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:39:23 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:39:23 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:39:23 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:39:23 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:39:23 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:39:23 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:39:23 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:39:23 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:39:24 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0503 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:39:29 d2.evaluation.evaluator]: \u001b[0mInference done 104/245. Dataloading: 0.0016 s/iter. Inference: 0.0519 s/iter. Eval: 0.0003 s/iter. Total: 0.0539 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:39:34 d2.evaluation.evaluator]: \u001b[0mInference done 200/245. Dataloading: 0.0015 s/iter. Inference: 0.0512 s/iter. Eval: 0.0003 s/iter. Total: 0.0531 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:39:36 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.916052 (0.053817 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:39:36 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.051142 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:39:36 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:39:36 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:39:37 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.62s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.09s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.224\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.596\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.113\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.152\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.316\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.130\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.391\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.448\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.423\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.486\n",
            "\u001b[32m[01/11 01:39:37 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 22.443 | 59.559 | 11.288 | 0.000 | 15.171 | 31.634 |\n",
            "\u001b[32m[01/11 01:39:37 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 22.443 |\n",
            "\u001b[32m[01/11 01:39:37 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:39:37 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:39:37 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:39:37 d2.evaluation.testing]: \u001b[0mcopypaste: 22.4433,59.5589,11.2880,0.0000,15.1710,31.6340\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:39:45 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:39:45 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:39:45 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:39:45 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:39:45 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:39:45 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:39:45 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:39:45 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:39:46 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0503 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:39:51 d2.evaluation.evaluator]: \u001b[0mInference done 105/245. Dataloading: 0.0022 s/iter. Inference: 0.0504 s/iter. Eval: 0.0003 s/iter. Total: 0.0530 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:39:56 d2.evaluation.evaluator]: \u001b[0mInference done 201/245. Dataloading: 0.0021 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0528 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:39:59 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.798314 (0.053326 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:39:59 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050106 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:39:59 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:39:59 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:39:59 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.66s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.09s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.249\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.615\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.149\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.153\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.356\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.140\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.423\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.469\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.422\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.522\n",
            "\u001b[32m[01/11 01:40:00 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 24.919 | 61.527 | 14.892 | 0.000 | 15.251 | 35.593 |\n",
            "\u001b[32m[01/11 01:40:00 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 24.919 |\n",
            "\u001b[32m[01/11 01:40:00 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:40:00 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:40:00 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:40:00 d2.evaluation.testing]: \u001b[0mcopypaste: 24.9192,61.5268,14.8916,0.0000,15.2508,35.5931\n",
            "\u001b[32m[01/11 01:40:00 d2.utils.events]: \u001b[0m eta: 0:57:53  iter: 479  total_loss: 0.7942  loss_cls: 0.1893  loss_box_reg: 0.5213  loss_rpn_cls: 0.03435  loss_rpn_loc: 0.02028    time: 0.7756  last_time: 0.7648  data_time: 0.0303  last_data_time: 0.0248   lr: 0.00011988  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:40:08 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:40:08 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:40:08 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:40:08 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:40:08 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:40:08 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:40:08 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:40:08 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:40:09 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0528 s/iter. Eval: 0.0003 s/iter. Total: 0.0540 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:40:14 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0018 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:40:19 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0019 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0523 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:40:22 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.692046 (0.052884 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:40:22 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050021 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:40:22 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:40:22 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:40:22 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.59s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.08s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.247\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.621\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.143\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.151\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.351\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.140\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.418\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.461\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.403\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.520\n",
            "\u001b[32m[01/11 01:40:22 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 24.730 | 62.103 | 14.266 | 0.000 | 15.063 | 35.126 |\n",
            "\u001b[32m[01/11 01:40:22 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 24.730 |\n",
            "\u001b[32m[01/11 01:40:22 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:40:22 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:40:22 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:40:22 d2.evaluation.testing]: \u001b[0mcopypaste: 24.7305,62.1033,14.2664,0.0000,15.0634,35.1263\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:40:30 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:40:30 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:40:30 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:40:30 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:40:30 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:40:30 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:40:30 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:40:30 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:40:31 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0507 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:40:36 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0015 s/iter. Inference: 0.0510 s/iter. Eval: 0.0003 s/iter. Total: 0.0529 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:40:41 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0019 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:40:44 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.717658 (0.052990 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:40:44 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049990 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:40:44 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:40:44 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:40:44 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.51s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.08s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.254\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.628\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.158\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.157\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.360\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.143\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.423\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.462\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.412\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.518\n",
            "\u001b[32m[01/11 01:40:44 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 25.446 | 62.787 | 15.813 | 0.000 | 15.683 | 35.982 |\n",
            "\u001b[32m[01/11 01:40:44 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 25.446 |\n",
            "\u001b[32m[01/11 01:40:44 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:40:44 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:40:44 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:40:44 d2.evaluation.testing]: \u001b[0mcopypaste: 25.4461,62.7868,15.8133,0.0000,15.6831,35.9815\n",
            "\u001b[32m[01/11 01:40:44 d2.utils.events]: \u001b[0m eta: 0:57:39  iter: 499  total_loss: 0.7051  loss_cls: 0.1644  loss_box_reg: 0.4777  loss_rpn_cls: 0.01982  loss_rpn_loc: 0.01742    time: 0.7757  last_time: 0.7794  data_time: 0.0330  last_data_time: 0.0262   lr: 0.00012488  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:40:53 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:40:53 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:40:53 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:40:53 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:40:53 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:40:53 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:40:53 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:40:53 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:40:54 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0007 s/iter. Inference: 0.0506 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:40:59 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0021 s/iter. Inference: 0.0502 s/iter. Eval: 0.0003 s/iter. Total: 0.0527 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:41:04 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0019 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:41:06 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.588766 (0.052453 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:41:06 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049474 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:41:06 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:41:06 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:41:06 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.47s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.07s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.272\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.639\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.181\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.162\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.379\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.151\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.439\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.475\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.409\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.541\n",
            "\u001b[32m[01/11 01:41:07 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 27.246 | 63.870 | 18.097 | 0.000 | 16.209 | 37.858 |\n",
            "\u001b[32m[01/11 01:41:07 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 27.246 |\n",
            "\u001b[32m[01/11 01:41:07 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:41:07 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:41:07 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:41:07 d2.evaluation.testing]: \u001b[0mcopypaste: 27.2457,63.8699,18.0966,0.0000,16.2094,37.8576\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:41:14 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:41:14 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:41:14 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:41:14 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:41:14 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:41:14 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:41:14 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:41:15 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:41:16 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0008 s/iter. Inference: 0.0488 s/iter. Eval: 0.0003 s/iter. Total: 0.0499 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:41:21 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0018 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:41:26 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0019 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:41:28 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.585785 (0.052441 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:41:28 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049472 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:41:28 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:41:28 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:41:28 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.50s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.07s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.271\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.640\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.173\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.159\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.377\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.152\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.473\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.413\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.535\n",
            "\u001b[32m[01/11 01:41:29 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 27.142 | 64.038 | 17.331 | 0.000 | 15.898 | 37.654 |\n",
            "\u001b[32m[01/11 01:41:29 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 27.142 |\n",
            "\u001b[32m[01/11 01:41:29 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:41:29 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:41:29 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:41:29 d2.evaluation.testing]: \u001b[0mcopypaste: 27.1416,64.0380,17.3312,0.0000,15.8983,37.6538\n",
            "\u001b[32m[01/11 01:41:29 d2.utils.events]: \u001b[0m eta: 0:57:25  iter: 519  total_loss: 0.7217  loss_cls: 0.1707  loss_box_reg: 0.4735  loss_rpn_cls: 0.02826  loss_rpn_loc: 0.02151    time: 0.7758  last_time: 0.7690  data_time: 0.0362  last_data_time: 0.0252   lr: 0.00012987  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:41:37 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:41:37 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:41:37 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:41:37 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:41:37 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:41:37 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:41:37 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:41:37 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:41:38 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0033 s/iter. Inference: 0.0488 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:41:43 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0020 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:41:48 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0019 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 01:41:50 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.511752 (0.052132 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:41:50 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049207 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:41:50 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:41:50 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:41:50 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.47s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.07s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.230\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.617\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.109\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.145\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.326\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.131\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.391\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.442\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.417\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.481\n",
            "\u001b[32m[01/11 01:41:51 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 22.990 | 61.679 | 10.907 | 0.000 | 14.456 | 32.594 |\n",
            "\u001b[32m[01/11 01:41:51 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 22.990 |\n",
            "\u001b[32m[01/11 01:41:51 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:41:51 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:41:51 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:41:51 d2.evaluation.testing]: \u001b[0mcopypaste: 22.9895,61.6790,10.9068,0.0000,14.4558,32.5935\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:41:58 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:41:58 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:41:58 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:41:58 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:41:58 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:41:58 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:41:58 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:41:58 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:41:59 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0472 s/iter. Eval: 0.0003 s/iter. Total: 0.0486 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:42:04 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0027 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:42:09 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0022 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:42:12 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.628432 (0.052618 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:42:12 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049408 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:42:12 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:42:12 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:42:12 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.50s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.07s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.256\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.625\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.162\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.155\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.358\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.143\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.415\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.467\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.421\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.521\n",
            "\u001b[32m[01/11 01:42:12 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 25.570 | 62.535 | 16.250 | 0.000 | 15.504 | 35.833 |\n",
            "\u001b[32m[01/11 01:42:12 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 25.570 |\n",
            "\u001b[32m[01/11 01:42:12 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:42:12 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:42:12 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:42:12 d2.evaluation.testing]: \u001b[0mcopypaste: 25.5697,62.5351,16.2498,0.0000,15.5038,35.8335\n",
            "\u001b[32m[01/11 01:42:12 d2.utils.events]: \u001b[0m eta: 0:57:09  iter: 539  total_loss: 0.7164  loss_cls: 0.1511  loss_box_reg: 0.4756  loss_rpn_cls: 0.02803  loss_rpn_loc: 0.0279    time: 0.7747  last_time: 0.6995  data_time: 0.0317  last_data_time: 0.0231   lr: 0.00013487  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:42:20 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:42:20 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:42:20 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:42:20 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:42:20 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:42:20 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:42:20 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:42:20 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:42:21 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0452 s/iter. Eval: 0.0003 s/iter. Total: 0.0467 s/iter. ETA=0:00:10\n",
            "\u001b[32m[01/11 01:42:26 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0019 s/iter. Inference: 0.0488 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:42:31 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0017 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 01:42:33 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.389103 (0.051621 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:42:33 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048816 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:42:33 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:42:33 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:42:33 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.53s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.08s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.261\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.627\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.167\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.157\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.370\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.145\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.426\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.475\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.423\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.532\n",
            "\u001b[32m[01/11 01:42:34 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 26.145 | 62.719 | 16.663 | 0.000 | 15.672 | 36.969 |\n",
            "\u001b[32m[01/11 01:42:34 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 26.145 |\n",
            "\u001b[32m[01/11 01:42:34 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:42:34 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:42:34 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:42:34 d2.evaluation.testing]: \u001b[0mcopypaste: 26.1448,62.7186,16.6629,0.0000,15.6719,36.9688\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:42:42 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:42:42 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:42:42 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:42:42 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:42:42 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:42:42 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:42:42 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:42:42 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:42:43 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0476 s/iter. Eval: 0.0003 s/iter. Total: 0.0488 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:42:48 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0023 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:42:53 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0022 s/iter. Inference: 0.0488 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:42:55 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.480686 (0.052003 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:42:55 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048799 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:42:55 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:42:55 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:42:55 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.86s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.274\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.639\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.186\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.156\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.388\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.149\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.446\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.482\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.414\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.550\n",
            "\u001b[32m[01/11 01:42:56 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 27.417 | 63.905 | 18.626 | 0.000 | 15.587 | 38.772 |\n",
            "\u001b[32m[01/11 01:42:56 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 27.417 |\n",
            "\u001b[32m[01/11 01:42:56 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:42:56 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:42:56 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:42:56 d2.evaluation.testing]: \u001b[0mcopypaste: 27.4170,63.9045,18.6255,0.0000,15.5874,38.7719\n",
            "\u001b[32m[01/11 01:42:56 d2.utils.events]: \u001b[0m eta: 0:56:54  iter: 559  total_loss: 0.7026  loss_cls: 0.1767  loss_box_reg: 0.4636  loss_rpn_cls: 0.03073  loss_rpn_loc: 0.02277    time: 0.7745  last_time: 0.7744  data_time: 0.0330  last_data_time: 0.0252   lr: 0.00013986  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:43:04 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:43:04 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:43:04 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:43:04 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:43:04 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:43:04 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:43:04 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:43:04 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:43:05 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0503 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:43:10 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0018 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:43:15 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0018 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 01:43:18 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.493546 (0.052056 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:43:18 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049281 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:43:18 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:43:18 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:43:18 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.43s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.260\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.630\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.160\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.157\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.361\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.144\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.425\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.466\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.418\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.520\n",
            "\u001b[32m[01/11 01:43:18 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 25.976 | 63.040 | 16.024 | 0.000 | 15.693 | 36.090 |\n",
            "\u001b[32m[01/11 01:43:18 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 25.976 |\n",
            "\u001b[32m[01/11 01:43:18 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:43:18 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:43:18 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:43:18 d2.evaluation.testing]: \u001b[0mcopypaste: 25.9758,63.0396,16.0235,0.0000,15.6935,36.0899\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:43:26 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:43:26 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:43:26 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:43:26 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:43:26 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:43:26 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:43:26 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:43:26 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:43:27 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0505 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:43:32 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0016 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:43:37 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0016 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:43:39 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.555036 (0.052313 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:43:39 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049702 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:43:39 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:43:39 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:43:39 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.43s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.266\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.633\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.170\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.160\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.366\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.149\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.434\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.473\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.417\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.533\n",
            "\u001b[32m[01/11 01:43:40 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 26.633 | 63.291 | 16.987 | 0.000 | 16.016 | 36.650 |\n",
            "\u001b[32m[01/11 01:43:40 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 26.633 |\n",
            "\u001b[32m[01/11 01:43:40 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:43:40 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:43:40 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:43:40 d2.evaluation.testing]: \u001b[0mcopypaste: 26.6329,63.2906,16.9867,0.0000,16.0164,36.6497\n",
            "\u001b[32m[01/11 01:43:40 d2.utils.events]: \u001b[0m eta: 0:56:38  iter: 579  total_loss: 0.689  loss_cls: 0.1737  loss_box_reg: 0.4521  loss_rpn_cls: 0.02593  loss_rpn_loc: 0.01762    time: 0.7741  last_time: 0.6950  data_time: 0.0282  last_data_time: 0.0237   lr: 0.00014486  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:43:48 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:43:48 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:43:48 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:43:48 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:43:48 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:43:48 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:43:48 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:43:48 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:43:49 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0509 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:43:54 d2.evaluation.evaluator]: \u001b[0mInference done 103/245. Dataloading: 0.0014 s/iter. Inference: 0.0525 s/iter. Eval: 0.0003 s/iter. Total: 0.0543 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:43:59 d2.evaluation.evaluator]: \u001b[0mInference done 199/245. Dataloading: 0.0015 s/iter. Inference: 0.0515 s/iter. Eval: 0.0003 s/iter. Total: 0.0533 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:44:01 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.785997 (0.053275 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:44:01 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050847 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:44:01 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:44:01 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:44:01 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.47s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.260\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.629\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.167\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.156\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.357\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.146\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.429\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.472\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.430\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.523\n",
            "\u001b[32m[01/11 01:44:02 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 26.045 | 62.883 | 16.704 | 0.000 | 15.555 | 35.746 |\n",
            "\u001b[32m[01/11 01:44:02 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 26.045 |\n",
            "\u001b[32m[01/11 01:44:02 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:44:02 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:44:02 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:44:02 d2.evaluation.testing]: \u001b[0mcopypaste: 26.0451,62.8828,16.7044,0.0000,15.5551,35.7462\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:44:10 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:44:10 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:44:10 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:44:10 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:44:10 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:44:10 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:44:10 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:44:10 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:44:11 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0052 s/iter. Inference: 0.0482 s/iter. Eval: 0.0003 s/iter. Total: 0.0538 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:44:16 d2.evaluation.evaluator]: \u001b[0mInference done 105/245. Dataloading: 0.0017 s/iter. Inference: 0.0513 s/iter. Eval: 0.0003 s/iter. Total: 0.0534 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:44:21 d2.evaluation.evaluator]: \u001b[0mInference done 202/245. Dataloading: 0.0020 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0527 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:44:23 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.757824 (0.053158 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:44:23 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050178 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:44:23 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:44:23 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:44:23 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.48s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.07s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.274\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.638\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.183\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.157\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.372\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.151\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.439\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.482\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.429\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.540\n",
            "\u001b[32m[01/11 01:44:24 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 27.389 | 63.845 | 18.312 | 0.000 | 15.711 | 37.163 |\n",
            "\u001b[32m[01/11 01:44:24 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 27.389 |\n",
            "\u001b[32m[01/11 01:44:24 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:44:24 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:44:24 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:44:24 d2.evaluation.testing]: \u001b[0mcopypaste: 27.3887,63.8445,18.3123,0.0000,15.7113,37.1633\n",
            "\u001b[32m[01/11 01:44:24 d2.utils.events]: \u001b[0m eta: 0:56:23  iter: 599  total_loss: 0.7727  loss_cls: 0.1744  loss_box_reg: 0.4901  loss_rpn_cls: 0.03813  loss_rpn_loc: 0.02575    time: 0.7740  last_time: 0.7850  data_time: 0.0313  last_data_time: 0.0287   lr: 0.00014985  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:44:32 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:44:32 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:44:32 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:44:32 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:44:32 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:44:32 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:44:32 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:44:32 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:44:33 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0505 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:44:38 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0015 s/iter. Inference: 0.0500 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:44:43 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0017 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:44:45 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.565738 (0.052357 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:44:45 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049605 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:44:45 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:44:45 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:44:45 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.43s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.248\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.637\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.136\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.149\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.339\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.143\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.403\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.453\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.419\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.497\n",
            "\u001b[32m[01/11 01:44:46 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 24.823 | 63.743 | 13.568 | 0.000 | 14.916 | 33.861 |\n",
            "\u001b[32m[01/11 01:44:46 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 24.823 |\n",
            "\u001b[32m[01/11 01:44:46 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:44:46 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:44:46 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:44:46 d2.evaluation.testing]: \u001b[0mcopypaste: 24.8229,63.7431,13.5675,0.0000,14.9156,33.8608\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:44:53 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:44:53 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:44:53 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:44:53 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:44:53 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:44:53 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:44:53 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:44:53 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:44:54 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0008 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0506 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:44:59 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0016 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:45:04 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0017 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:45:07 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:13.018372 (0.054243 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:45:07 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.051447 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:45:07 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:45:07 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:45:07 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.46s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.280\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.654\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.194\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.159\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.379\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.155\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.435\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.475\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.415\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.538\n",
            "\u001b[32m[01/11 01:45:08 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 27.983 | 65.399 | 19.415 | 0.000 | 15.925 | 37.873 |\n",
            "\u001b[32m[01/11 01:45:08 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 27.983 |\n",
            "\u001b[32m[01/11 01:45:08 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:45:08 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:45:08 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:45:08 d2.evaluation.testing]: \u001b[0mcopypaste: 27.9835,65.3988,19.4149,0.0000,15.9254,37.8731\n",
            "\u001b[32m[01/11 01:45:08 d2.utils.events]: \u001b[0m eta: 0:56:08  iter: 619  total_loss: 0.7027  loss_cls: 0.1715  loss_box_reg: 0.4651  loss_rpn_cls: 0.02815  loss_rpn_loc: 0.01624    time: 0.7739  last_time: 0.7741  data_time: 0.0346  last_data_time: 0.0276   lr: 0.00015485  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:45:16 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:45:16 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:45:16 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:45:16 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:45:16 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:45:16 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:45:16 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:45:16 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:45:17 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0466 s/iter. Eval: 0.0003 s/iter. Total: 0.0479 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:45:22 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0017 s/iter. Inference: 0.0492 s/iter. Eval: 0.0004 s/iter. Total: 0.0514 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:45:27 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0020 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:45:29 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.627925 (0.052616 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:45:29 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049421 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:45:29 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:45:29 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:45:30 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.45s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.07s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.291\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.658\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.203\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.159\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.392\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.158\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.442\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.485\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.423\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.549\n",
            "\u001b[32m[01/11 01:45:30 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.055 | 65.818 | 20.342 | 0.000 | 15.855 | 39.194 |\n",
            "\u001b[32m[01/11 01:45:30 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.055 |\n",
            "\u001b[32m[01/11 01:45:30 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:45:30 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:45:30 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:45:30 d2.evaluation.testing]: \u001b[0mcopypaste: 29.0548,65.8183,20.3418,0.0000,15.8546,39.1942\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:45:38 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:45:38 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:45:38 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:45:38 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:45:38 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:45:38 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:45:38 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:45:38 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:45:39 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0510 s/iter. Eval: 0.0003 s/iter. Total: 0.0526 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:45:44 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0015 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:45:49 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0014 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 01:45:51 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.467030 (0.051946 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:45:51 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049372 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:45:51 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:45:51 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:45:51 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.39s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.268\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.642\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.169\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.151\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.364\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.151\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.421\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.465\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.417\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.518\n",
            "\u001b[32m[01/11 01:45:51 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 26.846 | 64.164 | 16.877 | 0.000 | 15.109 | 36.424 |\n",
            "\u001b[32m[01/11 01:45:51 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 26.846 |\n",
            "\u001b[32m[01/11 01:45:51 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:45:51 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:45:51 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:45:51 d2.evaluation.testing]: \u001b[0mcopypaste: 26.8455,64.1640,16.8775,0.0000,15.1086,36.4243\n",
            "\u001b[32m[01/11 01:45:51 d2.utils.events]: \u001b[0m eta: 0:55:52  iter: 639  total_loss: 0.6876  loss_cls: 0.1491  loss_box_reg: 0.4546  loss_rpn_cls: 0.03428  loss_rpn_loc: 0.02584    time: 0.7735  last_time: 0.7578  data_time: 0.0305  last_data_time: 0.0157   lr: 0.00015984  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:45:59 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:45:59 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:45:59 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:45:59 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:45:59 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:45:59 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:45:59 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:45:59 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:46:00 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0502 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:46:06 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0016 s/iter. Inference: 0.0488 s/iter. Eval: 0.0003 s/iter. Total: 0.0508 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:46:11 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0017 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:46:13 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.499223 (0.052080 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:46:13 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049292 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:46:13 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:46:13 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:46:13 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.42s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.278\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.646\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.194\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.152\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.380\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.152\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.476\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.416\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.538\n",
            "\u001b[32m[01/11 01:46:13 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 27.802 | 64.630 | 19.414 | 0.000 | 15.232 | 38.038 |\n",
            "\u001b[32m[01/11 01:46:13 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 27.802 |\n",
            "\u001b[32m[01/11 01:46:13 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:46:13 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:46:13 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:46:13 d2.evaluation.testing]: \u001b[0mcopypaste: 27.8017,64.6301,19.4135,0.0000,15.2323,38.0379\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:46:21 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:46:21 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:46:21 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:46:21 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:46:21 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:46:21 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:46:21 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:46:21 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:46:22 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0510 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:46:27 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0023 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:46:32 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0021 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:46:35 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.751126 (0.053130 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:46:35 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049997 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:46:35 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:46:35 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:46:35 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.39s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.273\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.648\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.172\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.158\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.369\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.152\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.422\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.460\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.409\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.515\n",
            "\u001b[32m[01/11 01:46:35 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 27.274 | 64.760 | 17.167 | 0.000 | 15.801 | 36.860 |\n",
            "\u001b[32m[01/11 01:46:35 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 27.274 |\n",
            "\u001b[32m[01/11 01:46:35 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:46:35 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:46:35 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:46:35 d2.evaluation.testing]: \u001b[0mcopypaste: 27.2738,64.7598,17.1673,0.0000,15.8014,36.8600\n",
            "\u001b[32m[01/11 01:46:35 d2.utils.events]: \u001b[0m eta: 0:55:36  iter: 659  total_loss: 0.7169  loss_cls: 0.1686  loss_box_reg: 0.4897  loss_rpn_cls: 0.02456  loss_rpn_loc: 0.01786    time: 0.7734  last_time: 0.7888  data_time: 0.0353  last_data_time: 0.0324   lr: 0.00016484  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:46:43 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:46:43 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:46:43 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:46:43 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:46:43 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:46:43 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:46:43 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:46:43 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:46:44 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0482 s/iter. Eval: 0.0003 s/iter. Total: 0.0493 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:46:50 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0017 s/iter. Inference: 0.0506 s/iter. Eval: 0.0003 s/iter. Total: 0.0526 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:46:55 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0016 s/iter. Inference: 0.0502 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:46:57 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.702598 (0.052927 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:46:57 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050254 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:46:57 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:46:57 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:46:57 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.42s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.286\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.648\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.206\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.156\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.390\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.157\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.438\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.478\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.416\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.541\n",
            "\u001b[32m[01/11 01:46:57 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 28.565 | 64.821 | 20.604 | 0.000 | 15.560 | 38.993 |\n",
            "\u001b[32m[01/11 01:46:57 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 28.565 |\n",
            "\u001b[32m[01/11 01:46:57 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:46:57 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:46:57 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:46:57 d2.evaluation.testing]: \u001b[0mcopypaste: 28.5650,64.8209,20.6043,0.0000,15.5600,38.9931\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:47:05 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:47:05 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:47:05 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:47:05 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:47:05 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:47:05 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:47:05 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:47:05 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:47:06 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0504 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:47:11 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0015 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:47:16 d2.evaluation.evaluator]: \u001b[0mInference done 198/245. Dataloading: 0.0019 s/iter. Inference: 0.0514 s/iter. Eval: 0.0003 s/iter. Total: 0.0537 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:47:19 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.939656 (0.053915 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:47:19 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050913 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:47:19 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:47:19 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:47:19 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.43s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.269\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.642\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.166\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.151\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.364\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.149\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.422\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.466\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.417\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.521\n",
            "\u001b[32m[01/11 01:47:19 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 26.874 | 64.180 | 16.586 | 0.000 | 15.069 | 36.356 |\n",
            "\u001b[32m[01/11 01:47:19 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 26.874 |\n",
            "\u001b[32m[01/11 01:47:19 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:47:19 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:47:19 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:47:19 d2.evaluation.testing]: \u001b[0mcopypaste: 26.8739,64.1801,16.5858,0.0000,15.0692,36.3561\n",
            "\u001b[32m[01/11 01:47:19 d2.utils.events]: \u001b[0m eta: 0:55:20  iter: 679  total_loss: 0.7189  loss_cls: 0.1742  loss_box_reg: 0.4841  loss_rpn_cls: 0.03335  loss_rpn_loc: 0.04191    time: 0.7732  last_time: 0.6989  data_time: 0.0336  last_data_time: 0.0258   lr: 0.00016983  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:47:27 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:47:27 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:47:27 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:47:27 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:47:27 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:47:27 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:47:27 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:47:27 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:47:28 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0487 s/iter. Eval: 0.0003 s/iter. Total: 0.0499 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:47:33 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0014 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:47:39 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0014 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:47:41 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.595960 (0.052483 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:47:41 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049972 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:47:41 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:47:41 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:47:41 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.41s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.271\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.646\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.178\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.153\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.366\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.151\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.417\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.461\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.415\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.514\n",
            "\u001b[32m[01/11 01:47:41 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 27.092 | 64.597 | 17.843 | 0.000 | 15.258 | 36.574 |\n",
            "\u001b[32m[01/11 01:47:41 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 27.092 |\n",
            "\u001b[32m[01/11 01:47:41 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:47:41 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:47:41 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:47:41 d2.evaluation.testing]: \u001b[0mcopypaste: 27.0921,64.5970,17.8429,0.0000,15.2578,36.5743\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:47:49 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:47:49 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:47:49 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:47:49 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:47:49 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:47:49 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:47:49 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:47:49 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:47:50 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0015 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:47:55 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0019 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0523 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:48:00 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0019 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:48:02 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.582467 (0.052427 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:48:02 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049565 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:48:02 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:48:02 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:48:02 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.35s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.290\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.665\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.196\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.160\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.386\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.157\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.435\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.471\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.405\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.536\n",
            "\u001b[32m[01/11 01:48:03 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.006 | 66.466 | 19.550 | 0.000 | 16.017 | 38.576 |\n",
            "\u001b[32m[01/11 01:48:03 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.006 |\n",
            "\u001b[32m[01/11 01:48:03 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:48:03 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:48:03 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:48:03 d2.evaluation.testing]: \u001b[0mcopypaste: 29.0060,66.4657,19.5504,0.0000,16.0166,38.5759\n",
            "\u001b[32m[01/11 01:48:03 d2.utils.events]: \u001b[0m eta: 0:55:04  iter: 699  total_loss: 0.6889  loss_cls: 0.1426  loss_box_reg: 0.4636  loss_rpn_cls: 0.02806  loss_rpn_loc: 0.01533    time: 0.7728  last_time: 0.7679  data_time: 0.0330  last_data_time: 0.0256   lr: 0.00017483  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:48:11 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:48:11 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:48:11 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:48:11 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:48:11 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:48:11 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:48:11 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:48:11 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:48:12 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0546 s/iter. Eval: 0.0003 s/iter. Total: 0.0558 s/iter. ETA=0:00:13\n",
            "\u001b[32m[01/11 01:48:17 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0016 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:48:22 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0019 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:48:24 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.521301 (0.052172 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:48:24 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049220 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:48:24 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:48:24 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:48:24 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.36s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.254\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.650\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.143\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.152\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.335\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.147\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.393\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.395\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.484\n",
            "\u001b[32m[01/11 01:48:25 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 25.448 | 64.991 | 14.296 | 0.000 | 15.177 | 33.504 |\n",
            "\u001b[32m[01/11 01:48:25 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 25.448 |\n",
            "\u001b[32m[01/11 01:48:25 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:48:25 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:48:25 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:48:25 d2.evaluation.testing]: \u001b[0mcopypaste: 25.4483,64.9909,14.2965,0.0000,15.1775,33.5037\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:48:32 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:48:32 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:48:32 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:48:32 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:48:32 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:48:32 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:48:32 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:48:32 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:48:33 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0508 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:48:38 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0019 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:48:43 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0018 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:48:45 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.578878 (0.052412 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:48:45 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049565 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:48:45 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:48:45 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:48:45 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.36s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.287\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.670\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.195\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.159\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.380\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.159\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.426\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.467\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.418\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.522\n",
            "\u001b[32m[01/11 01:48:46 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 28.653 | 67.011 | 19.511 | 0.000 | 15.907 | 37.984 |\n",
            "\u001b[32m[01/11 01:48:46 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 28.653 |\n",
            "\u001b[32m[01/11 01:48:46 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:48:46 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:48:46 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:48:46 d2.evaluation.testing]: \u001b[0mcopypaste: 28.6526,67.0106,19.5112,0.0000,15.9074,37.9843\n",
            "\u001b[32m[01/11 01:48:46 d2.utils.events]: \u001b[0m eta: 0:54:48  iter: 719  total_loss: 0.7091  loss_cls: 0.1513  loss_box_reg: 0.4653  loss_rpn_cls: 0.01845  loss_rpn_loc: 0.01531    time: 0.7726  last_time: 0.7838  data_time: 0.0331  last_data_time: 0.0282   lr: 0.00017982  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:48:54 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:48:54 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:48:54 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:48:54 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:48:54 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:48:54 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:48:54 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:48:54 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:48:55 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0047 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0540 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:49:00 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0022 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:49:05 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0021 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:49:07 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.529298 (0.052205 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:49:07 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049095 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:49:08 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:49:08 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:49:08 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.41s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.284\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.656\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.196\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.159\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.381\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.160\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.428\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.472\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.421\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.528\n",
            "\u001b[32m[01/11 01:49:08 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 28.434 | 65.627 | 19.647 | 0.000 | 15.859 | 38.149 |\n",
            "\u001b[32m[01/11 01:49:08 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 28.434 |\n",
            "\u001b[32m[01/11 01:49:08 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:49:08 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:49:08 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:49:08 d2.evaluation.testing]: \u001b[0mcopypaste: 28.4337,65.6270,19.6471,0.0000,15.8591,38.1490\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:49:16 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:49:16 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:49:16 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:49:16 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:49:16 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:49:16 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:49:16 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:49:16 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:49:17 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0512 s/iter. Eval: 0.0003 s/iter. Total: 0.0526 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:49:22 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0022 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:49:27 d2.evaluation.evaluator]: \u001b[0mInference done 202/245. Dataloading: 0.0019 s/iter. Inference: 0.0504 s/iter. Eval: 0.0003 s/iter. Total: 0.0526 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:49:29 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.726325 (0.053026 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:49:29 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050226 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:49:29 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:49:29 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:49:29 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.38s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.38s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.280\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.655\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.198\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.158\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.371\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.158\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.427\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.472\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.434\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.519\n",
            "\u001b[32m[01/11 01:49:30 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 27.953 | 65.489 | 19.759 | 0.000 | 15.819 | 37.089 |\n",
            "\u001b[32m[01/11 01:49:30 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 27.953 |\n",
            "\u001b[32m[01/11 01:49:30 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:49:30 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:49:30 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:49:30 d2.evaluation.testing]: \u001b[0mcopypaste: 27.9527,65.4885,19.7585,0.0000,15.8193,37.0894\n",
            "\u001b[32m[01/11 01:49:30 d2.utils.events]: \u001b[0m eta: 0:54:34  iter: 739  total_loss: 0.7121  loss_cls: 0.1816  loss_box_reg: 0.4571  loss_rpn_cls: 0.02982  loss_rpn_loc: 0.01874    time: 0.7728  last_time: 0.7640  data_time: 0.0349  last_data_time: 0.0216   lr: 0.00018482  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:49:38 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:49:38 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:49:38 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:49:38 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:49:38 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:49:38 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:49:38 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:49:38 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:49:39 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0515 s/iter. Eval: 0.0003 s/iter. Total: 0.0529 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:49:45 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0017 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:49:50 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0015 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 01:49:52 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.436555 (0.051819 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:49:52 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049249 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:49:52 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:49:52 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:49:52 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.29s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.281\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.660\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.195\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.165\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.369\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.158\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.426\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.460\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.416\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.512\n",
            "\u001b[32m[01/11 01:49:52 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 28.055 | 66.007 | 19.498 | 0.000 | 16.467 | 36.850 |\n",
            "\u001b[32m[01/11 01:49:52 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 28.055 |\n",
            "\u001b[32m[01/11 01:49:52 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:49:52 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:49:52 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:49:52 d2.evaluation.testing]: \u001b[0mcopypaste: 28.0551,66.0071,19.4979,0.0000,16.4674,36.8503\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:50:00 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:50:00 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:50:00 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:50:00 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:50:00 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:50:00 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:50:00 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:50:00 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:50:01 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0015 s/iter. Inference: 0.0513 s/iter. Eval: 0.0003 s/iter. Total: 0.0531 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:50:06 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0017 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:50:11 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0020 s/iter. Inference: 0.0500 s/iter. Eval: 0.0003 s/iter. Total: 0.0523 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:50:13 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.655607 (0.052732 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:50:13 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049849 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:50:13 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:50:13 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:50:13 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.32s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.276\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.664\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.182\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.163\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.361\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.156\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.421\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.459\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.416\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.509\n",
            "\u001b[32m[01/11 01:50:14 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 27.624 | 66.437 | 18.167 | 0.000 | 16.335 | 36.067 |\n",
            "\u001b[32m[01/11 01:50:14 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 27.624 |\n",
            "\u001b[32m[01/11 01:50:14 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:50:14 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:50:14 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:50:14 d2.evaluation.testing]: \u001b[0mcopypaste: 27.6245,66.4373,18.1673,0.0000,16.3345,36.0669\n",
            "\u001b[32m[01/11 01:50:14 d2.utils.events]: \u001b[0m eta: 0:54:19  iter: 759  total_loss: 0.7119  loss_cls: 0.1718  loss_box_reg: 0.4568  loss_rpn_cls: 0.02461  loss_rpn_loc: 0.02079    time: 0.7728  last_time: 0.7792  data_time: 0.0339  last_data_time: 0.0329   lr: 0.00018981  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:50:22 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:50:22 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:50:22 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:50:22 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:50:22 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:50:22 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:50:22 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:50:22 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:50:23 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0036 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0538 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:50:28 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0017 s/iter. Inference: 0.0500 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:50:33 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0016 s/iter. Inference: 0.0504 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:50:35 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.765311 (0.053189 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:50:35 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050528 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:50:36 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:50:36 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:50:36 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.40s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.290\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.664\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.200\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.165\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.379\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.158\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.435\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.479\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.428\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.535\n",
            "\u001b[32m[01/11 01:50:36 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 28.959 | 66.407 | 19.975 | 0.000 | 16.465 | 37.877 |\n",
            "\u001b[32m[01/11 01:50:36 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 28.959 |\n",
            "\u001b[32m[01/11 01:50:36 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:50:36 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:50:36 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:50:36 d2.evaluation.testing]: \u001b[0mcopypaste: 28.9594,66.4067,19.9749,0.0000,16.4653,37.8768\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:50:44 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:50:44 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:50:44 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:50:44 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:50:44 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:50:44 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:50:44 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:50:44 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:50:45 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:50:50 d2.evaluation.evaluator]: \u001b[0mInference done 111/245. Dataloading: 0.0015 s/iter. Inference: 0.0484 s/iter. Eval: 0.0003 s/iter. Total: 0.0503 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:50:55 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0016 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 01:50:57 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.450677 (0.051878 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:50:57 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049271 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:50:57 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:50:57 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:50:57 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.36s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.284\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.667\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.188\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.164\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.373\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.160\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.434\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.470\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.425\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.523\n",
            "\u001b[32m[01/11 01:50:58 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 28.424 | 66.686 | 18.811 | 0.000 | 16.385 | 37.315 |\n",
            "\u001b[32m[01/11 01:50:58 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 28.424 |\n",
            "\u001b[32m[01/11 01:50:58 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:50:58 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:50:58 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:50:58 d2.evaluation.testing]: \u001b[0mcopypaste: 28.4238,66.6862,18.8113,0.0000,16.3855,37.3148\n",
            "\u001b[32m[01/11 01:50:58 d2.utils.events]: \u001b[0m eta: 0:54:04  iter: 779  total_loss: 0.6719  loss_cls: 0.1456  loss_box_reg: 0.4829  loss_rpn_cls: 0.02334  loss_rpn_loc: 0.02055    time: 0.7731  last_time: 0.7694  data_time: 0.0323  last_data_time: 0.0237   lr: 0.00019481  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:51:06 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:51:06 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:51:06 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:51:06 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:51:06 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:51:06 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:51:06 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:51:06 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:51:07 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0523 s/iter. Eval: 0.0003 s/iter. Total: 0.0534 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:51:12 d2.evaluation.evaluator]: \u001b[0mInference done 104/245. Dataloading: 0.0015 s/iter. Inference: 0.0522 s/iter. Eval: 0.0003 s/iter. Total: 0.0541 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:51:17 d2.evaluation.evaluator]: \u001b[0mInference done 199/245. Dataloading: 0.0017 s/iter. Inference: 0.0516 s/iter. Eval: 0.0003 s/iter. Total: 0.0536 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:51:19 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.984784 (0.054103 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:51:19 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.051327 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:51:19 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:51:19 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:51:20 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.35s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.283\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.669\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.184\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.166\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.374\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.160\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.428\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.467\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.420\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.520\n",
            "\u001b[32m[01/11 01:51:20 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 28.346 | 66.875 | 18.369 | 0.000 | 16.559 | 37.364 |\n",
            "\u001b[32m[01/11 01:51:20 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 28.346 |\n",
            "\u001b[32m[01/11 01:51:20 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:51:20 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:51:20 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:51:20 d2.evaluation.testing]: \u001b[0mcopypaste: 28.3457,66.8749,18.3691,0.0000,16.5589,37.3641\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:51:28 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:51:28 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:51:28 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:51:28 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:51:28 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:51:28 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:51:28 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:51:28 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:51:29 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0514 s/iter. Eval: 0.0003 s/iter. Total: 0.0527 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:51:34 d2.evaluation.evaluator]: \u001b[0mInference done 105/245. Dataloading: 0.0020 s/iter. Inference: 0.0513 s/iter. Eval: 0.0003 s/iter. Total: 0.0537 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:51:39 d2.evaluation.evaluator]: \u001b[0mInference done 202/245. Dataloading: 0.0019 s/iter. Inference: 0.0505 s/iter. Eval: 0.0003 s/iter. Total: 0.0527 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:51:41 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.803433 (0.053348 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:51:41 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050346 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:51:41 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:51:41 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:51:41 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.44s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.283\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.656\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.180\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.159\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.380\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.430\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.476\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.429\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.530\n",
            "\u001b[32m[01/11 01:51:42 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 28.330 | 65.647 | 18.015 | 0.000 | 15.866 | 37.950 |\n",
            "\u001b[32m[01/11 01:51:42 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 28.330 |\n",
            "\u001b[32m[01/11 01:51:42 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:51:42 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:51:42 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:51:42 d2.evaluation.testing]: \u001b[0mcopypaste: 28.3296,65.6466,18.0151,0.0000,15.8658,37.9503\n",
            "\u001b[32m[01/11 01:51:42 d2.utils.events]: \u001b[0m eta: 0:53:49  iter: 799  total_loss: 0.7164  loss_cls: 0.1667  loss_box_reg: 0.4515  loss_rpn_cls: 0.03133  loss_rpn_loc: 0.02328    time: 0.7729  last_time: 0.7638  data_time: 0.0317  last_data_time: 0.0223   lr: 0.0001998  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:51:50 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:51:50 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:51:50 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:51:50 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:51:50 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:51:50 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:51:50 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:51:50 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:51:51 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0507 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:51:56 d2.evaluation.evaluator]: \u001b[0mInference done 99/245. Dataloading: 0.0017 s/iter. Inference: 0.0548 s/iter. Eval: 0.0003 s/iter. Total: 0.0569 s/iter. ETA=0:00:08\n",
            "\u001b[32m[01/11 01:52:01 d2.evaluation.evaluator]: \u001b[0mInference done 198/245. Dataloading: 0.0015 s/iter. Inference: 0.0519 s/iter. Eval: 0.0003 s/iter. Total: 0.0538 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:52:04 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.947656 (0.053949 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:52:04 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.051344 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:52:04 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:52:04 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:52:04 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.40s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.266\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.662\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.159\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.160\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.355\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.155\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.409\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.455\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.426\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.497\n",
            "\u001b[32m[01/11 01:52:04 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 26.636 | 66.154 | 15.921 | 0.000 | 15.956 | 35.461 |\n",
            "\u001b[32m[01/11 01:52:04 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 26.636 |\n",
            "\u001b[32m[01/11 01:52:04 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:52:04 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:52:04 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:52:04 d2.evaluation.testing]: \u001b[0mcopypaste: 26.6364,66.1535,15.9211,0.0000,15.9563,35.4609\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:52:12 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:52:12 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:52:12 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:52:12 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:52:12 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:52:12 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:52:12 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:52:12 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:52:13 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0503 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:52:18 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0019 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:52:23 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0020 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:52:26 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.614240 (0.052559 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:52:26 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049535 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:52:26 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:52:26 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:52:26 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.33s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.302\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.679\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.196\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.171\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.395\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.165\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.441\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.475\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.420\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.534\n",
            "\u001b[32m[01/11 01:52:26 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.225 | 67.924 | 19.611 | 0.000 | 17.100 | 39.495 |\n",
            "\u001b[32m[01/11 01:52:26 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.225 |\n",
            "\u001b[32m[01/11 01:52:26 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:52:26 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:52:26 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:52:26 d2.evaluation.testing]: \u001b[0mcopypaste: 30.2254,67.9244,19.6107,0.0000,17.0995,39.4951\n",
            "\u001b[32m[01/11 01:52:26 d2.utils.events]: \u001b[0m eta: 0:53:33  iter: 819  total_loss: 0.6844  loss_cls: 0.1604  loss_box_reg: 0.476  loss_rpn_cls: 0.03298  loss_rpn_loc: 0.01995    time: 0.7729  last_time: 0.7823  data_time: 0.0327  last_data_time: 0.0348   lr: 0.0002048  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:52:34 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:52:34 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:52:34 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:52:34 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:52:34 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:52:34 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:52:34 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:52:34 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:52:36 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0511 s/iter. Eval: 0.0003 s/iter. Total: 0.0523 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:52:41 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0017 s/iter. Inference: 0.0487 s/iter. Eval: 0.0003 s/iter. Total: 0.0508 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:52:46 d2.evaluation.evaluator]: \u001b[0mInference done 211/245. Dataloading: 0.0016 s/iter. Inference: 0.0483 s/iter. Eval: 0.0003 s/iter. Total: 0.0502 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 01:52:47 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.240889 (0.051004 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:52:47 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048450 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:52:47 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:52:47 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:52:48 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.36s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.287\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.668\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.181\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.161\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.376\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.161\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.427\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.467\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.423\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.519\n",
            "\u001b[32m[01/11 01:52:48 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 28.712 | 66.813 | 18.058 | 0.000 | 16.099 | 37.597 |\n",
            "\u001b[32m[01/11 01:52:48 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 28.712 |\n",
            "\u001b[32m[01/11 01:52:48 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:52:48 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:52:48 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:52:48 d2.evaluation.testing]: \u001b[0mcopypaste: 28.7117,66.8127,18.0582,0.0000,16.0989,37.5965\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:52:55 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:52:56 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:52:56 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:52:56 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:52:56 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:52:56 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:52:56 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:52:56 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:52:57 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0509 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:53:02 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0014 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:53:07 d2.evaluation.evaluator]: \u001b[0mInference done 210/245. Dataloading: 0.0015 s/iter. Inference: 0.0488 s/iter. Eval: 0.0003 s/iter. Total: 0.0507 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 01:53:09 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.254487 (0.051060 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:53:09 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048536 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:53:09 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:53:09 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:53:09 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.39s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.292\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.670\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.207\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.161\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.390\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.160\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.434\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.476\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.430\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.529\n",
            "\u001b[32m[01/11 01:53:09 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.227 | 67.004 | 20.745 | 0.000 | 16.128 | 38.953 |\n",
            "\u001b[32m[01/11 01:53:09 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.227 |\n",
            "\u001b[32m[01/11 01:53:09 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:53:09 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:53:09 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:53:09 d2.evaluation.testing]: \u001b[0mcopypaste: 29.2272,67.0042,20.7450,0.0000,16.1282,38.9533\n",
            "\u001b[32m[01/11 01:53:09 d2.utils.events]: \u001b[0m eta: 0:53:18  iter: 839  total_loss: 0.674  loss_cls: 0.1568  loss_box_reg: 0.465  loss_rpn_cls: 0.02832  loss_rpn_loc: 0.01953    time: 0.7727  last_time: 0.6707  data_time: 0.0321  last_data_time: 0.0219   lr: 0.00020979  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:53:17 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:53:17 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:53:17 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:53:17 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:53:17 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:53:17 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:53:17 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:53:17 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:53:18 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0479 s/iter. Eval: 0.0002 s/iter. Total: 0.0490 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:53:23 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0020 s/iter. Inference: 0.0486 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:53:28 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0018 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:53:31 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.581472 (0.052423 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:53:31 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049400 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:53:31 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:53:31 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:53:31 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.33s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.299\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.677\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.205\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.165\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.391\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.164\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.434\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.468\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.410\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.529\n",
            "\u001b[32m[01/11 01:53:31 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.940 | 67.746 | 20.481 | 0.000 | 16.505 | 39.095 |\n",
            "\u001b[32m[01/11 01:53:31 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.940 |\n",
            "\u001b[32m[01/11 01:53:31 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:53:31 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:53:31 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:53:31 d2.evaluation.testing]: \u001b[0mcopypaste: 29.9400,67.7458,20.4806,0.0000,16.5050,39.0947\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:53:39 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:53:39 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:53:39 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:53:39 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:53:39 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:53:39 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:53:39 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:53:39 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:53:40 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0033 s/iter. Inference: 0.0508 s/iter. Eval: 0.0003 s/iter. Total: 0.0544 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:53:45 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0020 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:53:50 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0017 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:53:52 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.557189 (0.052322 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:53:52 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049701 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:53:52 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:53:52 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:53:52 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.37s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.292\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.682\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.194\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.165\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.379\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.159\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.431\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.468\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.420\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.521\n",
            "\u001b[32m[01/11 01:53:52 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.248 | 68.191 | 19.399 | 0.000 | 16.458 | 37.851 |\n",
            "\u001b[32m[01/11 01:53:52 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.248 |\n",
            "\u001b[32m[01/11 01:53:52 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:53:52 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:53:52 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:53:52 d2.evaluation.testing]: \u001b[0mcopypaste: 29.2476,68.1908,19.3990,0.0000,16.4582,37.8512\n",
            "\u001b[32m[01/11 01:53:52 d2.utils.events]: \u001b[0m eta: 0:53:03  iter: 859  total_loss: 0.6791  loss_cls: 0.1608  loss_box_reg: 0.451  loss_rpn_cls: 0.02565  loss_rpn_loc: 0.0142    time: 0.7725  last_time: 0.6134  data_time: 0.0359  last_data_time: 0.0248   lr: 0.00021479  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:54:01 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:54:01 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:54:01 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:54:01 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:54:01 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:54:01 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:54:01 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:54:01 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:54:02 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0488 s/iter. Eval: 0.0003 s/iter. Total: 0.0502 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:54:07 d2.evaluation.evaluator]: \u001b[0mInference done 100/245. Dataloading: 0.0016 s/iter. Inference: 0.0543 s/iter. Eval: 0.0003 s/iter. Total: 0.0563 s/iter. ETA=0:00:08\n",
            "\u001b[32m[01/11 01:54:12 d2.evaluation.evaluator]: \u001b[0mInference done 198/245. Dataloading: 0.0016 s/iter. Inference: 0.0518 s/iter. Eval: 0.0003 s/iter. Total: 0.0538 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:54:14 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.947994 (0.053950 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:54:14 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.051382 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:54:14 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:54:14 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:54:15 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.34s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.257\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.655\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.148\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.149\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.340\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.147\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.400\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.447\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.410\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.493\n",
            "\u001b[32m[01/11 01:54:15 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 25.714 | 65.452 | 14.763 | 0.000 | 14.911 | 33.984 |\n",
            "\u001b[32m[01/11 01:54:15 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 25.714 |\n",
            "\u001b[32m[01/11 01:54:15 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:54:15 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:54:15 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:54:15 d2.evaluation.testing]: \u001b[0mcopypaste: 25.7144,65.4523,14.7629,0.0000,14.9107,33.9843\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:54:23 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:54:23 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:54:23 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:54:23 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:54:23 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:54:23 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:54:23 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:54:23 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:54:24 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0017 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:54:29 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0024 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:54:34 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0022 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:54:36 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.702271 (0.052926 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:54:36 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049846 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:54:36 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:54:36 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:54:36 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.34s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.285\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.648\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.186\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.160\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.388\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.164\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.435\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.471\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.423\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.526\n",
            "\u001b[32m[01/11 01:54:37 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 28.467 | 64.804 | 18.579 | 0.000 | 15.959 | 38.753 |\n",
            "\u001b[32m[01/11 01:54:37 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 28.467 |\n",
            "\u001b[32m[01/11 01:54:37 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:54:37 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:54:37 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:54:37 d2.evaluation.testing]: \u001b[0mcopypaste: 28.4668,64.8039,18.5786,0.0000,15.9588,38.7526\n",
            "\u001b[32m[01/11 01:54:37 d2.utils.events]: \u001b[0m eta: 0:52:47  iter: 879  total_loss: 0.685  loss_cls: 0.1624  loss_box_reg: 0.4756  loss_rpn_cls: 0.02125  loss_rpn_loc: 0.02147    time: 0.7727  last_time: 0.6513  data_time: 0.0357  last_data_time: 0.0256   lr: 0.00021978  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:54:45 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:54:45 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:54:45 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:54:45 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:54:45 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:54:45 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:54:45 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:54:45 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:54:46 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0482 s/iter. Eval: 0.0003 s/iter. Total: 0.0496 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:54:51 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0016 s/iter. Inference: 0.0487 s/iter. Eval: 0.0003 s/iter. Total: 0.0507 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:54:56 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0019 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 01:54:58 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.499157 (0.052080 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:54:58 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049201 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:54:58 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:54:58 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:54:58 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.38s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.276\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.655\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.174\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.157\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.373\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.155\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.423\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.466\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.422\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.518\n",
            "\u001b[32m[01/11 01:54:59 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 27.569 | 65.457 | 17.358 | 0.000 | 15.664 | 37.261 |\n",
            "\u001b[32m[01/11 01:54:59 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 27.569 |\n",
            "\u001b[32m[01/11 01:54:59 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:54:59 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:54:59 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:54:59 d2.evaluation.testing]: \u001b[0mcopypaste: 27.5686,65.4566,17.3580,0.0000,15.6638,37.2606\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:55:07 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:55:07 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:55:07 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:55:07 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:55:07 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:55:07 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:55:07 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:55:07 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:55:08 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0504 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:55:13 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0019 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:55:18 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0018 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:55:20 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.583118 (0.052430 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:55:20 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049531 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:55:20 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:55:20 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:55:20 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.34s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.267\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.657\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.158\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.157\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.360\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.152\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.414\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.455\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.426\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.497\n",
            "\u001b[32m[01/11 01:55:20 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 26.739 | 65.659 | 15.839 | 0.000 | 15.707 | 36.034 |\n",
            "\u001b[32m[01/11 01:55:20 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 26.739 |\n",
            "\u001b[32m[01/11 01:55:20 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:55:20 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:55:20 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:55:20 d2.evaluation.testing]: \u001b[0mcopypaste: 26.7392,65.6587,15.8391,0.0000,15.7075,36.0338\n",
            "\u001b[32m[01/11 01:55:20 d2.utils.events]: \u001b[0m eta: 0:52:33  iter: 899  total_loss: 0.7007  loss_cls: 0.1695  loss_box_reg: 0.4625  loss_rpn_cls: 0.02108  loss_rpn_loc: 0.01728    time: 0.7729  last_time: 0.7692  data_time: 0.0340  last_data_time: 0.0244   lr: 0.00022478  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:55:29 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:55:29 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:55:29 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:55:29 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:55:29 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:55:29 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:55:29 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:55:29 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:55:30 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0518 s/iter. Eval: 0.0003 s/iter. Total: 0.0531 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:55:35 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0021 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:55:40 d2.evaluation.evaluator]: \u001b[0mInference done 202/245. Dataloading: 0.0022 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0525 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:55:42 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.682146 (0.052842 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:55:42 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049667 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:55:42 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:55:42 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:55:42 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.29s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.301\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.680\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.207\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.172\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.394\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.438\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.474\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.422\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.531\n",
            "\u001b[32m[01/11 01:55:42 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.140 | 68.038 | 20.664 | 0.000 | 17.172 | 39.445 |\n",
            "\u001b[32m[01/11 01:55:42 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.140 |\n",
            "\u001b[32m[01/11 01:55:42 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:55:42 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:55:42 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:55:42 d2.evaluation.testing]: \u001b[0mcopypaste: 30.1401,68.0376,20.6642,0.0000,17.1725,39.4449\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:55:50 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:55:50 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:55:50 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:55:50 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:55:50 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:55:50 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:55:50 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:55:50 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:55:51 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0483 s/iter. Eval: 0.0003 s/iter. Total: 0.0495 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:55:56 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0016 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0509 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:56:01 d2.evaluation.evaluator]: \u001b[0mInference done 210/245. Dataloading: 0.0016 s/iter. Inference: 0.0487 s/iter. Eval: 0.0003 s/iter. Total: 0.0506 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 01:56:03 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.280796 (0.051170 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:56:03 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048547 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:56:03 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:56:03 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:56:03 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.32s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.277\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.669\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.178\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.161\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.363\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.155\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.416\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.456\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.426\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.498\n",
            "\u001b[32m[01/11 01:56:04 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 27.684 | 66.902 | 17.760 | 0.000 | 16.145 | 36.348 |\n",
            "\u001b[32m[01/11 01:56:04 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 27.684 |\n",
            "\u001b[32m[01/11 01:56:04 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:56:04 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:56:04 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:56:04 d2.evaluation.testing]: \u001b[0mcopypaste: 27.6839,66.9021,17.7605,0.0000,16.1445,36.3478\n",
            "\u001b[32m[01/11 01:56:04 d2.utils.events]: \u001b[0m eta: 0:52:17  iter: 919  total_loss: 0.6135  loss_cls: 0.154  loss_box_reg: 0.4158  loss_rpn_cls: 0.024  loss_rpn_loc: 0.01743    time: 0.7729  last_time: 0.7055  data_time: 0.0346  last_data_time: 0.0323   lr: 0.00022977  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:56:12 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:56:12 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:56:12 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:56:12 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:56:12 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:56:12 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:56:12 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:56:12 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:56:13 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0475 s/iter. Eval: 0.0003 s/iter. Total: 0.0488 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:56:18 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0015 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0509 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:56:23 d2.evaluation.evaluator]: \u001b[0mInference done 209/245. Dataloading: 0.0015 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0509 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 01:56:25 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.380660 (0.051586 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:56:25 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049065 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:56:25 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:56:25 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:56:25 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.70s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.290\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.677\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.191\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.168\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.378\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.161\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.426\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.458\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.422\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.505\n",
            "\u001b[32m[01/11 01:56:26 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.039 | 67.656 | 19.133 | 0.000 | 16.750 | 37.775 |\n",
            "\u001b[32m[01/11 01:56:26 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.039 |\n",
            "\u001b[32m[01/11 01:56:26 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:56:26 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:56:26 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:56:26 d2.evaluation.testing]: \u001b[0mcopypaste: 29.0391,67.6556,19.1328,0.0000,16.7501,37.7749\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:56:33 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:56:33 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:56:33 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:56:33 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:56:33 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:56:33 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:56:33 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:56:33 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:56:34 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0013 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:56:39 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0019 s/iter. Inference: 0.0506 s/iter. Eval: 0.0003 s/iter. Total: 0.0528 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:56:45 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0018 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:56:47 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.550204 (0.052293 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:56:47 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049497 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:56:47 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:56:47 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:56:47 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.31s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.320\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.688\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.235\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.185\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.413\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.170\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.450\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.489\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.429\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.552\n",
            "\u001b[32m[01/11 01:56:47 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.030 | 68.839 | 23.495 | 0.000 | 18.501 | 41.270 |\n",
            "\u001b[32m[01/11 01:56:47 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.030 |\n",
            "\u001b[32m[01/11 01:56:47 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:56:47 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:56:47 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:56:47 d2.evaluation.testing]: \u001b[0mcopypaste: 32.0297,68.8394,23.4949,0.0000,18.5014,41.2696\n",
            "\u001b[32m[01/11 01:56:47 d2.utils.events]: \u001b[0m eta: 0:52:02  iter: 939  total_loss: 0.6928  loss_cls: 0.1593  loss_box_reg: 0.4514  loss_rpn_cls: 0.02702  loss_rpn_loc: 0.01961    time: 0.7730  last_time: 0.7788  data_time: 0.0357  last_data_time: 0.0272   lr: 0.00023477  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:56:55 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:56:55 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:56:55 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:56:55 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:56:55 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:56:55 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:56:55 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:56:55 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:56:56 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0473 s/iter. Eval: 0.0003 s/iter. Total: 0.0487 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:57:01 d2.evaluation.evaluator]: \u001b[0mInference done 105/245. Dataloading: 0.0016 s/iter. Inference: 0.0511 s/iter. Eval: 0.0003 s/iter. Total: 0.0530 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:57:06 d2.evaluation.evaluator]: \u001b[0mInference done 201/245. Dataloading: 0.0015 s/iter. Inference: 0.0508 s/iter. Eval: 0.0003 s/iter. Total: 0.0526 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:57:09 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.756071 (0.053150 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:57:09 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050614 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:57:09 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:57:09 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:57:09 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.32s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.292\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.685\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.213\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.179\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.369\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.157\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.427\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.466\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.432\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.511\n",
            "\u001b[32m[01/11 01:57:09 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.242 | 68.529 | 21.329 | 0.000 | 17.859 | 36.879 |\n",
            "\u001b[32m[01/11 01:57:09 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.242 |\n",
            "\u001b[32m[01/11 01:57:09 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:57:09 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:57:09 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:57:09 d2.evaluation.testing]: \u001b[0mcopypaste: 29.2424,68.5288,21.3287,0.0000,17.8587,36.8795\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:57:17 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:57:17 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:57:17 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:57:17 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:57:17 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:57:17 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:57:17 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:57:17 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:57:18 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0486 s/iter. Eval: 0.0003 s/iter. Total: 0.0498 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:57:23 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0020 s/iter. Inference: 0.0500 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:57:28 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0019 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0525 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:57:30 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.781978 (0.053258 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:57:30 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050388 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:57:30 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:57:30 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:57:30 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.36s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.302\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.690\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.206\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.184\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.380\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.432\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.470\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.425\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.523\n",
            "\u001b[32m[01/11 01:57:31 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.172 | 69.035 | 20.632 | 0.000 | 18.396 | 37.974 |\n",
            "\u001b[32m[01/11 01:57:31 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.172 |\n",
            "\u001b[32m[01/11 01:57:31 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:57:31 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:57:31 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:57:31 d2.evaluation.testing]: \u001b[0mcopypaste: 30.1724,69.0352,20.6317,0.0000,18.3962,37.9742\n",
            "\u001b[32m[01/11 01:57:31 d2.utils.events]: \u001b[0m eta: 0:51:47  iter: 959  total_loss: 0.6468  loss_cls: 0.1543  loss_box_reg: 0.4183  loss_rpn_cls: 0.02207  loss_rpn_loc: 0.02236    time: 0.7729  last_time: 0.7717  data_time: 0.0322  last_data_time: 0.0286   lr: 0.00023976  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:57:39 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:57:39 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:57:39 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:57:39 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:57:39 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:57:39 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:57:39 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:57:39 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:57:40 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:57:45 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0016 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:57:50 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0016 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 01:57:52 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.431170 (0.051797 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:57:52 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049175 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:57:52 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:57:52 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:57:52 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.33s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.302\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.696\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.205\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.182\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.381\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.163\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.432\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.471\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.434\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.518\n",
            "\u001b[32m[01/11 01:57:53 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.179 | 69.620 | 20.543 | 0.000 | 18.234 | 38.105 |\n",
            "\u001b[32m[01/11 01:57:53 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.179 |\n",
            "\u001b[32m[01/11 01:57:53 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:57:53 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:57:53 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:57:53 d2.evaluation.testing]: \u001b[0mcopypaste: 30.1793,69.6198,20.5431,0.0000,18.2343,38.1049\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:58:01 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:58:01 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:58:01 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:58:01 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:58:01 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:58:01 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:58:01 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:58:01 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:58:02 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0503 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:58:07 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0014 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:58:12 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0015 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:58:14 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.615908 (0.052566 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:58:14 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050009 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:58:14 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:58:14 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:58:14 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.33s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.302\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.694\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.207\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.182\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.381\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.164\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.435\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.471\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.430\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.522\n",
            "\u001b[32m[01/11 01:58:14 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.208 | 69.357 | 20.683 | 0.000 | 18.220 | 38.134 |\n",
            "\u001b[32m[01/11 01:58:14 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.208 |\n",
            "\u001b[32m[01/11 01:58:14 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:58:14 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:58:14 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:58:14 d2.evaluation.testing]: \u001b[0mcopypaste: 30.2077,69.3573,20.6831,0.0000,18.2201,38.1344\n",
            "\u001b[32m[01/11 01:58:14 d2.utils.events]: \u001b[0m eta: 0:51:32  iter: 979  total_loss: 0.6321  loss_cls: 0.1567  loss_box_reg: 0.4125  loss_rpn_cls: 0.03445  loss_rpn_loc: 0.02182    time: 0.7730  last_time: 0.7713  data_time: 0.0330  last_data_time: 0.0263   lr: 0.00024476  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:58:23 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:58:23 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:58:23 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:58:23 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:58:23 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:58:23 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:58:23 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:58:23 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:58:24 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0027 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0526 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:58:29 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0016 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:58:34 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0019 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:58:36 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.554451 (0.052310 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:58:36 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049535 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:58:36 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:58:36 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:58:36 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.35s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.287\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.681\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.190\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.164\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.374\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.157\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.427\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.470\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.429\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.520\n",
            "\u001b[32m[01/11 01:58:36 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 28.686 | 68.128 | 19.043 | 0.000 | 16.405 | 37.380 |\n",
            "\u001b[32m[01/11 01:58:36 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 28.686 |\n",
            "\u001b[32m[01/11 01:58:36 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:58:36 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:58:36 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:58:36 d2.evaluation.testing]: \u001b[0mcopypaste: 28.6856,68.1276,19.0432,0.0000,16.4055,37.3803\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:58:44 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:58:44 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:58:44 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:58:44 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:58:44 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:58:44 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:58:44 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:58:44 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:58:45 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0015 s/iter. Inference: 0.0480 s/iter. Eval: 0.0002 s/iter. Total: 0.0498 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:58:50 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0022 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:58:55 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0020 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 01:58:57 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.443686 (0.051849 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:58:57 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048943 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:58:57 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:58:57 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:58:57 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.42s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.34s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.291\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.672\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.198\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.164\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.386\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.159\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.426\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.474\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.430\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.526\n",
            "\u001b[32m[01/11 01:58:58 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.053 | 67.248 | 19.771 | 0.000 | 16.374 | 38.591 |\n",
            "\u001b[32m[01/11 01:58:58 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.053 |\n",
            "\u001b[32m[01/11 01:58:58 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:58:58 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:58:58 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:58:58 d2.evaluation.testing]: \u001b[0mcopypaste: 29.0535,67.2476,19.7714,0.0000,16.3743,38.5907\n",
            "\u001b[32m[01/11 01:58:58 d2.utils.events]: \u001b[0m eta: 0:51:16  iter: 999  total_loss: 0.6606  loss_cls: 0.1577  loss_box_reg: 0.4682  loss_rpn_cls: 0.02733  loss_rpn_loc: 0.01599    time: 0.7731  last_time: 0.7631  data_time: 0.0334  last_data_time: 0.0239   lr: 0.00024975  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:59:06 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:59:06 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:59:06 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:59:06 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:59:06 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:59:06 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:59:07 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:59:07 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:59:08 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0485 s/iter. Eval: 0.0002 s/iter. Total: 0.0498 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:59:13 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0019 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 01:59:18 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0017 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 01:59:20 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.537230 (0.052238 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:59:20 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049541 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:59:20 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:59:20 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:59:20 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.30s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.295\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.679\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.209\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.168\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.387\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.163\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.433\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.471\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.427\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.522\n",
            "\u001b[32m[01/11 01:59:20 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.532 | 67.868 | 20.863 | 0.000 | 16.838 | 38.740 |\n",
            "\u001b[32m[01/11 01:59:20 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.532 |\n",
            "\u001b[32m[01/11 01:59:20 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:59:20 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:59:20 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:59:20 d2.evaluation.testing]: \u001b[0mcopypaste: 29.5317,67.8679,20.8627,0.0000,16.8382,38.7402\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:59:28 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:59:28 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:59:28 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:59:28 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:59:28 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:59:28 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:59:28 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:59:28 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:59:29 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0485 s/iter. Eval: 0.0003 s/iter. Total: 0.0499 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 01:59:34 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0017 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 01:59:39 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0017 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 01:59:41 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.538554 (0.052244 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:59:41 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049508 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 01:59:41 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 01:59:41 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 01:59:41 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.37s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.309\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.686\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.228\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.169\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.403\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.166\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.444\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.483\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.420\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.547\n",
            "\u001b[32m[01/11 01:59:42 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.878 | 68.589 | 22.818 | 0.000 | 16.895 | 40.275 |\n",
            "\u001b[32m[01/11 01:59:42 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.878 |\n",
            "\u001b[32m[01/11 01:59:42 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 01:59:42 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 01:59:42 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 01:59:42 d2.evaluation.testing]: \u001b[0mcopypaste: 30.8778,68.5886,22.8175,0.0000,16.8955,40.2746\n",
            "\u001b[32m[01/11 01:59:42 d2.utils.events]: \u001b[0m eta: 0:51:01  iter: 1019  total_loss: 0.6735  loss_cls: 0.1444  loss_box_reg: 0.4792  loss_rpn_cls: 0.02318  loss_rpn_loc: 0.02651    time: 0.7730  last_time: 0.7035  data_time: 0.0357  last_data_time: 0.0247   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 01:59:50 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 01:59:50 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 01:59:50 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 01:59:50 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 01:59:50 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 01:59:50 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 01:59:50 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 01:59:50 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 01:59:51 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0503 s/iter. Eval: 0.0002 s/iter. Total: 0.0516 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 01:59:56 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0019 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:00:01 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0018 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:00:03 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.545740 (0.052274 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:00:03 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049419 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:00:03 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:00:03 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:00:03 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.31s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.287\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.684\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.201\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.168\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.369\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.161\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.419\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.463\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.423\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.512\n",
            "\u001b[32m[01/11 02:00:04 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 28.663 | 68.366 | 20.068 | 0.000 | 16.808 | 36.886 |\n",
            "\u001b[32m[01/11 02:00:04 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 28.663 |\n",
            "\u001b[32m[01/11 02:00:04 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:00:04 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:00:04 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:00:04 d2.evaluation.testing]: \u001b[0mcopypaste: 28.6635,68.3664,20.0681,0.0000,16.8077,36.8862\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:00:11 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:00:11 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:00:11 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:00:11 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:00:11 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:00:11 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:00:11 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:00:11 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:00:12 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0026 s/iter. Inference: 0.0483 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:00:17 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0019 s/iter. Inference: 0.0505 s/iter. Eval: 0.0003 s/iter. Total: 0.0527 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:00:22 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0017 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:00:25 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.589625 (0.052457 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:00:25 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049646 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:00:25 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:00:25 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:00:25 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.315\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.692\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.225\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.182\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.407\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.448\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.483\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.425\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.544\n",
            "\u001b[32m[01/11 02:00:25 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.502 | 69.205 | 22.496 | 0.000 | 18.162 | 40.657 |\n",
            "\u001b[32m[01/11 02:00:25 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.502 |\n",
            "\u001b[32m[01/11 02:00:25 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:00:25 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:00:25 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:00:25 d2.evaluation.testing]: \u001b[0mcopypaste: 31.5022,69.2051,22.4962,0.0000,18.1620,40.6574\n",
            "\u001b[32m[01/11 02:00:25 d2.utils.events]: \u001b[0m eta: 0:50:45  iter: 1039  total_loss: 0.6739  loss_cls: 0.1492  loss_box_reg: 0.466  loss_rpn_cls: 0.02381  loss_rpn_loc: 0.01846    time: 0.7728  last_time: 0.6935  data_time: 0.0344  last_data_time: 0.0236   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:00:33 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:00:33 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:00:33 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:00:33 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:00:33 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:00:33 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:00:33 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:00:33 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:00:34 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0033 s/iter. Inference: 0.0543 s/iter. Eval: 0.0003 s/iter. Total: 0.0579 s/iter. ETA=0:00:13\n",
            "\u001b[32m[01/11 02:00:39 d2.evaluation.evaluator]: \u001b[0mInference done 105/245. Dataloading: 0.0018 s/iter. Inference: 0.0516 s/iter. Eval: 0.0003 s/iter. Total: 0.0537 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:00:44 d2.evaluation.evaluator]: \u001b[0mInference done 201/245. Dataloading: 0.0020 s/iter. Inference: 0.0508 s/iter. Eval: 0.0003 s/iter. Total: 0.0531 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:00:47 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.823479 (0.053431 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:00:47 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050434 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:00:47 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:00:47 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:00:47 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.32s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.288\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.680\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.194\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.170\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.370\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.160\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.425\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.460\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.421\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.508\n",
            "\u001b[32m[01/11 02:00:47 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 28.760 | 67.986 | 19.417 | 0.000 | 17.024 | 36.960 |\n",
            "\u001b[32m[01/11 02:00:47 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 28.760 |\n",
            "\u001b[32m[01/11 02:00:47 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:00:47 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:00:47 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:00:47 d2.evaluation.testing]: \u001b[0mcopypaste: 28.7596,67.9857,19.4174,0.0000,17.0244,36.9604\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:00:55 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:00:55 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:00:55 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:00:55 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:00:55 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:00:55 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:00:55 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:00:55 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:00:56 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0511 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:01:01 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0018 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:01:06 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0016 s/iter. Inference: 0.0502 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:01:08 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.665836 (0.052774 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:01:08 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050090 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:01:08 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:01:08 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:01:08 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.32s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.287\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.683\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.195\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.173\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.369\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.160\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.423\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.459\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.420\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.507\n",
            "\u001b[32m[01/11 02:01:09 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 28.735 | 68.289 | 19.495 | 0.000 | 17.313 | 36.943 |\n",
            "\u001b[32m[01/11 02:01:09 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 28.735 |\n",
            "\u001b[32m[01/11 02:01:09 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:01:09 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:01:09 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:01:09 d2.evaluation.testing]: \u001b[0mcopypaste: 28.7351,68.2892,19.4950,0.0000,17.3130,36.9429\n",
            "\u001b[32m[01/11 02:01:09 d2.utils.events]: \u001b[0m eta: 0:50:30  iter: 1059  total_loss: 0.7007  loss_cls: 0.1542  loss_box_reg: 0.4575  loss_rpn_cls: 0.0223  loss_rpn_loc: 0.02022    time: 0.7728  last_time: 0.7694  data_time: 0.0306  last_data_time: 0.0218   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:01:17 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:01:17 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:01:17 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:01:17 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:01:17 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:01:17 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:01:17 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:01:17 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:01:18 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0015 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:01:23 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0019 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0525 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:01:28 d2.evaluation.evaluator]: \u001b[0mInference done 201/245. Dataloading: 0.0017 s/iter. Inference: 0.0508 s/iter. Eval: 0.0003 s/iter. Total: 0.0528 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:01:30 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.748165 (0.053117 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:01:30 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050297 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:01:31 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:01:31 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:01:31 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.37s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.33s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.305\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.682\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.207\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.178\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.396\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.163\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.446\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.479\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.426\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.536\n",
            "\u001b[32m[01/11 02:01:31 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.481 | 68.210 | 20.709 | 0.000 | 17.758 | 39.567 |\n",
            "\u001b[32m[01/11 02:01:31 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.481 |\n",
            "\u001b[32m[01/11 02:01:31 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:01:31 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:01:31 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:01:31 d2.evaluation.testing]: \u001b[0mcopypaste: 30.4811,68.2102,20.7094,0.0000,17.7578,39.5669\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:01:39 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:01:39 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:01:39 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:01:39 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:01:39 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:01:39 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:01:39 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:01:39 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:01:40 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0014 s/iter. Inference: 0.0488 s/iter. Eval: 0.0003 s/iter. Total: 0.0505 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:01:45 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0016 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:01:50 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0018 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:01:53 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.715625 (0.052982 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:01:53 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049990 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:01:53 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:01:53 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:01:53 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.33s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.264\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.669\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.149\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.168\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.334\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.148\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.403\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.445\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.433\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.476\n",
            "\u001b[32m[01/11 02:01:53 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 26.439 | 66.935 | 14.860 | 0.001 | 16.806 | 33.440 |\n",
            "\u001b[32m[01/11 02:01:53 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 26.439 |\n",
            "\u001b[32m[01/11 02:01:53 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:01:53 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:01:53 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:01:53 d2.evaluation.testing]: \u001b[0mcopypaste: 26.4389,66.9352,14.8604,0.0012,16.8059,33.4402\n",
            "\u001b[32m[01/11 02:01:53 d2.utils.events]: \u001b[0m eta: 0:50:15  iter: 1079  total_loss: 0.6505  loss_cls: 0.1648  loss_box_reg: 0.4339  loss_rpn_cls: 0.02094  loss_rpn_loc: 0.01607    time: 0.7728  last_time: 0.7706  data_time: 0.0375  last_data_time: 0.0269   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:02:01 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:02:01 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:02:01 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:02:01 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:02:01 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:02:01 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:02:01 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:02:01 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:02:02 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0015 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:02:07 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0020 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:02:12 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0019 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:02:15 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.516898 (0.052154 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:02:15 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049122 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:02:15 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:02:15 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:02:15 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.33s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.307\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.685\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.211\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.180\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.391\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.441\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.476\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.433\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.528\n",
            "\u001b[32m[01/11 02:02:15 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.697 | 68.469 | 21.069 | 0.000 | 17.983 | 39.117 |\n",
            "\u001b[32m[01/11 02:02:15 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.697 |\n",
            "\u001b[32m[01/11 02:02:15 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:02:15 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:02:15 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:02:15 d2.evaluation.testing]: \u001b[0mcopypaste: 30.6974,68.4692,21.0691,0.0000,17.9829,39.1172\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:02:23 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:02:23 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:02:23 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:02:23 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:02:23 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:02:23 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:02:23 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:02:23 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:02:24 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0035 s/iter. Inference: 0.0535 s/iter. Eval: 0.0004 s/iter. Total: 0.0574 s/iter. ETA=0:00:13\n",
            "\u001b[32m[01/11 02:02:29 d2.evaluation.evaluator]: \u001b[0mInference done 112/245. Dataloading: 0.0015 s/iter. Inference: 0.0485 s/iter. Eval: 0.0003 s/iter. Total: 0.0504 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:02:34 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0015 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:02:36 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.488953 (0.052037 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:02:36 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049478 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:02:36 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:02:36 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:02:36 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.32s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.291\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.683\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.196\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.178\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.368\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.154\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.425\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.463\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.504\n",
            "\u001b[32m[01/11 02:02:37 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.122 | 68.312 | 19.554 | 0.000 | 17.762 | 36.761 |\n",
            "\u001b[32m[01/11 02:02:37 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.122 |\n",
            "\u001b[32m[01/11 02:02:37 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:02:37 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:02:37 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:02:37 d2.evaluation.testing]: \u001b[0mcopypaste: 29.1216,68.3124,19.5537,0.0000,17.7615,36.7614\n",
            "\u001b[32m[01/11 02:02:37 d2.utils.events]: \u001b[0m eta: 0:50:00  iter: 1099  total_loss: 0.6715  loss_cls: 0.1512  loss_box_reg: 0.4259  loss_rpn_cls: 0.02902  loss_rpn_loc: 0.02717    time: 0.7729  last_time: 0.7777  data_time: 0.0370  last_data_time: 0.0356   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:02:45 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:02:45 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:02:45 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:02:45 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:02:45 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:02:45 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:02:45 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:02:45 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:02:46 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0505 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:02:51 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0017 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:02:56 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0016 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:02:58 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.432822 (0.051803 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:02:58 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049362 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:02:58 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:02:58 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:02:58 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.39s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.297\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.695\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.189\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.178\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.376\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.158\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.431\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.471\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.432\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.520\n",
            "\u001b[32m[01/11 02:02:59 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.653 | 69.514 | 18.895 | 0.000 | 17.816 | 37.558 |\n",
            "\u001b[32m[01/11 02:02:59 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.653 |\n",
            "\u001b[32m[01/11 02:02:59 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:02:59 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:02:59 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:02:59 d2.evaluation.testing]: \u001b[0mcopypaste: 29.6525,69.5140,18.8954,0.0000,17.8159,37.5576\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:03:06 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:03:06 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:03:06 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:03:06 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:03:06 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:03:06 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:03:06 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:03:06 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:03:07 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0477 s/iter. Eval: 0.0003 s/iter. Total: 0.0490 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:03:12 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0018 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:03:17 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0018 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:03:19 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.471012 (0.051963 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:03:19 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049305 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:03:19 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:03:19 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:03:19 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.31s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.302\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.685\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.208\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.177\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.383\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.160\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.433\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.466\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.423\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.516\n",
            "\u001b[32m[01/11 02:03:20 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.246 | 68.462 | 20.832 | 0.000 | 17.686 | 38.268 |\n",
            "\u001b[32m[01/11 02:03:20 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.246 |\n",
            "\u001b[32m[01/11 02:03:20 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:03:20 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:03:20 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:03:20 d2.evaluation.testing]: \u001b[0mcopypaste: 30.2459,68.4619,20.8323,0.0000,17.6856,38.2684\n",
            "\u001b[32m[01/11 02:03:20 d2.utils.events]: \u001b[0m eta: 0:49:45  iter: 1119  total_loss: 0.6037  loss_cls: 0.1405  loss_box_reg: 0.4451  loss_rpn_cls: 0.01665  loss_rpn_loc: 0.01325    time: 0.7726  last_time: 0.7742  data_time: 0.0332  last_data_time: 0.0264   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:03:28 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:03:28 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:03:28 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:03:28 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:03:28 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:03:28 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:03:28 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:03:28 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:03:29 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0502 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:03:34 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0019 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:03:39 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0017 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:03:41 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.646634 (0.052694 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:03:41 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049936 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:03:41 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:03:41 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:03:41 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.33s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.308\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.701\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.209\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.180\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.389\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.163\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.438\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.472\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.430\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.522\n",
            "\u001b[32m[01/11 02:03:42 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.844 | 70.062 | 20.861 | 0.000 | 18.018 | 38.947 |\n",
            "\u001b[32m[01/11 02:03:42 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.844 |\n",
            "\u001b[32m[01/11 02:03:42 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:03:42 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:03:42 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:03:42 d2.evaluation.testing]: \u001b[0mcopypaste: 30.8444,70.0622,20.8610,0.0000,18.0177,38.9473\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:03:49 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:03:49 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:03:49 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:03:49 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:03:49 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:03:49 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:03:49 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:03:50 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:03:51 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0524 s/iter. Eval: 0.0003 s/iter. Total: 0.0537 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:03:56 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0020 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:04:01 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0018 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:04:03 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.472912 (0.051970 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:04:03 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049205 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:04:03 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:04:03 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:04:03 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.39s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.279\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.686\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.169\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.170\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.353\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.150\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.411\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.456\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.438\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.491\n",
            "\u001b[32m[01/11 02:04:03 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 27.869 | 68.580 | 16.860 | 0.000 | 16.993 | 35.268 |\n",
            "\u001b[32m[01/11 02:04:03 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 27.869 |\n",
            "\u001b[32m[01/11 02:04:03 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:04:03 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:04:03 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:04:03 d2.evaluation.testing]: \u001b[0mcopypaste: 27.8688,68.5802,16.8596,0.0000,16.9926,35.2682\n",
            "\u001b[32m[01/11 02:04:03 d2.utils.events]: \u001b[0m eta: 0:49:29  iter: 1139  total_loss: 0.6896  loss_cls: 0.1629  loss_box_reg: 0.482  loss_rpn_cls: 0.01713  loss_rpn_loc: 0.01689    time: 0.7724  last_time: 0.7670  data_time: 0.0325  last_data_time: 0.0222   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:04:12 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:04:12 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:04:12 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:04:12 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:04:12 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:04:12 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:04:12 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:04:12 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:04:13 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0013 s/iter. Inference: 0.0511 s/iter. Eval: 0.0002 s/iter. Total: 0.0527 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:04:18 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0017 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:04:23 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0017 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:04:25 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.419913 (0.051750 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:04:25 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049016 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:04:25 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:04:25 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:04:25 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.30s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.307\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.688\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.177\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.392\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.163\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.444\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.479\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.530\n",
            "\u001b[32m[01/11 02:04:25 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.721 | 68.817 | 21.621 | 0.000 | 17.663 | 39.211 |\n",
            "\u001b[32m[01/11 02:04:25 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.721 |\n",
            "\u001b[32m[01/11 02:04:25 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:04:25 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:04:25 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:04:25 d2.evaluation.testing]: \u001b[0mcopypaste: 30.7213,68.8173,21.6213,0.0000,17.6628,39.2112\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:04:33 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:04:33 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:04:33 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:04:33 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:04:33 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:04:33 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:04:33 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:04:33 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:04:34 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0507 s/iter. Eval: 0.0002 s/iter. Total: 0.0521 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:04:39 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0016 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:04:44 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0016 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:04:46 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.480358 (0.052001 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:04:46 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049521 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:04:46 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:04:46 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:04:46 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.294\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.690\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.199\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.179\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.370\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.161\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.426\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.463\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.435\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.505\n",
            "\u001b[32m[01/11 02:04:46 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.413 | 69.001 | 19.893 | 0.000 | 17.931 | 37.048 |\n",
            "\u001b[32m[01/11 02:04:46 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.413 |\n",
            "\u001b[32m[01/11 02:04:46 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:04:46 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:04:46 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:04:46 d2.evaluation.testing]: \u001b[0mcopypaste: 29.4133,69.0010,19.8934,0.0000,17.9311,37.0482\n",
            "\u001b[32m[01/11 02:04:46 d2.utils.events]: \u001b[0m eta: 0:49:14  iter: 1159  total_loss: 0.6614  loss_cls: 0.1623  loss_box_reg: 0.4545  loss_rpn_cls: 0.02854  loss_rpn_loc: 0.01876    time: 0.7724  last_time: 0.7639  data_time: 0.0333  last_data_time: 0.0228   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:04:55 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:04:55 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:04:55 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:04:55 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:04:55 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:04:55 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:04:55 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:04:55 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:04:56 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0022 s/iter. Inference: 0.0505 s/iter. Eval: 0.0003 s/iter. Total: 0.0529 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:05:01 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0021 s/iter. Inference: 0.0486 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:05:06 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0022 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:05:08 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.506102 (0.052109 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:05:08 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049035 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:05:08 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:05:08 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:05:08 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.303\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.685\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.212\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.180\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.381\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.164\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.439\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.472\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.437\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.519\n",
            "\u001b[32m[01/11 02:05:09 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.269 | 68.545 | 21.217 | 0.000 | 17.960 | 38.117 |\n",
            "\u001b[32m[01/11 02:05:09 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.269 |\n",
            "\u001b[32m[01/11 02:05:09 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:05:09 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:05:09 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:05:09 d2.evaluation.testing]: \u001b[0mcopypaste: 30.2686,68.5451,21.2171,0.0000,17.9605,38.1165\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:05:16 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:05:16 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:05:16 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:05:16 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:05:16 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:05:16 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:05:16 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:05:16 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:05:17 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0018 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:05:22 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0022 s/iter. Inference: 0.0485 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:05:27 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0021 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:05:29 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.502769 (0.052095 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:05:29 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049015 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:05:30 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:05:30 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:05:30 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.38s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.706\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.226\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.182\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.406\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.171\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.454\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.498\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.447\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.556\n",
            "\u001b[32m[01/11 02:05:30 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.920 | 70.632 | 22.592 | 0.000 | 18.187 | 40.630 |\n",
            "\u001b[32m[01/11 02:05:30 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.920 |\n",
            "\u001b[32m[01/11 02:05:30 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:05:30 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:05:30 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:05:30 d2.evaluation.testing]: \u001b[0mcopypaste: 31.9198,70.6315,22.5920,0.0000,18.1875,40.6298\n",
            "\u001b[32m[01/11 02:05:30 d2.utils.events]: \u001b[0m eta: 0:48:58  iter: 1179  total_loss: 0.6652  loss_cls: 0.1668  loss_box_reg: 0.4621  loss_rpn_cls: 0.02031  loss_rpn_loc: 0.01876    time: 0.7724  last_time: 0.7668  data_time: 0.0349  last_data_time: 0.0223   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:05:38 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:05:38 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:05:38 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:05:38 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:05:38 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:05:38 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:05:38 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:05:38 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:05:40 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0013 s/iter. Inference: 0.0517 s/iter. Eval: 0.0003 s/iter. Total: 0.0533 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:05:45 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0020 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0523 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:05:50 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0018 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:05:52 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.546181 (0.052276 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:05:52 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049466 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:05:52 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:05:52 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:05:52 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.38s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.286\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.685\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.187\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.168\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.362\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.156\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.425\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.468\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.442\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.510\n",
            "\u001b[32m[01/11 02:05:52 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 28.615 | 68.472 | 18.670 | 0.000 | 16.820 | 36.238 |\n",
            "\u001b[32m[01/11 02:05:52 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 28.615 |\n",
            "\u001b[32m[01/11 02:05:52 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:05:52 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:05:52 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:05:52 d2.evaluation.testing]: \u001b[0mcopypaste: 28.6147,68.4720,18.6704,0.0000,16.8205,36.2378\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:06:00 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:06:00 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:06:00 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:06:00 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:06:00 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:06:00 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:06:00 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:06:00 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:06:01 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0514 s/iter. Eval: 0.0003 s/iter. Total: 0.0526 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:06:06 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0020 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:06:11 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0019 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:06:13 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.452123 (0.051884 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:06:13 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049127 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:06:13 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:06:13 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:06:13 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.70s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.298\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.693\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.202\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.176\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.377\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.164\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.429\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.469\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.431\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.518\n",
            "\u001b[32m[01/11 02:06:14 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.760 | 69.251 | 20.213 | 0.000 | 17.581 | 37.733 |\n",
            "\u001b[32m[01/11 02:06:14 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.760 |\n",
            "\u001b[32m[01/11 02:06:14 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:06:14 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:06:14 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:06:14 d2.evaluation.testing]: \u001b[0mcopypaste: 29.7601,69.2510,20.2129,0.0000,17.5811,37.7331\n",
            "\u001b[32m[01/11 02:06:14 d2.utils.events]: \u001b[0m eta: 0:48:42  iter: 1199  total_loss: 0.756  loss_cls: 0.1735  loss_box_reg: 0.4622  loss_rpn_cls: 0.02297  loss_rpn_loc: 0.02166    time: 0.7723  last_time: 0.7622  data_time: 0.0315  last_data_time: 0.0216   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:06:22 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:06:22 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:06:22 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:06:22 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:06:22 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:06:22 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:06:22 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:06:22 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:06:23 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0041 s/iter. Inference: 0.0515 s/iter. Eval: 0.0003 s/iter. Total: 0.0559 s/iter. ETA=0:00:13\n",
            "\u001b[32m[01/11 02:06:28 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0017 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:06:33 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0015 s/iter. Inference: 0.0504 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:06:36 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.700609 (0.052919 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:06:36 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050288 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:06:36 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:06:36 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:06:36 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.30s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.301\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.690\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.223\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.171\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.387\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.165\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.435\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.473\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.427\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.527\n",
            "\u001b[32m[01/11 02:06:36 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.089 | 69.009 | 22.265 | 0.000 | 17.119 | 38.739 |\n",
            "\u001b[32m[01/11 02:06:36 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.089 |\n",
            "\u001b[32m[01/11 02:06:36 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:06:36 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:06:36 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:06:36 d2.evaluation.testing]: \u001b[0mcopypaste: 30.0890,69.0093,22.2651,0.0000,17.1189,38.7385\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:06:44 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:06:44 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:06:44 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:06:44 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:06:44 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:06:44 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:06:44 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:06:44 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:06:45 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0509 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:06:50 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0015 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:06:55 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0016 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:06:57 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.433045 (0.051804 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:06:57 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049171 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:06:57 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:06:57 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:06:57 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.33s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.300\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.686\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.199\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.177\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.381\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.160\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.431\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.473\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.521\n",
            "\u001b[32m[01/11 02:06:58 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.039 | 68.562 | 19.857 | 0.000 | 17.697 | 38.099 |\n",
            "\u001b[32m[01/11 02:06:58 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.039 |\n",
            "\u001b[32m[01/11 02:06:58 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:06:58 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:06:58 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:06:58 d2.evaluation.testing]: \u001b[0mcopypaste: 30.0387,68.5621,19.8570,0.0000,17.6969,38.0989\n",
            "\u001b[32m[01/11 02:06:58 d2.utils.events]: \u001b[0m eta: 0:48:27  iter: 1219  total_loss: 0.6482  loss_cls: 0.1636  loss_box_reg: 0.4157  loss_rpn_cls: 0.02156  loss_rpn_loc: 0.0164    time: 0.7724  last_time: 0.7626  data_time: 0.0320  last_data_time: 0.0155   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:07:06 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:07:06 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:07:06 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:07:06 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:07:06 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:07:06 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:07:06 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:07:06 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:07:07 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0485 s/iter. Eval: 0.0003 s/iter. Total: 0.0500 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:07:12 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0021 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:07:17 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0022 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:07:19 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.485606 (0.052023 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:07:19 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048913 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:07:19 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:07:19 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:07:19 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.30s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.312\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.696\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.228\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.187\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.390\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.165\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.480\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.440\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.529\n",
            "\u001b[32m[01/11 02:07:20 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.155 | 69.594 | 22.800 | 0.000 | 18.685 | 39.037 |\n",
            "\u001b[32m[01/11 02:07:20 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.155 |\n",
            "\u001b[32m[01/11 02:07:20 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:07:20 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:07:20 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:07:20 d2.evaluation.testing]: \u001b[0mcopypaste: 31.1548,69.5940,22.8002,0.0000,18.6850,39.0375\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:07:27 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:07:27 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:07:27 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:07:27 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:07:27 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:07:27 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:07:27 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:07:27 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:07:28 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0022 s/iter. Inference: 0.0483 s/iter. Eval: 0.0002 s/iter. Total: 0.0508 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:07:33 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0017 s/iter. Inference: 0.0488 s/iter. Eval: 0.0003 s/iter. Total: 0.0508 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:07:38 d2.evaluation.evaluator]: \u001b[0mInference done 209/245. Dataloading: 0.0017 s/iter. Inference: 0.0487 s/iter. Eval: 0.0002 s/iter. Total: 0.0507 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:07:40 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.349787 (0.051457 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:07:40 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048577 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:07:40 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:07:40 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:07:40 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.30s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.298\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.690\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.207\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.184\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.371\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.432\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.468\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.513\n",
            "\u001b[32m[01/11 02:07:41 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.770 | 68.964 | 20.666 | 0.002 | 18.363 | 37.113 |\n",
            "\u001b[32m[01/11 02:07:41 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.770 |\n",
            "\u001b[32m[01/11 02:07:41 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:07:41 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:07:41 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:07:41 d2.evaluation.testing]: \u001b[0mcopypaste: 29.7699,68.9638,20.6659,0.0018,18.3629,37.1134\n",
            "\u001b[32m[01/11 02:07:41 d2.utils.events]: \u001b[0m eta: 0:48:12  iter: 1239  total_loss: 0.6364  loss_cls: 0.145  loss_box_reg: 0.4328  loss_rpn_cls: 0.02047  loss_rpn_loc: 0.0194    time: 0.7725  last_time: 0.7726  data_time: 0.0323  last_data_time: 0.0255   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:07:49 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:07:49 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:07:49 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:07:49 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:07:49 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:07:49 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:07:49 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:07:49 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:07:50 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0013 s/iter. Inference: 0.0487 s/iter. Eval: 0.0002 s/iter. Total: 0.0503 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:07:55 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0022 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:08:00 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0019 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:08:02 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.480674 (0.052003 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:08:02 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049202 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:08:02 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:08:02 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:08:02 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.29s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.294\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.685\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.203\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.183\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.368\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.159\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.430\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.465\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.432\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.509\n",
            "\u001b[32m[01/11 02:08:03 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.387 | 68.488 | 20.294 | 0.001 | 18.283 | 36.809 |\n",
            "\u001b[32m[01/11 02:08:03 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.387 |\n",
            "\u001b[32m[01/11 02:08:03 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:08:03 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:08:03 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:08:03 d2.evaluation.testing]: \u001b[0mcopypaste: 29.3867,68.4880,20.2941,0.0015,18.2832,36.8093\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:08:10 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:08:10 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:08:10 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:08:10 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:08:10 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:08:10 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:08:10 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:08:10 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:08:11 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0013 s/iter. Inference: 0.0508 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:08:16 d2.evaluation.evaluator]: \u001b[0mInference done 111/245. Dataloading: 0.0017 s/iter. Inference: 0.0482 s/iter. Eval: 0.0003 s/iter. Total: 0.0503 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:08:21 d2.evaluation.evaluator]: \u001b[0mInference done 210/245. Dataloading: 0.0017 s/iter. Inference: 0.0485 s/iter. Eval: 0.0003 s/iter. Total: 0.0505 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:08:23 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.302826 (0.051262 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:08:23 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048522 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:08:23 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:08:23 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:08:23 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.707\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.241\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.189\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.400\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.167\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.447\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.475\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.423\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.532\n",
            "\u001b[32m[01/11 02:08:24 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.727 | 70.707 | 24.141 | 0.003 | 18.868 | 40.023 |\n",
            "\u001b[32m[01/11 02:08:24 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.727 |\n",
            "\u001b[32m[01/11 02:08:24 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:08:24 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:08:24 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:08:24 d2.evaluation.testing]: \u001b[0mcopypaste: 31.7272,70.7069,24.1408,0.0031,18.8678,40.0234\n",
            "\u001b[32m[01/11 02:08:24 d2.utils.events]: \u001b[0m eta: 0:47:56  iter: 1259  total_loss: 0.648  loss_cls: 0.1487  loss_box_reg: 0.4482  loss_rpn_cls: 0.03087  loss_rpn_loc: 0.01777    time: 0.7726  last_time: 0.7732  data_time: 0.0364  last_data_time: 0.0273   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:08:32 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:08:32 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:08:32 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:08:32 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:08:32 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:08:32 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:08:32 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:08:32 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:08:33 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0025 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0528 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:08:38 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0017 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:08:43 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0019 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:08:45 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.556251 (0.052318 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:08:45 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049392 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:08:45 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:08:45 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:08:45 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.29s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.282\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.672\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.176\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.175\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.352\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.156\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.418\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.451\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.434\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.486\n",
            "\u001b[32m[01/11 02:08:46 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 28.214 | 67.249 | 17.572 | 0.000 | 17.515 | 35.210 |\n",
            "\u001b[32m[01/11 02:08:46 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 28.214 |\n",
            "\u001b[32m[01/11 02:08:46 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:08:46 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:08:46 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:08:46 d2.evaluation.testing]: \u001b[0mcopypaste: 28.2144,67.2490,17.5724,0.0000,17.5152,35.2096\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:08:54 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:08:54 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:08:54 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:08:54 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:08:54 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:08:54 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:08:54 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:08:54 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:08:55 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0541 s/iter. Eval: 0.0002 s/iter. Total: 0.0554 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:09:00 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0017 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:09:05 d2.evaluation.evaluator]: \u001b[0mInference done 195/245. Dataloading: 0.0018 s/iter. Inference: 0.0527 s/iter. Eval: 0.0003 s/iter. Total: 0.0548 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:09:08 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:13.174644 (0.054894 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:09:08 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.052086 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:09:08 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:09:08 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:09:08 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.292\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.685\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.189\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.183\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.367\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.157\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.433\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.462\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.439\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.501\n",
            "\u001b[32m[01/11 02:09:08 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.188 | 68.472 | 18.910 | 0.000 | 18.278 | 36.654 |\n",
            "\u001b[32m[01/11 02:09:08 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.188 |\n",
            "\u001b[32m[01/11 02:09:08 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:09:08 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:09:08 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:09:08 d2.evaluation.testing]: \u001b[0mcopypaste: 29.1880,68.4718,18.9098,0.0000,18.2778,36.6544\n",
            "\u001b[32m[01/11 02:09:08 d2.utils.events]: \u001b[0m eta: 0:47:41  iter: 1279  total_loss: 0.6647  loss_cls: 0.1476  loss_box_reg: 0.428  loss_rpn_cls: 0.02825  loss_rpn_loc: 0.02062    time: 0.7727  last_time: 0.7754  data_time: 0.0339  last_data_time: 0.0249   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:09:16 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:09:16 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:09:16 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:09:16 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:09:16 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:09:16 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:09:16 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:09:16 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:09:17 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0510 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:09:22 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0014 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0508 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:09:27 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0014 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:09:29 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.448707 (0.051870 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:09:29 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049405 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:09:29 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:09:29 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:09:29 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.291\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.679\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.192\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.176\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.369\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.160\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.432\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.462\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.502\n",
            "\u001b[32m[01/11 02:09:30 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.093 | 67.874 | 19.168 | 0.000 | 17.562 | 36.948 |\n",
            "\u001b[32m[01/11 02:09:30 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.093 |\n",
            "\u001b[32m[01/11 02:09:30 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:09:30 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:09:30 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:09:30 d2.evaluation.testing]: \u001b[0mcopypaste: 29.0929,67.8744,19.1677,0.0000,17.5615,36.9475\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:09:37 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:09:37 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:09:37 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:09:37 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:09:37 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:09:37 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:09:37 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:09:37 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:09:38 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0030 s/iter. Inference: 0.0484 s/iter. Eval: 0.0002 s/iter. Total: 0.0517 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:09:44 d2.evaluation.evaluator]: \u001b[0mInference done 111/245. Dataloading: 0.0019 s/iter. Inference: 0.0482 s/iter. Eval: 0.0003 s/iter. Total: 0.0505 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:09:49 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0017 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:09:51 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.493168 (0.052055 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:09:51 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049361 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:09:51 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:09:51 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:09:51 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.30s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.305\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.684\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.213\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.179\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.387\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.161\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.442\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.477\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.433\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.529\n",
            "\u001b[32m[01/11 02:09:51 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.479 | 68.374 | 21.322 | 0.000 | 17.882 | 38.724 |\n",
            "\u001b[32m[01/11 02:09:51 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.479 |\n",
            "\u001b[32m[01/11 02:09:51 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:09:51 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:09:51 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:09:51 d2.evaluation.testing]: \u001b[0mcopypaste: 30.4791,68.3736,21.3222,0.0000,17.8821,38.7236\n",
            "\u001b[32m[01/11 02:09:51 d2.utils.events]: \u001b[0m eta: 0:47:25  iter: 1299  total_loss: 0.6363  loss_cls: 0.1539  loss_box_reg: 0.4224  loss_rpn_cls: 0.01987  loss_rpn_loc: 0.01459    time: 0.7725  last_time: 0.6980  data_time: 0.0309  last_data_time: 0.0296   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:10:00 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:10:00 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:10:00 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:10:00 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:10:00 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:10:00 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:10:00 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:10:00 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:10:01 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0493 s/iter. Eval: 0.0002 s/iter. Total: 0.0505 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:10:06 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0016 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0509 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:10:11 d2.evaluation.evaluator]: \u001b[0mInference done 209/245. Dataloading: 0.0017 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0509 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:10:13 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.417512 (0.051740 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:10:13 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048811 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:10:13 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:10:13 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:10:13 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.29s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.292\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.687\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.193\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.179\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.366\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.155\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.429\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.464\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.439\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.504\n",
            "\u001b[32m[01/11 02:10:13 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.156 | 68.722 | 19.267 | 0.000 | 17.915 | 36.608 |\n",
            "\u001b[32m[01/11 02:10:13 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.156 |\n",
            "\u001b[32m[01/11 02:10:13 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:10:13 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:10:13 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:10:13 d2.evaluation.testing]: \u001b[0mcopypaste: 29.1561,68.7222,19.2675,0.0000,17.9154,36.6082\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:10:21 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:10:21 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:10:21 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:10:21 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:10:21 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:10:21 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:10:21 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:10:21 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:10:22 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0506 s/iter. Eval: 0.0002 s/iter. Total: 0.0519 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:10:27 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0019 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:10:32 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0019 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:10:34 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.375350 (0.051564 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:10:34 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048765 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:10:34 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:10:34 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:10:34 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.34s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.07s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.294\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.684\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.194\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.179\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.369\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.158\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.429\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.469\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.454\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.503\n",
            "\u001b[32m[01/11 02:10:34 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.397 | 68.437 | 19.430 | 0.001 | 17.859 | 36.928 |\n",
            "\u001b[32m[01/11 02:10:34 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.397 |\n",
            "\u001b[32m[01/11 02:10:34 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:10:34 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:10:34 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:10:34 d2.evaluation.testing]: \u001b[0mcopypaste: 29.3969,68.4373,19.4303,0.0010,17.8585,36.9281\n",
            "\u001b[32m[01/11 02:10:34 d2.utils.events]: \u001b[0m eta: 0:47:10  iter: 1319  total_loss: 0.6559  loss_cls: 0.1484  loss_box_reg: 0.4449  loss_rpn_cls: 0.02604  loss_rpn_loc: 0.01806    time: 0.7723  last_time: 0.7638  data_time: 0.0337  last_data_time: 0.0248   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:10:43 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:10:43 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:10:43 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:10:43 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:10:43 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:10:43 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:10:43 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:10:43 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:10:44 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:10:49 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0015 s/iter. Inference: 0.0510 s/iter. Eval: 0.0003 s/iter. Total: 0.0528 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:10:54 d2.evaluation.evaluator]: \u001b[0mInference done 202/245. Dataloading: 0.0015 s/iter. Inference: 0.0507 s/iter. Eval: 0.0003 s/iter. Total: 0.0526 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:10:56 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.790382 (0.053293 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:10:56 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050792 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:10:56 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:10:56 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:10:56 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.31s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.305\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.688\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.185\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.385\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.163\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.440\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.473\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.448\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.512\n",
            "\u001b[32m[01/11 02:10:57 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.478 | 68.764 | 21.708 | 0.000 | 18.522 | 38.509 |\n",
            "\u001b[32m[01/11 02:10:57 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.478 |\n",
            "\u001b[32m[01/11 02:10:57 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:10:57 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:10:57 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:10:57 d2.evaluation.testing]: \u001b[0mcopypaste: 30.4781,68.7644,21.7080,0.0000,18.5220,38.5091\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:11:04 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:11:04 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:11:04 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:11:04 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:11:04 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:11:04 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:11:04 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:11:04 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:11:05 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0510 s/iter. Eval: 0.0002 s/iter. Total: 0.0522 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:11:10 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0015 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0507 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:11:15 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0016 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0509 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:11:17 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.409567 (0.051707 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:11:17 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048879 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:11:17 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:11:17 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:11:17 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.29s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.303\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.696\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.208\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.185\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.381\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.160\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.464\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.433\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.507\n",
            "\u001b[32m[01/11 02:11:18 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.288 | 69.615 | 20.829 | 0.000 | 18.490 | 38.077 |\n",
            "\u001b[32m[01/11 02:11:18 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.288 |\n",
            "\u001b[32m[01/11 02:11:18 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:11:18 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:11:18 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:11:18 d2.evaluation.testing]: \u001b[0mcopypaste: 30.2880,69.6154,20.8290,0.0000,18.4897,38.0771\n",
            "\u001b[32m[01/11 02:11:18 d2.utils.events]: \u001b[0m eta: 0:46:54  iter: 1339  total_loss: 0.667  loss_cls: 0.1598  loss_box_reg: 0.4442  loss_rpn_cls: 0.0268  loss_rpn_loc: 0.02401    time: 0.7722  last_time: 0.6212  data_time: 0.0333  last_data_time: 0.0339   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:11:26 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:11:26 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:11:26 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:11:26 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:11:26 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:11:26 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:11:26 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:11:26 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:11:27 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0508 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:11:32 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0017 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:11:37 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0016 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:11:39 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.563046 (0.052346 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:11:39 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049674 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:11:39 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:11:39 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:11:39 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.69s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.292\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.688\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.204\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.186\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.363\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.159\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.429\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.466\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.449\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.501\n",
            "\u001b[32m[01/11 02:11:40 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.197 | 68.759 | 20.426 | 0.000 | 18.579 | 36.292 |\n",
            "\u001b[32m[01/11 02:11:40 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.197 |\n",
            "\u001b[32m[01/11 02:11:40 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:11:40 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:11:40 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:11:40 d2.evaluation.testing]: \u001b[0mcopypaste: 29.1975,68.7589,20.4257,0.0000,18.5787,36.2923\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:11:48 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:11:48 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:11:48 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:11:48 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:11:48 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:11:48 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:11:48 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:11:48 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:11:49 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0507 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:11:54 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0019 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:11:59 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0021 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:12:01 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.711357 (0.052964 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:12:01 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049992 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:12:01 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:12:01 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:12:01 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.35s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.307\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.693\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.226\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.185\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.386\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.165\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.441\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.480\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.445\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.526\n",
            "\u001b[32m[01/11 02:12:01 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.665 | 69.324 | 22.619 | 0.001 | 18.500 | 38.610 |\n",
            "\u001b[32m[01/11 02:12:01 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.665 |\n",
            "\u001b[32m[01/11 02:12:01 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:12:01 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:12:01 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:12:01 d2.evaluation.testing]: \u001b[0mcopypaste: 30.6651,69.3241,22.6187,0.0009,18.5003,38.6104\n",
            "\u001b[32m[01/11 02:12:01 d2.utils.events]: \u001b[0m eta: 0:46:39  iter: 1359  total_loss: 0.6907  loss_cls: 0.1589  loss_box_reg: 0.4348  loss_rpn_cls: 0.02131  loss_rpn_loc: 0.02287    time: 0.7721  last_time: 0.7737  data_time: 0.0326  last_data_time: 0.0273   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:12:10 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:12:10 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:12:10 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:12:10 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:12:10 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:12:10 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:12:10 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:12:10 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:12:11 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0484 s/iter. Eval: 0.0002 s/iter. Total: 0.0497 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:12:16 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0015 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:12:21 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0015 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:12:23 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.472667 (0.051969 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:12:23 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049421 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:12:23 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:12:23 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:12:23 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.294\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.689\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.195\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.188\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.366\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.158\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.426\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.456\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.430\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.496\n",
            "\u001b[32m[01/11 02:12:23 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.404 | 68.896 | 19.494 | 0.000 | 18.807 | 36.614 |\n",
            "\u001b[32m[01/11 02:12:23 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.404 |\n",
            "\u001b[32m[01/11 02:12:23 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:12:23 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:12:23 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:12:23 d2.evaluation.testing]: \u001b[0mcopypaste: 29.4040,68.8965,19.4940,0.0000,18.8068,36.6143\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:12:31 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:12:31 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:12:31 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:12:31 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:12:31 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:12:31 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:12:31 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:12:31 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:12:32 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0502 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:12:37 d2.evaluation.evaluator]: \u001b[0mInference done 111/245. Dataloading: 0.0015 s/iter. Inference: 0.0486 s/iter. Eval: 0.0003 s/iter. Total: 0.0504 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:12:42 d2.evaluation.evaluator]: \u001b[0mInference done 209/245. Dataloading: 0.0017 s/iter. Inference: 0.0488 s/iter. Eval: 0.0003 s/iter. Total: 0.0508 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:12:44 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.314513 (0.051310 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:12:44 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048640 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:12:44 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:12:44 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:12:44 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.29s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.302\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.690\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.212\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.192\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.375\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.441\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.470\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.439\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.515\n",
            "\u001b[32m[01/11 02:12:44 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.232 | 68.966 | 21.176 | 0.002 | 19.250 | 37.483 |\n",
            "\u001b[32m[01/11 02:12:44 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.232 |\n",
            "\u001b[32m[01/11 02:12:44 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:12:44 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:12:44 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:12:44 d2.evaluation.testing]: \u001b[0mcopypaste: 30.2324,68.9656,21.1757,0.0021,19.2499,37.4827\n",
            "\u001b[32m[01/11 02:12:44 d2.utils.events]: \u001b[0m eta: 0:46:23  iter: 1379  total_loss: 0.6545  loss_cls: 0.1677  loss_box_reg: 0.4374  loss_rpn_cls: 0.02438  loss_rpn_loc: 0.019    time: 0.7720  last_time: 0.7672  data_time: 0.0359  last_data_time: 0.0198   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:12:53 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:12:53 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:12:53 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:12:53 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:12:53 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:12:53 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:12:53 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:12:53 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:12:54 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:12:59 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0019 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:13:04 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0017 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:13:06 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.417144 (0.051738 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:13:06 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049031 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:13:06 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:13:06 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:13:06 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.301\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.699\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.191\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.190\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.374\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.160\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.472\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.444\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.514\n",
            "\u001b[32m[01/11 02:13:06 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.147 | 69.859 | 19.090 | 0.003 | 19.007 | 37.369 |\n",
            "\u001b[32m[01/11 02:13:06 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.147 |\n",
            "\u001b[32m[01/11 02:13:06 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:13:06 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:13:06 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:13:06 d2.evaluation.testing]: \u001b[0mcopypaste: 30.1473,69.8590,19.0904,0.0030,19.0074,37.3691\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:13:14 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:13:14 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:13:14 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:13:14 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:13:14 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:13:14 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:13:14 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:13:14 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:13:15 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0482 s/iter. Eval: 0.0003 s/iter. Total: 0.0496 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:13:20 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0016 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:13:25 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0016 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:13:27 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.617438 (0.052573 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:13:27 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049959 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:13:27 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:13:27 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:13:27 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.33s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.312\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.697\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.225\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.190\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.390\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.444\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.483\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.445\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.531\n",
            "\u001b[32m[01/11 02:13:28 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.159 | 69.748 | 22.519 | 0.002 | 19.024 | 38.977 |\n",
            "\u001b[32m[01/11 02:13:28 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.159 |\n",
            "\u001b[32m[01/11 02:13:28 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:13:28 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:13:28 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:13:28 d2.evaluation.testing]: \u001b[0mcopypaste: 31.1594,69.7483,22.5189,0.0024,19.0240,38.9774\n",
            "\u001b[32m[01/11 02:13:28 d2.utils.events]: \u001b[0m eta: 0:46:08  iter: 1399  total_loss: 0.6146  loss_cls: 0.1496  loss_box_reg: 0.4308  loss_rpn_cls: 0.02878  loss_rpn_loc: 0.01852    time: 0.7720  last_time: 0.7744  data_time: 0.0330  last_data_time: 0.0282   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:13:36 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:13:36 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:13:36 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:13:36 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:13:36 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:13:36 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:13:36 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:13:36 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:13:37 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0507 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:13:42 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0018 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:13:47 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0019 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:13:49 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.731219 (0.053047 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:13:49 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050097 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:13:49 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:13:49 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:13:49 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.295\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.691\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.194\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.179\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.367\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.161\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.428\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.457\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.430\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.497\n",
            "\u001b[32m[01/11 02:13:50 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.464 | 69.088 | 19.432 | 0.000 | 17.918 | 36.749 |\n",
            "\u001b[32m[01/11 02:13:50 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.464 |\n",
            "\u001b[32m[01/11 02:13:50 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:13:50 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:13:50 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:13:50 d2.evaluation.testing]: \u001b[0mcopypaste: 29.4639,69.0879,19.4317,0.0000,17.9184,36.7490\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:13:58 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:13:58 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:13:58 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:13:58 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:13:58 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:13:58 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:13:58 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:13:58 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:13:59 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0524 s/iter. Eval: 0.0002 s/iter. Total: 0.0538 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:14:04 d2.evaluation.evaluator]: \u001b[0mInference done 111/245. Dataloading: 0.0017 s/iter. Inference: 0.0484 s/iter. Eval: 0.0002 s/iter. Total: 0.0505 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:14:09 d2.evaluation.evaluator]: \u001b[0mInference done 210/245. Dataloading: 0.0017 s/iter. Inference: 0.0487 s/iter. Eval: 0.0002 s/iter. Total: 0.0507 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:14:11 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.338885 (0.051412 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:14:11 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048583 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:14:11 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:14:11 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:14:11 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.30s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.329\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.709\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.241\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.187\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.418\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.175\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.464\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.499\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.440\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.561\n",
            "\u001b[32m[01/11 02:14:11 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.888 | 70.917 | 24.110 | 0.000 | 18.686 | 41.759 |\n",
            "\u001b[32m[01/11 02:14:11 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.888 |\n",
            "\u001b[32m[01/11 02:14:11 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:14:11 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:14:11 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:14:11 d2.evaluation.testing]: \u001b[0mcopypaste: 32.8882,70.9171,24.1103,0.0000,18.6859,41.7595\n",
            "\u001b[32m[01/11 02:14:11 d2.utils.events]: \u001b[0m eta: 0:45:52  iter: 1419  total_loss: 0.6375  loss_cls: 0.1414  loss_box_reg: 0.4555  loss_rpn_cls: 0.01935  loss_rpn_loc: 0.01518    time: 0.7722  last_time: 0.7882  data_time: 0.0335  last_data_time: 0.0225   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:14:19 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:14:19 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:14:19 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:14:19 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:14:19 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:14:19 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:14:19 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:14:19 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:14:20 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0542 s/iter. Eval: 0.0002 s/iter. Total: 0.0555 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:14:25 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0016 s/iter. Inference: 0.0509 s/iter. Eval: 0.0003 s/iter. Total: 0.0529 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:14:30 d2.evaluation.evaluator]: \u001b[0mInference done 195/245. Dataloading: 0.0017 s/iter. Inference: 0.0524 s/iter. Eval: 0.0003 s/iter. Total: 0.0545 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:14:33 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:13.027062 (0.054279 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:14:33 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.051592 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:14:33 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:14:33 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:14:33 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.29s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.265\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.671\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.149\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.167\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.330\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.153\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.395\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.427\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.417\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.455\n",
            "\u001b[32m[01/11 02:14:34 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 26.466 | 67.114 | 14.888 | 0.000 | 16.662 | 32.997 |\n",
            "\u001b[32m[01/11 02:14:34 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 26.466 |\n",
            "\u001b[32m[01/11 02:14:34 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:14:34 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:14:34 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:14:34 d2.evaluation.testing]: \u001b[0mcopypaste: 26.4658,67.1137,14.8880,0.0000,16.6620,32.9966\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:14:41 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:14:41 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:14:41 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:14:41 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:14:41 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:14:41 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:14:41 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:14:41 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:14:42 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0469 s/iter. Eval: 0.0003 s/iter. Total: 0.0482 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:14:47 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0023 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:14:52 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0019 s/iter. Inference: 0.0488 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:14:54 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.411203 (0.051713 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:14:54 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048840 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:14:54 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:14:54 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:14:54 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.30s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.332\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.712\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.249\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.189\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.422\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.466\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.503\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.441\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.567\n",
            "\u001b[32m[01/11 02:14:55 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.201 | 71.170 | 24.855 | 0.003 | 18.906 | 42.189 |\n",
            "\u001b[32m[01/11 02:14:55 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.201 |\n",
            "\u001b[32m[01/11 02:14:55 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:14:55 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:14:55 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:14:55 d2.evaluation.testing]: \u001b[0mcopypaste: 33.2013,71.1696,24.8552,0.0028,18.9063,42.1894\n",
            "\u001b[32m[01/11 02:14:55 d2.utils.events]: \u001b[0m eta: 0:45:37  iter: 1439  total_loss: 0.6677  loss_cls: 0.1795  loss_box_reg: 0.4266  loss_rpn_cls: 0.01953  loss_rpn_loc: 0.01656    time: 0.7722  last_time: 0.7647  data_time: 0.0344  last_data_time: 0.0235   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:15:03 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:15:03 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:15:03 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:15:03 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:15:03 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:15:03 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:15:03 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:15:03 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:15:04 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0475 s/iter. Eval: 0.0002 s/iter. Total: 0.0488 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:15:09 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0014 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0509 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:15:14 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0017 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:15:16 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.441631 (0.051840 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:15:16 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049034 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:15:16 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:15:16 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:15:16 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.298\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.698\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.196\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.190\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.370\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.161\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.437\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.468\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.439\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.511\n",
            "\u001b[32m[01/11 02:15:17 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.814 | 69.819 | 19.575 | 0.002 | 18.995 | 36.956 |\n",
            "\u001b[32m[01/11 02:15:17 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.814 |\n",
            "\u001b[32m[01/11 02:15:17 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:15:17 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:15:17 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:15:17 d2.evaluation.testing]: \u001b[0mcopypaste: 29.8141,69.8194,19.5751,0.0018,18.9947,36.9556\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:15:24 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:15:24 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:15:24 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:15:24 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:15:24 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:15:24 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:15:24 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:15:24 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:15:25 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0512 s/iter. Eval: 0.0002 s/iter. Total: 0.0524 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:15:30 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0020 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:15:36 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0021 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:15:38 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.639880 (0.052666 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:15:38 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049645 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:15:38 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:15:38 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:15:38 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.31s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.309\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.694\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.218\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.186\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.388\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.165\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.447\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.484\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.448\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.532\n",
            "\u001b[32m[01/11 02:15:38 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.940 | 69.388 | 21.846 | 0.000 | 18.628 | 38.844 |\n",
            "\u001b[32m[01/11 02:15:38 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.940 |\n",
            "\u001b[32m[01/11 02:15:38 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:15:38 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:15:38 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:15:38 d2.evaluation.testing]: \u001b[0mcopypaste: 30.9400,69.3875,21.8463,0.0000,18.6284,38.8438\n",
            "\u001b[32m[01/11 02:15:38 d2.utils.events]: \u001b[0m eta: 0:45:22  iter: 1459  total_loss: 0.6473  loss_cls: 0.1536  loss_box_reg: 0.4444  loss_rpn_cls: 0.0176  loss_rpn_loc: 0.01498    time: 0.7722  last_time: 0.7699  data_time: 0.0324  last_data_time: 0.0283   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:15:47 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:15:47 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:15:47 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:15:47 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:15:47 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:15:47 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:15:47 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:15:47 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:15:48 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0484 s/iter. Eval: 0.0002 s/iter. Total: 0.0496 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:15:53 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0014 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:15:58 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0015 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:16:00 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.653762 (0.052724 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:16:00 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050097 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:16:00 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:16:00 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:16:00 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.30s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.314\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.703\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.219\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.193\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.391\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.167\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.450\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.484\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.452\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.529\n",
            "\u001b[32m[01/11 02:16:01 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.407 | 70.299 | 21.934 | 0.002 | 19.261 | 39.081 |\n",
            "\u001b[32m[01/11 02:16:01 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.407 |\n",
            "\u001b[32m[01/11 02:16:01 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:16:01 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:16:01 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:16:01 d2.evaluation.testing]: \u001b[0mcopypaste: 31.4067,70.2994,21.9335,0.0019,19.2611,39.0811\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:16:08 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:16:08 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:16:08 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:16:08 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:16:08 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:16:08 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:16:08 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:16:08 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:16:09 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0477 s/iter. Eval: 0.0003 s/iter. Total: 0.0490 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:16:14 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0015 s/iter. Inference: 0.0505 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:16:19 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0017 s/iter. Inference: 0.0502 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:16:22 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.668321 (0.052785 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:16:22 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050132 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:16:22 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:16:22 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:16:22 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.30s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.308\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.698\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.208\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.185\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.384\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.163\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.442\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.479\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.448\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.523\n",
            "\u001b[32m[01/11 02:16:22 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.803 | 69.795 | 20.828 | 0.000 | 18.522 | 38.402 |\n",
            "\u001b[32m[01/11 02:16:22 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.803 |\n",
            "\u001b[32m[01/11 02:16:22 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:16:22 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:16:22 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:16:22 d2.evaluation.testing]: \u001b[0mcopypaste: 30.8033,69.7953,20.8276,0.0000,18.5215,38.4024\n",
            "\u001b[32m[01/11 02:16:22 d2.utils.events]: \u001b[0m eta: 0:45:06  iter: 1479  total_loss: 0.6102  loss_cls: 0.1449  loss_box_reg: 0.4196  loss_rpn_cls: 0.0191  loss_rpn_loc: 0.01386    time: 0.7723  last_time: 0.7629  data_time: 0.0312  last_data_time: 0.0229   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:16:31 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:16:31 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:16:31 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:16:31 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:16:31 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:16:31 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:16:31 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:16:31 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:16:32 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0504 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:16:37 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0018 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:16:42 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0018 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:16:44 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.615912 (0.052566 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:16:44 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049701 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:16:44 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:16:44 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:16:44 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.29s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.288\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.688\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.184\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.180\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.358\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.161\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.426\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.457\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.431\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.497\n",
            "\u001b[32m[01/11 02:16:44 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 28.801 | 68.777 | 18.391 | 0.000 | 18.004 | 35.778 |\n",
            "\u001b[32m[01/11 02:16:44 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 28.801 |\n",
            "\u001b[32m[01/11 02:16:44 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:16:44 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:16:44 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:16:44 d2.evaluation.testing]: \u001b[0mcopypaste: 28.8013,68.7768,18.3911,0.0000,18.0041,35.7784\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:16:52 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:16:52 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:16:52 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:16:52 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:16:52 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:16:52 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:16:52 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:16:52 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:16:53 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0509 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:16:58 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0017 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:17:03 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0018 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:17:05 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.619419 (0.052581 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:17:05 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049839 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:17:05 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:17:05 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:17:06 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.68s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.288\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.690\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.180\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.179\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.359\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.155\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.425\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.449\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.425\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.488\n",
            "\u001b[32m[01/11 02:17:06 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 28.830 | 69.049 | 17.994 | 0.000 | 17.928 | 35.912 |\n",
            "\u001b[32m[01/11 02:17:06 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 28.830 |\n",
            "\u001b[32m[01/11 02:17:06 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:17:06 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:17:06 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:17:06 d2.evaluation.testing]: \u001b[0mcopypaste: 28.8301,69.0490,17.9940,0.0000,17.9278,35.9123\n",
            "\u001b[32m[01/11 02:17:06 d2.utils.events]: \u001b[0m eta: 0:44:51  iter: 1499  total_loss: 0.6343  loss_cls: 0.1656  loss_box_reg: 0.4404  loss_rpn_cls: 0.01616  loss_rpn_loc: 0.01359    time: 0.7725  last_time: 0.7896  data_time: 0.0337  last_data_time: 0.0318   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:17:14 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:17:14 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:17:14 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:17:14 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:17:14 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:17:14 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:17:14 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:17:14 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:17:15 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0014 s/iter. Inference: 0.0509 s/iter. Eval: 0.0002 s/iter. Total: 0.0525 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:17:20 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0024 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:17:25 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0020 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:17:28 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.494712 (0.052061 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:17:28 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049066 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:17:28 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:17:28 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:17:28 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.29s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.302\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.691\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.203\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.188\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.379\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.165\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.472\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.448\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.513\n",
            "\u001b[32m[01/11 02:17:28 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.215 | 69.141 | 20.341 | 0.000 | 18.819 | 37.918 |\n",
            "\u001b[32m[01/11 02:17:28 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.215 |\n",
            "\u001b[32m[01/11 02:17:28 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:17:28 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:17:28 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:17:28 d2.evaluation.testing]: \u001b[0mcopypaste: 30.2149,69.1414,20.3414,0.0000,18.8186,37.9176\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:17:36 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:17:36 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:17:36 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:17:36 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:17:36 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:17:36 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:17:36 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:17:36 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:17:37 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0038 s/iter. Inference: 0.0483 s/iter. Eval: 0.0002 s/iter. Total: 0.0524 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:17:42 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0019 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:17:47 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0019 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:17:49 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.552206 (0.052301 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:17:49 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049441 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:17:49 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:17:49 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:17:49 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.29s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.303\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.693\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.218\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.190\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.380\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.161\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.444\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.470\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.448\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.508\n",
            "\u001b[32m[01/11 02:17:49 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.329 | 69.332 | 21.825 | 0.000 | 19.040 | 38.023 |\n",
            "\u001b[32m[01/11 02:17:49 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.329 |\n",
            "\u001b[32m[01/11 02:17:49 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:17:49 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:17:49 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:17:49 d2.evaluation.testing]: \u001b[0mcopypaste: 30.3286,69.3316,21.8248,0.0000,19.0404,38.0230\n",
            "\u001b[32m[01/11 02:17:49 d2.utils.events]: \u001b[0m eta: 0:44:35  iter: 1519  total_loss: 0.6604  loss_cls: 0.1596  loss_box_reg: 0.4216  loss_rpn_cls: 0.02035  loss_rpn_loc: 0.01865    time: 0.7724  last_time: 0.7011  data_time: 0.0326  last_data_time: 0.0238   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:17:58 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:17:58 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:17:58 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:17:58 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:17:58 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:17:58 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:17:58 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:17:58 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:17:59 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0516 s/iter. Eval: 0.0002 s/iter. Total: 0.0528 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:18:04 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0016 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:18:09 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0019 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:18:11 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.433961 (0.051808 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:18:11 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048909 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:18:11 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:18:11 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:18:11 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.279\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.683\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.173\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.181\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.348\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.156\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.418\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.445\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.428\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.478\n",
            "\u001b[32m[01/11 02:18:11 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 27.946 | 68.269 | 17.299 | 0.000 | 18.082 | 34.764 |\n",
            "\u001b[32m[01/11 02:18:11 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 27.946 |\n",
            "\u001b[32m[01/11 02:18:11 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:18:11 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:18:11 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:18:11 d2.evaluation.testing]: \u001b[0mcopypaste: 27.9464,68.2692,17.2993,0.0000,18.0825,34.7645\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:18:19 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:18:19 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:18:19 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:18:19 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:18:19 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:18:19 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:18:19 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:18:19 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:18:20 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0498 s/iter. Eval: 0.0002 s/iter. Total: 0.0510 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:18:25 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0015 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:18:30 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0021 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:18:32 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.430261 (0.051793 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:18:32 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048814 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:18:32 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:18:32 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:18:32 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.30s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.326\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.707\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.246\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.190\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.414\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.169\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.457\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.490\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.431\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.552\n",
            "\u001b[32m[01/11 02:18:32 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.603 | 70.717 | 24.568 | 0.001 | 19.011 | 41.410 |\n",
            "\u001b[32m[01/11 02:18:32 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.603 |\n",
            "\u001b[32m[01/11 02:18:32 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:18:32 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:18:32 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:18:32 d2.evaluation.testing]: \u001b[0mcopypaste: 32.6031,70.7169,24.5680,0.0012,19.0105,41.4102\n",
            "\u001b[32m[01/11 02:18:32 d2.utils.events]: \u001b[0m eta: 0:44:20  iter: 1539  total_loss: 0.6907  loss_cls: 0.1557  loss_box_reg: 0.4414  loss_rpn_cls: 0.02612  loss_rpn_loc: 0.02102    time: 0.7724  last_time: 0.7671  data_time: 0.0332  last_data_time: 0.0275   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:18:40 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:18:40 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:18:40 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:18:40 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:18:40 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:18:40 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:18:40 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:18:40 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:18:41 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0484 s/iter. Eval: 0.0002 s/iter. Total: 0.0496 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:18:46 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0020 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:18:51 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0019 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:18:54 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.490655 (0.052044 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:18:54 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049201 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:18:54 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:18:54 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:18:54 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.300\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.696\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.209\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.186\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.373\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.166\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.437\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.460\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.428\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.503\n",
            "\u001b[32m[01/11 02:18:54 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.992 | 69.584 | 20.863 | 0.006 | 18.614 | 37.310 |\n",
            "\u001b[32m[01/11 02:18:54 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.992 |\n",
            "\u001b[32m[01/11 02:18:54 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:18:54 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:18:54 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:18:54 d2.evaluation.testing]: \u001b[0mcopypaste: 29.9921,69.5841,20.8634,0.0056,18.6141,37.3097\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:19:02 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:19:02 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:19:02 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:19:02 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:19:02 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:19:02 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:19:02 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:19:02 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:19:03 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0481 s/iter. Eval: 0.0002 s/iter. Total: 0.0493 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:19:08 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0017 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:19:13 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0016 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:19:15 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.533349 (0.052222 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:19:15 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049549 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:19:15 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:19:15 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:19:15 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.29s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.299\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.693\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.200\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.176\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.376\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.163\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.438\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.466\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.432\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.512\n",
            "\u001b[32m[01/11 02:19:15 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.933 | 69.335 | 20.021 | 0.000 | 17.599 | 37.629 |\n",
            "\u001b[32m[01/11 02:19:15 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.933 |\n",
            "\u001b[32m[01/11 02:19:15 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:19:15 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:19:15 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:19:15 d2.evaluation.testing]: \u001b[0mcopypaste: 29.9327,69.3346,20.0212,0.0000,17.5989,37.6292\n",
            "\u001b[32m[01/11 02:19:15 d2.utils.events]: \u001b[0m eta: 0:44:04  iter: 1559  total_loss: 0.6204  loss_cls: 0.1441  loss_box_reg: 0.4264  loss_rpn_cls: 0.02319  loss_rpn_loc: 0.01259    time: 0.7723  last_time: 0.7675  data_time: 0.0330  last_data_time: 0.0255   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:19:24 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:19:24 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:19:24 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:19:24 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:19:24 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:19:24 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:19:24 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:19:24 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:19:25 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0014 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:19:30 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0016 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:19:35 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0017 s/iter. Inference: 0.0502 s/iter. Eval: 0.0003 s/iter. Total: 0.0523 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:19:37 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.638874 (0.052662 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:19:37 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049910 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:19:37 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:19:37 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:19:37 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.32s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.314\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.704\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.214\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.181\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.396\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.166\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.449\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.482\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.536\n",
            "\u001b[32m[01/11 02:19:37 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.376 | 70.443 | 21.404 | 0.000 | 18.053 | 39.631 |\n",
            "\u001b[32m[01/11 02:19:37 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.376 |\n",
            "\u001b[32m[01/11 02:19:37 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:19:37 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:19:37 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:19:37 d2.evaluation.testing]: \u001b[0mcopypaste: 31.3762,70.4434,21.4036,0.0000,18.0532,39.6309\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:19:45 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:19:45 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:19:45 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:19:45 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:19:45 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:19:45 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:19:45 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:19:45 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:19:46 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0482 s/iter. Eval: 0.0002 s/iter. Total: 0.0495 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:19:51 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0017 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:19:56 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0015 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:19:58 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.472152 (0.051967 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:19:58 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049356 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:19:58 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:19:58 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:19:58 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.69s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.292\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.694\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.177\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.187\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.359\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.428\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.453\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.434\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.490\n",
            "\u001b[32m[01/11 02:19:59 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.171 | 69.445 | 17.653 | 0.000 | 18.715 | 35.949 |\n",
            "\u001b[32m[01/11 02:19:59 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.171 |\n",
            "\u001b[32m[01/11 02:19:59 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:19:59 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:19:59 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:19:59 d2.evaluation.testing]: \u001b[0mcopypaste: 29.1710,69.4447,17.6530,0.0000,18.7155,35.9492\n",
            "\u001b[32m[01/11 02:19:59 d2.utils.events]: \u001b[0m eta: 0:43:49  iter: 1579  total_loss: 0.6111  loss_cls: 0.1625  loss_box_reg: 0.4122  loss_rpn_cls: 0.01997  loss_rpn_loc: 0.0233    time: 0.7723  last_time: 0.7720  data_time: 0.0316  last_data_time: 0.0259   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:20:07 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:20:07 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:20:07 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:20:07 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:20:07 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:20:07 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:20:07 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:20:07 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:20:08 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0541 s/iter. Eval: 0.0003 s/iter. Total: 0.0554 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:20:13 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0014 s/iter. Inference: 0.0511 s/iter. Eval: 0.0003 s/iter. Total: 0.0529 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:20:18 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0019 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:20:21 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.573389 (0.052389 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:20:21 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049516 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:20:21 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:20:21 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:20:21 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.306\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.706\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.206\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.191\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.379\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.167\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.442\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.471\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.449\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.510\n",
            "\u001b[32m[01/11 02:20:21 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.608 | 70.570 | 20.638 | 0.002 | 19.112 | 37.918 |\n",
            "\u001b[32m[01/11 02:20:21 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.608 |\n",
            "\u001b[32m[01/11 02:20:21 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:20:21 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:20:21 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:20:21 d2.evaluation.testing]: \u001b[0mcopypaste: 30.6084,70.5703,20.6377,0.0015,19.1122,37.9184\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:20:29 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:20:29 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:20:29 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:20:29 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:20:29 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:20:29 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:20:29 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:20:29 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:20:30 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0497 s/iter. Eval: 0.0002 s/iter. Total: 0.0512 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:20:35 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0028 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:20:40 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0022 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:20:42 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.621938 (0.052591 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:20:42 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049302 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:20:42 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:20:42 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:20:42 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.30s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.320\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.708\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.230\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.193\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.401\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.170\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.455\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.486\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.538\n",
            "\u001b[32m[01/11 02:20:43 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.011 | 70.808 | 23.004 | 0.002 | 19.255 | 40.133 |\n",
            "\u001b[32m[01/11 02:20:43 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.011 |\n",
            "\u001b[32m[01/11 02:20:43 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:20:43 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:20:43 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:20:43 d2.evaluation.testing]: \u001b[0mcopypaste: 32.0107,70.8082,23.0044,0.0020,19.2546,40.1335\n",
            "\u001b[32m[01/11 02:20:43 d2.utils.events]: \u001b[0m eta: 0:43:33  iter: 1599  total_loss: 0.697  loss_cls: 0.1727  loss_box_reg: 0.4395  loss_rpn_cls: 0.02247  loss_rpn_loc: 0.0218    time: 0.7723  last_time: 0.7629  data_time: 0.0317  last_data_time: 0.0190   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:20:51 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:20:51 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:20:51 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:20:51 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:20:51 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:20:51 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:20:51 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:20:51 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:20:52 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0485 s/iter. Eval: 0.0002 s/iter. Total: 0.0499 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:20:57 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0020 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:21:02 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0019 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:21:04 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.632925 (0.052637 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:21:04 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049708 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:21:04 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:21:04 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:21:04 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.307\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.708\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.195\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.189\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.379\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.167\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.442\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.469\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.439\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.512\n",
            "\u001b[32m[01/11 02:21:05 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.679 | 70.847 | 19.480 | 0.002 | 18.877 | 37.881 |\n",
            "\u001b[32m[01/11 02:21:05 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.679 |\n",
            "\u001b[32m[01/11 02:21:05 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:21:05 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:21:05 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:21:05 d2.evaluation.testing]: \u001b[0mcopypaste: 30.6787,70.8467,19.4795,0.0019,18.8773,37.8806\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:21:12 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:21:12 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:21:12 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:21:12 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:21:12 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:21:12 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:21:12 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:21:12 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:21:13 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0490 s/iter. Eval: 0.0002 s/iter. Total: 0.0504 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:21:18 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0022 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:21:23 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0019 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:21:26 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.473486 (0.051973 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:21:26 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049018 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:21:26 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:21:26 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:21:26 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.33s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.325\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.710\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.235\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.196\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.405\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.171\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.460\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.501\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.029\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.448\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.559\n",
            "\u001b[32m[01/11 02:21:26 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.494 | 70.955 | 23.493 | 0.007 | 19.625 | 40.542 |\n",
            "\u001b[32m[01/11 02:21:26 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.494 |\n",
            "\u001b[32m[01/11 02:21:26 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:21:26 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:21:26 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:21:26 d2.evaluation.testing]: \u001b[0mcopypaste: 32.4938,70.9552,23.4931,0.0070,19.6254,40.5417\n",
            "\u001b[32m[01/11 02:21:26 d2.utils.events]: \u001b[0m eta: 0:43:18  iter: 1619  total_loss: 0.67  loss_cls: 0.1745  loss_box_reg: 0.4404  loss_rpn_cls: 0.02836  loss_rpn_loc: 0.02685    time: 0.7723  last_time: 0.7665  data_time: 0.0319  last_data_time: 0.0241   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:21:34 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:21:34 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:21:34 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:21:34 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:21:34 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:21:34 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:21:34 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:21:34 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:21:35 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0008 s/iter. Inference: 0.0515 s/iter. Eval: 0.0003 s/iter. Total: 0.0525 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:21:40 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0017 s/iter. Inference: 0.0487 s/iter. Eval: 0.0003 s/iter. Total: 0.0507 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:21:45 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0016 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0509 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:21:47 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.396039 (0.051650 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:21:47 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048979 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:21:47 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:21:47 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:21:47 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.33s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.316\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.704\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.226\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.187\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.394\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.169\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.445\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.492\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.457\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.538\n",
            "\u001b[32m[01/11 02:21:48 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.629 | 70.383 | 22.608 | 0.003 | 18.654 | 39.447 |\n",
            "\u001b[32m[01/11 02:21:48 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.629 |\n",
            "\u001b[32m[01/11 02:21:48 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:21:48 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:21:48 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:21:48 d2.evaluation.testing]: \u001b[0mcopypaste: 31.6291,70.3826,22.6081,0.0027,18.6536,39.4472\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:21:55 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:21:55 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:21:55 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:21:55 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:21:55 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:21:55 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:21:55 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:21:55 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:21:57 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0518 s/iter. Eval: 0.0002 s/iter. Total: 0.0530 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:22:02 d2.evaluation.evaluator]: \u001b[0mInference done 111/245. Dataloading: 0.0014 s/iter. Inference: 0.0487 s/iter. Eval: 0.0003 s/iter. Total: 0.0505 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:22:07 d2.evaluation.evaluator]: \u001b[0mInference done 209/245. Dataloading: 0.0015 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0509 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:22:09 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.374488 (0.051560 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:22:09 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049011 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:22:09 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:22:09 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:22:09 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.31s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.292\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.685\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.198\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.175\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.367\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.159\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.424\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.461\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.435\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.502\n",
            "\u001b[32m[01/11 02:22:09 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.185 | 68.491 | 19.843 | 0.000 | 17.465 | 36.704 |\n",
            "\u001b[32m[01/11 02:22:09 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.185 |\n",
            "\u001b[32m[01/11 02:22:09 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:22:09 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:22:09 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:22:09 d2.evaluation.testing]: \u001b[0mcopypaste: 29.1847,68.4907,19.8428,0.0000,17.4653,36.7041\n",
            "\u001b[32m[01/11 02:22:09 d2.utils.events]: \u001b[0m eta: 0:43:03  iter: 1639  total_loss: 0.6411  loss_cls: 0.1595  loss_box_reg: 0.4418  loss_rpn_cls: 0.02139  loss_rpn_loc: 0.01681    time: 0.7722  last_time: 0.7631  data_time: 0.0331  last_data_time: 0.0246   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:22:17 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:22:17 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:22:17 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:22:17 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:22:17 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:22:17 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:22:17 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:22:17 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:22:18 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0512 s/iter. Eval: 0.0003 s/iter. Total: 0.0525 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:22:23 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0022 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:22:28 d2.evaluation.evaluator]: \u001b[0mInference done 202/245. Dataloading: 0.0019 s/iter. Inference: 0.0502 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:22:31 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.707893 (0.052950 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:22:31 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049811 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:22:31 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:22:31 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:22:31 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.31s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.335\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.718\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.260\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.199\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.420\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.463\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.494\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.442\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.552\n",
            "\u001b[32m[01/11 02:22:31 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.516 | 71.750 | 25.962 | 0.000 | 19.867 | 42.046 |\n",
            "\u001b[32m[01/11 02:22:31 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.516 |\n",
            "\u001b[32m[01/11 02:22:31 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:22:31 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:22:31 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:22:31 d2.evaluation.testing]: \u001b[0mcopypaste: 33.5162,71.7502,25.9617,0.0000,19.8669,42.0463\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:22:39 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:22:39 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:22:39 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:22:39 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:22:39 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:22:39 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:22:39 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:22:39 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:22:40 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0017 s/iter. Inference: 0.0488 s/iter. Eval: 0.0002 s/iter. Total: 0.0507 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:22:45 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0021 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:22:50 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0020 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:22:53 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.613717 (0.052557 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:22:53 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049606 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:22:53 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:22:53 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:22:53 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.31s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.294\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.690\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.202\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.177\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.369\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.156\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.429\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.459\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.435\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.498\n",
            "\u001b[32m[01/11 02:22:53 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.383 | 68.967 | 20.167 | 0.002 | 17.734 | 36.857 |\n",
            "\u001b[32m[01/11 02:22:53 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.383 |\n",
            "\u001b[32m[01/11 02:22:53 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:22:53 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:22:53 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:22:53 d2.evaluation.testing]: \u001b[0mcopypaste: 29.3828,68.9672,20.1667,0.0017,17.7340,36.8566\n",
            "\u001b[32m[01/11 02:22:53 d2.utils.events]: \u001b[0m eta: 0:42:48  iter: 1659  total_loss: 0.6639  loss_cls: 0.1586  loss_box_reg: 0.4353  loss_rpn_cls: 0.02265  loss_rpn_loc: 0.0172    time: 0.7724  last_time: 0.7654  data_time: 0.0340  last_data_time: 0.0240   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:23:01 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:23:01 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:23:01 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:23:01 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:23:01 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:23:01 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:23:01 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:23:01 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:23:02 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0031 s/iter. Inference: 0.0486 s/iter. Eval: 0.0002 s/iter. Total: 0.0519 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:23:07 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0015 s/iter. Inference: 0.0507 s/iter. Eval: 0.0003 s/iter. Total: 0.0525 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:23:12 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0017 s/iter. Inference: 0.0500 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:23:15 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.692028 (0.052883 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:23:15 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050019 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:23:15 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:23:15 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:23:15 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.705\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.230\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.191\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.396\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.165\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.449\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.478\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.445\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.524\n",
            "\u001b[32m[01/11 02:23:15 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.718 | 70.507 | 22.983 | 0.000 | 19.053 | 39.617 |\n",
            "\u001b[32m[01/11 02:23:15 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.718 |\n",
            "\u001b[32m[01/11 02:23:15 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:23:15 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:23:15 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:23:15 d2.evaluation.testing]: \u001b[0mcopypaste: 31.7183,70.5073,22.9831,0.0000,19.0528,39.6169\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:23:23 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:23:23 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:23:23 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:23:23 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:23:23 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:23:23 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:23:23 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:23:23 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:23:24 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0035 s/iter. Inference: 0.0491 s/iter. Eval: 0.0002 s/iter. Total: 0.0529 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:23:29 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0021 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:23:34 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0020 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:23:36 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.563680 (0.052349 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:23:36 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049423 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:23:36 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:23:36 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:23:36 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.305\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.701\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.212\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.188\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.378\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.161\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.437\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.463\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.501\n",
            "\u001b[32m[01/11 02:23:36 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.479 | 70.108 | 21.218 | 0.000 | 18.837 | 37.773 |\n",
            "\u001b[32m[01/11 02:23:36 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.479 |\n",
            "\u001b[32m[01/11 02:23:36 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:23:36 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:23:36 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:23:36 d2.evaluation.testing]: \u001b[0mcopypaste: 30.4794,70.1080,21.2181,0.0000,18.8367,37.7735\n",
            "\u001b[32m[01/11 02:23:36 d2.utils.events]: \u001b[0m eta: 0:42:32  iter: 1679  total_loss: 0.6455  loss_cls: 0.1658  loss_box_reg: 0.4135  loss_rpn_cls: 0.02409  loss_rpn_loc: 0.02233    time: 0.7724  last_time: 0.7745  data_time: 0.0350  last_data_time: 0.0290   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:23:44 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:23:44 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:23:44 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:23:44 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:23:44 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:23:44 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:23:44 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:23:44 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:23:45 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0055 s/iter. Inference: 0.0482 s/iter. Eval: 0.0003 s/iter. Total: 0.0540 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:23:50 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0029 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0525 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:23:55 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0022 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:23:58 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.607971 (0.052533 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:23:58 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049463 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:23:58 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:23:58 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:23:58 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.323\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.711\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.234\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.200\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.398\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.169\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.453\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.487\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.459\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.530\n",
            "\u001b[32m[01/11 02:23:58 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.262 | 71.145 | 23.351 | 0.000 | 20.044 | 39.818 |\n",
            "\u001b[32m[01/11 02:23:58 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.262 |\n",
            "\u001b[32m[01/11 02:23:58 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:23:58 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:23:58 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:23:58 d2.evaluation.testing]: \u001b[0mcopypaste: 32.2618,71.1446,23.3506,0.0000,20.0438,39.8178\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:24:06 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:24:06 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:24:06 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:24:06 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:24:06 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:24:06 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:24:06 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:24:06 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:24:07 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:24:12 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0017 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:24:17 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0016 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:24:19 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.469896 (0.051958 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:24:19 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049343 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:24:19 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:24:19 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:24:19 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.321\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.716\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.225\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.201\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.398\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.170\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.451\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.482\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.532\n",
            "\u001b[32m[01/11 02:24:20 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.073 | 71.559 | 22.485 | 0.001 | 20.065 | 39.767 |\n",
            "\u001b[32m[01/11 02:24:20 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.073 |\n",
            "\u001b[32m[01/11 02:24:20 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:24:20 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:24:20 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:24:20 d2.evaluation.testing]: \u001b[0mcopypaste: 32.0730,71.5593,22.4848,0.0012,20.0655,39.7667\n",
            "\u001b[32m[01/11 02:24:20 d2.utils.events]: \u001b[0m eta: 0:42:17  iter: 1699  total_loss: 0.6335  loss_cls: 0.1487  loss_box_reg: 0.4213  loss_rpn_cls: 0.02084  loss_rpn_loc: 0.01529    time: 0.7723  last_time: 0.7855  data_time: 0.0336  last_data_time: 0.0324   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:24:28 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:24:28 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:24:28 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:24:28 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:24:28 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:24:28 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:24:28 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:24:28 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:24:29 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0492 s/iter. Eval: 0.0002 s/iter. Total: 0.0505 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:24:34 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0016 s/iter. Inference: 0.0504 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:24:39 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0017 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0523 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:24:41 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.702610 (0.052928 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:24:41 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050198 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:24:41 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:24:41 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:24:41 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.315\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.713\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.192\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.390\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.169\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.449\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.475\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.438\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.523\n",
            "\u001b[32m[01/11 02:24:42 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.477 | 71.344 | 21.727 | 0.000 | 19.155 | 38.976 |\n",
            "\u001b[32m[01/11 02:24:42 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.477 |\n",
            "\u001b[32m[01/11 02:24:42 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:24:42 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:24:42 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:24:42 d2.evaluation.testing]: \u001b[0mcopypaste: 31.4772,71.3436,21.7275,0.0000,19.1551,38.9756\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:24:49 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:24:49 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:24:49 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:24:49 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:24:49 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:24:49 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:24:49 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:24:49 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:24:50 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0527 s/iter. Eval: 0.0002 s/iter. Total: 0.0541 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:24:55 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0016 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:25:00 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0016 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:25:03 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.518218 (0.052159 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:25:03 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049547 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:25:03 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:25:03 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:25:03 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.302\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.695\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.179\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.377\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.158\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.434\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.465\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.434\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.508\n",
            "\u001b[32m[01/11 02:25:03 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.222 | 69.462 | 21.684 | 0.000 | 17.947 | 37.738 |\n",
            "\u001b[32m[01/11 02:25:03 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.222 |\n",
            "\u001b[32m[01/11 02:25:03 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:25:03 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:25:03 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:25:03 d2.evaluation.testing]: \u001b[0mcopypaste: 30.2223,69.4622,21.6842,0.0000,17.9465,37.7378\n",
            "\u001b[32m[01/11 02:25:03 d2.utils.events]: \u001b[0m eta: 0:42:02  iter: 1719  total_loss: 0.6527  loss_cls: 0.158  loss_box_reg: 0.4251  loss_rpn_cls: 0.01981  loss_rpn_loc: 0.01826    time: 0.7723  last_time: 0.7726  data_time: 0.0335  last_data_time: 0.0324   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:25:11 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:25:11 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:25:11 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:25:11 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:25:11 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:25:11 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:25:11 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:25:11 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:25:12 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.0003 s/iter. Total: 0.0540 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:25:17 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0017 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:25:22 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0016 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:25:24 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.471732 (0.051966 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:25:24 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049254 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:25:24 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:25:24 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:25:24 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.64s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.278\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.682\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.168\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.175\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.342\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.155\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.408\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.431\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.422\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.458\n",
            "\u001b[32m[01/11 02:25:25 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 27.789 | 68.248 | 16.751 | 0.000 | 17.454 | 34.151 |\n",
            "\u001b[32m[01/11 02:25:25 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 27.789 |\n",
            "\u001b[32m[01/11 02:25:25 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:25:25 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:25:25 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:25:25 d2.evaluation.testing]: \u001b[0mcopypaste: 27.7890,68.2478,16.7510,0.0000,17.4539,34.1509\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:25:33 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:25:33 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:25:33 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:25:33 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:25:33 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:25:33 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:25:33 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:25:33 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:25:34 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0524 s/iter. Eval: 0.0003 s/iter. Total: 0.0539 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:25:39 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0018 s/iter. Inference: 0.0506 s/iter. Eval: 0.0003 s/iter. Total: 0.0527 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:25:44 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0016 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0523 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:25:46 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.724646 (0.053019 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:25:46 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050415 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:25:46 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:25:46 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:25:46 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.323\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.713\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.231\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.195\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.402\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.166\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.452\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.482\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.441\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.533\n",
            "\u001b[32m[01/11 02:25:46 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.278 | 71.330 | 23.137 | 0.002 | 19.546 | 40.158 |\n",
            "\u001b[32m[01/11 02:25:46 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.278 |\n",
            "\u001b[32m[01/11 02:25:46 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:25:46 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:25:46 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:25:46 d2.evaluation.testing]: \u001b[0mcopypaste: 32.2778,71.3298,23.1371,0.0018,19.5455,40.1583\n",
            "\u001b[32m[01/11 02:25:46 d2.utils.events]: \u001b[0m eta: 0:41:46  iter: 1739  total_loss: 0.6526  loss_cls: 0.143  loss_box_reg: 0.4433  loss_rpn_cls: 0.02321  loss_rpn_loc: 0.01626    time: 0.7721  last_time: 0.7607  data_time: 0.0339  last_data_time: 0.0223   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:25:55 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:25:55 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:25:55 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:25:55 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:25:55 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:25:55 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:25:55 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:25:55 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:25:56 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0029 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0535 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:26:01 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0020 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:26:06 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0019 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:26:08 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.707787 (0.052949 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:26:08 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049986 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:26:08 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:26:08 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:26:08 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.314\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.710\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.226\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.190\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.390\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.165\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.447\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.473\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.521\n",
            "\u001b[32m[01/11 02:26:08 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.381 | 71.032 | 22.565 | 0.000 | 19.043 | 39.005 |\n",
            "\u001b[32m[01/11 02:26:08 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.381 |\n",
            "\u001b[32m[01/11 02:26:08 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:26:08 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:26:08 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:26:08 d2.evaluation.testing]: \u001b[0mcopypaste: 31.3808,71.0322,22.5649,0.0000,19.0431,39.0047\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:26:16 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:26:16 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:26:16 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:26:16 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:26:16 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:26:16 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:26:16 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:26:16 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:26:17 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0513 s/iter. Eval: 0.0002 s/iter. Total: 0.0525 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:26:22 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0015 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:26:27 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0015 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:26:29 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.645507 (0.052690 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:26:29 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050086 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:26:29 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:26:29 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:26:29 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.309\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.705\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.226\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.190\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.383\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.164\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.444\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.476\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.445\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.520\n",
            "\u001b[32m[01/11 02:26:30 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.864 | 70.525 | 22.562 | 0.000 | 18.963 | 38.276 |\n",
            "\u001b[32m[01/11 02:26:30 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.864 |\n",
            "\u001b[32m[01/11 02:26:30 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:26:30 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:26:30 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:26:30 d2.evaluation.testing]: \u001b[0mcopypaste: 30.8638,70.5250,22.5623,0.0000,18.9629,38.2763\n",
            "\u001b[32m[01/11 02:26:30 d2.utils.events]: \u001b[0m eta: 0:41:30  iter: 1759  total_loss: 0.6517  loss_cls: 0.1455  loss_box_reg: 0.4436  loss_rpn_cls: 0.02205  loss_rpn_loc: 0.02098    time: 0.7721  last_time: 0.7643  data_time: 0.0337  last_data_time: 0.0206   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:26:38 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:26:38 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:26:38 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:26:38 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:26:38 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:26:38 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:26:38 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:26:38 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:26:39 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0013 s/iter. Inference: 0.0542 s/iter. Eval: 0.0003 s/iter. Total: 0.0558 s/iter. ETA=0:00:13\n",
            "\u001b[32m[01/11 02:26:44 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0016 s/iter. Inference: 0.0508 s/iter. Eval: 0.0003 s/iter. Total: 0.0527 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:26:49 d2.evaluation.evaluator]: \u001b[0mInference done 201/245. Dataloading: 0.0016 s/iter. Inference: 0.0511 s/iter. Eval: 0.0003 s/iter. Total: 0.0530 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:26:52 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.835870 (0.053483 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:26:52 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050797 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:26:52 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:26:52 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:26:52 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.32s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.316\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.713\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.229\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.196\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.393\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.165\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.453\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.491\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.458\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.537\n",
            "\u001b[32m[01/11 02:26:52 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.644 | 71.281 | 22.886 | 0.000 | 19.572 | 39.338 |\n",
            "\u001b[32m[01/11 02:26:52 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.644 |\n",
            "\u001b[32m[01/11 02:26:52 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:26:52 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:26:52 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:26:52 d2.evaluation.testing]: \u001b[0mcopypaste: 31.6441,71.2811,22.8858,0.0000,19.5719,39.3379\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:27:00 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:27:00 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:27:00 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:27:00 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:27:00 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:27:00 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:27:00 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:27:00 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:27:01 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0028 s/iter. Inference: 0.0507 s/iter. Eval: 0.0003 s/iter. Total: 0.0537 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:27:06 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0020 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0527 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:27:11 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0022 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:27:13 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.673797 (0.052807 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:27:13 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049702 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:27:13 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:27:13 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:27:13 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.281\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.692\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.172\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.177\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.348\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.154\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.414\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.446\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.438\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.474\n",
            "\u001b[32m[01/11 02:27:13 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 28.059 | 69.175 | 17.242 | 0.003 | 17.721 | 34.772 |\n",
            "\u001b[32m[01/11 02:27:13 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 28.059 |\n",
            "\u001b[32m[01/11 02:27:13 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:27:13 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:27:13 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:27:13 d2.evaluation.testing]: \u001b[0mcopypaste: 28.0586,69.1753,17.2424,0.0034,17.7211,34.7715\n",
            "\u001b[32m[01/11 02:27:13 d2.utils.events]: \u001b[0m eta: 0:41:15  iter: 1779  total_loss: 0.6442  loss_cls: 0.1552  loss_box_reg: 0.4334  loss_rpn_cls: 0.02083  loss_rpn_loc: 0.01835    time: 0.7720  last_time: 0.6995  data_time: 0.0379  last_data_time: 0.0258   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:27:21 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:27:21 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:27:21 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:27:21 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:27:21 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:27:21 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:27:21 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:27:21 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:27:23 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0023 s/iter. Inference: 0.0518 s/iter. Eval: 0.0003 s/iter. Total: 0.0544 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:27:28 d2.evaluation.evaluator]: \u001b[0mInference done 105/245. Dataloading: 0.0019 s/iter. Inference: 0.0514 s/iter. Eval: 0.0003 s/iter. Total: 0.0537 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:27:33 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0016 s/iter. Inference: 0.0504 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:27:35 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.789769 (0.053291 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:27:35 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050576 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:27:35 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:27:35 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:27:35 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.29s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.302\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.698\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.210\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.182\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.376\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.163\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.439\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.467\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.441\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.507\n",
            "\u001b[32m[01/11 02:27:35 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.154 | 69.847 | 20.976 | 0.007 | 18.237 | 37.570 |\n",
            "\u001b[32m[01/11 02:27:35 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.154 |\n",
            "\u001b[32m[01/11 02:27:35 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:27:35 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:27:35 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:27:35 d2.evaluation.testing]: \u001b[0mcopypaste: 30.1542,69.8472,20.9757,0.0067,18.2366,37.5696\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:27:43 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:27:43 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:27:43 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:27:43 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:27:43 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:27:43 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:27:43 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:27:43 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:27:44 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0014 s/iter. Inference: 0.0487 s/iter. Eval: 0.0003 s/iter. Total: 0.0503 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:27:49 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0018 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:27:54 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0018 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:27:56 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.514176 (0.052142 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:27:56 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049234 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:27:56 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:27:56 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:27:56 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.30s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.314\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.701\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.222\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.185\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.395\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.448\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.478\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.529\n",
            "\u001b[32m[01/11 02:27:57 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.423 | 70.099 | 22.212 | 0.002 | 18.463 | 39.510 |\n",
            "\u001b[32m[01/11 02:27:57 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.423 |\n",
            "\u001b[32m[01/11 02:27:57 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:27:57 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:27:57 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:27:57 d2.evaluation.testing]: \u001b[0mcopypaste: 31.4226,70.0988,22.2125,0.0016,18.4633,39.5103\n",
            "\u001b[32m[01/11 02:27:57 d2.utils.events]: \u001b[0m eta: 0:41:00  iter: 1799  total_loss: 0.6666  loss_cls: 0.1652  loss_box_reg: 0.4751  loss_rpn_cls: 0.01226  loss_rpn_loc: 0.01367    time: 0.7718  last_time: 0.7653  data_time: 0.0345  last_data_time: 0.0249   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:28:05 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:28:05 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:28:05 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:28:05 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:28:05 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:28:05 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:28:05 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:28:05 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:28:06 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0511 s/iter. Eval: 0.0003 s/iter. Total: 0.0523 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:28:11 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0015 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:28:16 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0015 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:28:18 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.450593 (0.051877 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:28:18 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049314 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:28:18 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:28:18 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:28:18 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.32s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.323\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.712\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.234\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.200\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.401\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.170\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.457\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.496\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.455\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.548\n",
            "\u001b[32m[01/11 02:28:19 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.314 | 71.177 | 23.369 | 0.000 | 20.021 | 40.067 |\n",
            "\u001b[32m[01/11 02:28:19 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.314 |\n",
            "\u001b[32m[01/11 02:28:19 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:28:19 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:28:19 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:28:19 d2.evaluation.testing]: \u001b[0mcopypaste: 32.3137,71.1767,23.3691,0.0000,20.0207,40.0671\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:28:26 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:28:26 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:28:26 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:28:26 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:28:26 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:28:26 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:28:26 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:28:26 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:28:27 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0492 s/iter. Eval: 0.0002 s/iter. Total: 0.0506 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:28:32 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0019 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:28:37 d2.evaluation.evaluator]: \u001b[0mInference done 197/245. Dataloading: 0.0021 s/iter. Inference: 0.0515 s/iter. Eval: 0.0003 s/iter. Total: 0.0539 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:28:40 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:13.021191 (0.054255 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:28:40 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.051232 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:28:40 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:28:40 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:28:40 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.309\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.714\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.213\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.197\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.380\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.167\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.442\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.477\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.441\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.524\n",
            "\u001b[32m[01/11 02:28:40 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.871 | 71.363 | 21.326 | 0.000 | 19.687 | 37.968 |\n",
            "\u001b[32m[01/11 02:28:40 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.871 |\n",
            "\u001b[32m[01/11 02:28:40 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:28:40 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:28:40 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:28:40 d2.evaluation.testing]: \u001b[0mcopypaste: 30.8709,71.3628,21.3257,0.0000,19.6869,37.9684\n",
            "\u001b[32m[01/11 02:28:40 d2.utils.events]: \u001b[0m eta: 0:40:44  iter: 1819  total_loss: 0.606  loss_cls: 0.1459  loss_box_reg: 0.4136  loss_rpn_cls: 0.0144  loss_rpn_loc: 0.0123    time: 0.7718  last_time: 0.7650  data_time: 0.0335  last_data_time: 0.0206   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:28:49 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:28:49 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:28:49 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:28:49 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:28:49 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:28:49 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:28:49 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:28:49 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:28:50 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0493 s/iter. Eval: 0.0002 s/iter. Total: 0.0506 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:28:55 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0022 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:29:00 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0019 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0523 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:29:02 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.737973 (0.053075 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:29:02 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050110 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:29:02 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:29:02 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:29:02 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.29s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.292\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.696\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.188\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.184\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.359\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.156\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.426\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.458\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.446\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.489\n",
            "\u001b[32m[01/11 02:29:02 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.216 | 69.568 | 18.843 | 0.000 | 18.390 | 35.927 |\n",
            "\u001b[32m[01/11 02:29:02 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.216 |\n",
            "\u001b[32m[01/11 02:29:02 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:29:02 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:29:02 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:29:02 d2.evaluation.testing]: \u001b[0mcopypaste: 29.2159,69.5679,18.8429,0.0000,18.3901,35.9270\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:29:10 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:29:10 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:29:10 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:29:10 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:29:10 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:29:10 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:29:10 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:29:10 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:29:11 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0527 s/iter. Eval: 0.0003 s/iter. Total: 0.0539 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:29:16 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0014 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:29:21 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0021 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:29:23 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.568905 (0.052370 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:29:23 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049363 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:29:23 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:29:23 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:29:23 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.704\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.230\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.194\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.396\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.170\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.455\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.489\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.447\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.540\n",
            "\u001b[32m[01/11 02:29:24 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.870 | 70.384 | 23.012 | 0.000 | 19.414 | 39.649 |\n",
            "\u001b[32m[01/11 02:29:24 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.870 |\n",
            "\u001b[32m[01/11 02:29:24 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:29:24 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:29:24 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:29:24 d2.evaluation.testing]: \u001b[0mcopypaste: 31.8704,70.3842,23.0118,0.0000,19.4135,39.6488\n",
            "\u001b[32m[01/11 02:29:24 d2.utils.events]: \u001b[0m eta: 0:40:29  iter: 1839  total_loss: 0.6433  loss_cls: 0.1495  loss_box_reg: 0.4223  loss_rpn_cls: 0.02327  loss_rpn_loc: 0.01928    time: 0.7716  last_time: 0.7727  data_time: 0.0328  last_data_time: 0.0224   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:29:32 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:29:32 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:29:32 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:29:32 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:29:32 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:29:32 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:29:32 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:29:32 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:29:33 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0551 s/iter. Eval: 0.0003 s/iter. Total: 0.0564 s/iter. ETA=0:00:13\n",
            "\u001b[32m[01/11 02:29:38 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0016 s/iter. Inference: 0.0510 s/iter. Eval: 0.0003 s/iter. Total: 0.0530 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:29:43 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0016 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:29:46 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.671635 (0.052798 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:29:46 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050211 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:29:46 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:29:46 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:29:46 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.307\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.700\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.212\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.182\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.384\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.166\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.441\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.469\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.515\n",
            "\u001b[32m[01/11 02:29:46 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.691 | 69.983 | 21.203 | 0.000 | 18.165 | 38.384 |\n",
            "\u001b[32m[01/11 02:29:46 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.691 |\n",
            "\u001b[32m[01/11 02:29:46 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:29:46 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:29:46 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:29:46 d2.evaluation.testing]: \u001b[0mcopypaste: 30.6910,69.9828,21.2032,0.0000,18.1651,38.3840\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:29:54 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:29:54 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:29:54 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:29:54 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:29:54 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:29:54 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:29:54 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:29:54 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:29:55 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0481 s/iter. Eval: 0.0002 s/iter. Total: 0.0493 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:30:00 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0023 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:30:05 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0018 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:30:07 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.476696 (0.051986 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:30:07 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049150 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:30:07 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:30:07 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:30:07 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.307\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.700\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.220\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.183\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.383\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.163\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.441\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.470\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.435\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.517\n",
            "\u001b[32m[01/11 02:30:07 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.685 | 70.032 | 21.987 | 0.000 | 18.264 | 38.318 |\n",
            "\u001b[32m[01/11 02:30:07 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.685 |\n",
            "\u001b[32m[01/11 02:30:07 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:30:07 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:30:07 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:30:07 d2.evaluation.testing]: \u001b[0mcopypaste: 30.6851,70.0316,21.9869,0.0000,18.2641,38.3182\n",
            "\u001b[32m[01/11 02:30:07 d2.utils.events]: \u001b[0m eta: 0:40:13  iter: 1859  total_loss: 0.6581  loss_cls: 0.1514  loss_box_reg: 0.4034  loss_rpn_cls: 0.01541  loss_rpn_loc: 0.02102    time: 0.7718  last_time: 0.7768  data_time: 0.0354  last_data_time: 0.0313   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:30:16 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:30:16 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:30:16 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:30:16 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:30:16 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:30:16 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:30:16 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:30:16 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:30:17 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0502 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:30:22 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0016 s/iter. Inference: 0.0488 s/iter. Eval: 0.0003 s/iter. Total: 0.0508 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:30:27 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0017 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:30:29 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.450484 (0.051877 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:30:29 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049132 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:30:29 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:30:29 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:30:29 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.313\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.700\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.225\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.194\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.389\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.165\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.445\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.479\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.432\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.533\n",
            "\u001b[32m[01/11 02:30:29 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.342 | 70.011 | 22.459 | 0.000 | 19.416 | 38.899 |\n",
            "\u001b[32m[01/11 02:30:29 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.342 |\n",
            "\u001b[32m[01/11 02:30:29 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:30:29 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:30:29 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:30:29 d2.evaluation.testing]: \u001b[0mcopypaste: 31.3422,70.0108,22.4586,0.0000,19.4160,38.8989\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:30:37 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:30:37 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:30:37 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:30:37 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:30:37 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:30:37 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:30:37 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:30:37 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:30:38 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0017 s/iter. Inference: 0.0491 s/iter. Eval: 0.0002 s/iter. Total: 0.0510 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:30:43 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0018 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:30:48 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0017 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:30:50 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.595517 (0.052481 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:30:50 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049766 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:30:50 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:30:50 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:30:50 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.31s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.323\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.710\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.220\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.193\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.404\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.453\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.491\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.439\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.549\n",
            "\u001b[32m[01/11 02:30:51 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.269 | 70.969 | 22.025 | 0.003 | 19.277 | 40.437 |\n",
            "\u001b[32m[01/11 02:30:51 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.269 |\n",
            "\u001b[32m[01/11 02:30:51 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:30:51 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:30:51 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:30:51 d2.evaluation.testing]: \u001b[0mcopypaste: 32.2694,70.9691,22.0246,0.0026,19.2773,40.4371\n",
            "\u001b[32m[01/11 02:30:51 d2.utils.events]: \u001b[0m eta: 0:39:58  iter: 1879  total_loss: 0.7312  loss_cls: 0.1845  loss_box_reg: 0.4751  loss_rpn_cls: 0.0401  loss_rpn_loc: 0.02906    time: 0.7718  last_time: 0.7680  data_time: 0.0340  last_data_time: 0.0246   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:30:59 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:30:59 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:30:59 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:30:59 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:30:59 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:30:59 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:30:59 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:30:59 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:31:00 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0501 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:31:05 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0018 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:31:10 d2.evaluation.evaluator]: \u001b[0mInference done 209/245. Dataloading: 0.0016 s/iter. Inference: 0.0488 s/iter. Eval: 0.0003 s/iter. Total: 0.0507 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:31:12 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.333456 (0.051389 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:31:12 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048782 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:31:12 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:31:12 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:31:12 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.309\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.708\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.210\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.191\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.383\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.164\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.446\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.475\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.437\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.524\n",
            "\u001b[32m[01/11 02:31:12 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.942 | 70.774 | 20.951 | 0.000 | 19.085 | 38.289 |\n",
            "\u001b[32m[01/11 02:31:12 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.942 |\n",
            "\u001b[32m[01/11 02:31:12 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:31:12 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:31:12 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:31:12 d2.evaluation.testing]: \u001b[0mcopypaste: 30.9418,70.7735,20.9507,0.0000,19.0846,38.2886\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:31:20 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:31:20 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:31:20 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:31:20 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:31:20 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:31:20 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:31:20 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:31:20 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:31:22 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.1277 s/iter. Eval: 0.0002 s/iter. Total: 0.1290 s/iter. ETA=0:00:30\n",
            "\u001b[32m[01/11 02:31:27 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0018 s/iter. Inference: 0.0540 s/iter. Eval: 0.0002 s/iter. Total: 0.0561 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:31:32 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0017 s/iter. Inference: 0.0514 s/iter. Eval: 0.0002 s/iter. Total: 0.0534 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:31:34 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.973958 (0.054058 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:31:34 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.051316 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:31:34 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:31:34 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:31:34 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.29s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.324\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.712\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.236\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.190\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.408\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.453\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.488\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.431\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.549\n",
            "\u001b[32m[01/11 02:31:34 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.401 | 71.219 | 23.638 | 0.000 | 18.982 | 40.800 |\n",
            "\u001b[32m[01/11 02:31:34 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.401 |\n",
            "\u001b[32m[01/11 02:31:34 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:31:34 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:31:34 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:31:34 d2.evaluation.testing]: \u001b[0mcopypaste: 32.4007,71.2190,23.6380,0.0000,18.9820,40.7997\n",
            "\u001b[32m[01/11 02:31:34 d2.utils.events]: \u001b[0m eta: 0:39:43  iter: 1899  total_loss: 0.6215  loss_cls: 0.1457  loss_box_reg: 0.4066  loss_rpn_cls: 0.03424  loss_rpn_loc: 0.02063    time: 0.7718  last_time: 0.7665  data_time: 0.0356  last_data_time: 0.0236   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:31:42 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:31:42 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:31:42 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:31:42 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:31:42 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:31:42 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:31:42 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:31:42 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:31:43 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0512 s/iter. Eval: 0.0003 s/iter. Total: 0.0525 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:31:48 d2.evaluation.evaluator]: \u001b[0mInference done 111/245. Dataloading: 0.0015 s/iter. Inference: 0.0486 s/iter. Eval: 0.0003 s/iter. Total: 0.0505 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:31:53 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0017 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:31:55 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.500157 (0.052084 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:31:55 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049412 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:31:55 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:31:55 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:31:56 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.304\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.701\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.209\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.181\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.383\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.163\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.433\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.464\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.424\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.513\n",
            "\u001b[32m[01/11 02:31:56 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.415 | 70.130 | 20.942 | 0.000 | 18.069 | 38.310 |\n",
            "\u001b[32m[01/11 02:31:56 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.415 |\n",
            "\u001b[32m[01/11 02:31:56 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:31:56 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:31:56 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:31:56 d2.evaluation.testing]: \u001b[0mcopypaste: 30.4154,70.1301,20.9419,0.0000,18.0690,38.3103\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:32:03 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:32:03 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:32:03 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:32:03 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:32:03 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:32:03 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:32:03 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:32:03 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:32:04 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0031 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0526 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:32:09 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0018 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:32:14 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0021 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:32:17 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.505418 (0.052106 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:32:17 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049051 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:32:17 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:32:17 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:32:17 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.320\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.699\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.236\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.189\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.401\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.169\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.449\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.476\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.429\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.531\n",
            "\u001b[32m[01/11 02:32:17 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.003 | 69.925 | 23.558 | 0.005 | 18.912 | 40.149 |\n",
            "\u001b[32m[01/11 02:32:17 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.003 |\n",
            "\u001b[32m[01/11 02:32:17 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:32:17 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:32:17 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:32:17 d2.evaluation.testing]: \u001b[0mcopypaste: 32.0031,69.9246,23.5584,0.0046,18.9124,40.1489\n",
            "\u001b[32m[01/11 02:32:17 d2.utils.events]: \u001b[0m eta: 0:39:27  iter: 1919  total_loss: 0.6322  loss_cls: 0.1522  loss_box_reg: 0.4301  loss_rpn_cls: 0.02253  loss_rpn_loc: 0.01662    time: 0.7716  last_time: 0.7712  data_time: 0.0323  last_data_time: 0.0232   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:32:25 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:32:25 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:32:25 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:32:25 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:32:25 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:32:25 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:32:25 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:32:25 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:32:26 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0050 s/iter. Inference: 0.0488 s/iter. Eval: 0.0002 s/iter. Total: 0.0541 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:32:31 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0028 s/iter. Inference: 0.0491 s/iter. Eval: 0.0002 s/iter. Total: 0.0521 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:32:36 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0021 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:32:39 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.548321 (0.052285 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:32:39 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049323 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:32:39 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:32:39 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:32:39 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.314\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.703\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.225\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.190\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.391\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.167\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.445\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.471\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.427\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.522\n",
            "\u001b[32m[01/11 02:32:39 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.428 | 70.263 | 22.477 | 0.006 | 18.987 | 39.120 |\n",
            "\u001b[32m[01/11 02:32:39 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.428 |\n",
            "\u001b[32m[01/11 02:32:39 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:32:39 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:32:39 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:32:39 d2.evaluation.testing]: \u001b[0mcopypaste: 31.4282,70.2633,22.4767,0.0056,18.9866,39.1204\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:32:47 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:32:47 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:32:47 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:32:47 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:32:47 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:32:47 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:32:47 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:32:47 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:32:48 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0484 s/iter. Eval: 0.0003 s/iter. Total: 0.0498 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:32:53 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0016 s/iter. Inference: 0.0488 s/iter. Eval: 0.0003 s/iter. Total: 0.0507 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:32:58 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0016 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:33:00 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.486268 (0.052026 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:33:00 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049248 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:33:00 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:33:00 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:33:00 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.699\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.231\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.188\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.401\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.171\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.453\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.484\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.425\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.545\n",
            "\u001b[32m[01/11 02:33:00 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.925 | 69.928 | 23.062 | 0.000 | 18.848 | 40.139 |\n",
            "\u001b[32m[01/11 02:33:00 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.925 |\n",
            "\u001b[32m[01/11 02:33:00 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:33:00 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:33:00 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:33:00 d2.evaluation.testing]: \u001b[0mcopypaste: 31.9251,69.9278,23.0620,0.0000,18.8481,40.1395\n",
            "\u001b[32m[01/11 02:33:00 d2.utils.events]: \u001b[0m eta: 0:39:12  iter: 1939  total_loss: 0.6175  loss_cls: 0.1405  loss_box_reg: 0.4393  loss_rpn_cls: 0.01039  loss_rpn_loc: 0.01208    time: 0.7716  last_time: 0.7861  data_time: 0.0341  last_data_time: 0.0390   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:33:09 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:33:09 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:33:09 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:33:09 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:33:09 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:33:09 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:33:09 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:33:09 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:33:10 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0557 s/iter. Eval: 0.0002 s/iter. Total: 0.0570 s/iter. ETA=0:00:13\n",
            "\u001b[32m[01/11 02:33:15 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0015 s/iter. Inference: 0.0511 s/iter. Eval: 0.0003 s/iter. Total: 0.0530 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:33:20 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0015 s/iter. Inference: 0.0504 s/iter. Eval: 0.0003 s/iter. Total: 0.0523 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:33:22 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.702953 (0.052929 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:33:22 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050354 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:33:22 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:33:22 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:33:22 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.328\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.704\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.241\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.197\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.410\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.171\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.459\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.486\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.425\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.549\n",
            "\u001b[32m[01/11 02:33:22 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.780 | 70.419 | 24.058 | 0.000 | 19.704 | 40.994 |\n",
            "\u001b[32m[01/11 02:33:22 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.780 |\n",
            "\u001b[32m[01/11 02:33:22 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:33:22 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:33:22 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:33:22 d2.evaluation.testing]: \u001b[0mcopypaste: 32.7804,70.4195,24.0576,0.0000,19.7036,40.9940\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:33:30 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:33:30 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:33:30 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:33:30 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:33:30 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:33:30 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:33:30 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:33:30 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:33:31 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0469 s/iter. Eval: 0.0002 s/iter. Total: 0.0482 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:33:36 d2.evaluation.evaluator]: \u001b[0mInference done 104/245. Dataloading: 0.0019 s/iter. Inference: 0.0514 s/iter. Eval: 0.0003 s/iter. Total: 0.0536 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:33:41 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0017 s/iter. Inference: 0.0500 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:33:43 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.600059 (0.052500 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:33:43 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049783 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:33:43 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:33:43 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:33:44 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.296\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.696\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.200\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.184\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.367\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.160\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.432\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.459\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.497\n",
            "\u001b[32m[01/11 02:33:44 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.602 | 69.559 | 19.953 | 0.001 | 18.415 | 36.669 |\n",
            "\u001b[32m[01/11 02:33:44 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.602 |\n",
            "\u001b[32m[01/11 02:33:44 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:33:44 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:33:44 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:33:44 d2.evaluation.testing]: \u001b[0mcopypaste: 29.6019,69.5587,19.9534,0.0011,18.4149,36.6688\n",
            "\u001b[32m[01/11 02:33:44 d2.utils.events]: \u001b[0m eta: 0:38:57  iter: 1959  total_loss: 0.682  loss_cls: 0.1668  loss_box_reg: 0.4387  loss_rpn_cls: 0.02073  loss_rpn_loc: 0.02084    time: 0.7717  last_time: 0.7639  data_time: 0.0386  last_data_time: 0.0268   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:33:52 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:33:52 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:33:52 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:33:52 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:33:52 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:33:52 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:33:52 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:33:52 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:33:53 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0043 s/iter. Inference: 0.0483 s/iter. Eval: 0.0003 s/iter. Total: 0.0528 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:33:58 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0024 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0523 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:34:03 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0021 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:34:06 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.674757 (0.052811 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:34:06 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049567 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:34:06 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:34:06 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:34:06 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.32s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.307\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.707\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.227\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.186\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.383\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.472\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.450\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.511\n",
            "\u001b[32m[01/11 02:34:06 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.681 | 70.673 | 22.663 | 0.003 | 18.628 | 38.297 |\n",
            "\u001b[32m[01/11 02:34:06 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.681 |\n",
            "\u001b[32m[01/11 02:34:06 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:34:06 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:34:06 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:34:06 d2.evaluation.testing]: \u001b[0mcopypaste: 30.6808,70.6731,22.6626,0.0028,18.6284,38.2970\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:34:14 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:34:14 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:34:14 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:34:14 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:34:14 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:34:14 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:34:14 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:34:14 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:34:15 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0486 s/iter. Eval: 0.0002 s/iter. Total: 0.0499 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:34:20 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0015 s/iter. Inference: 0.0486 s/iter. Eval: 0.0003 s/iter. Total: 0.0505 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:34:25 d2.evaluation.evaluator]: \u001b[0mInference done 198/245. Dataloading: 0.0018 s/iter. Inference: 0.0513 s/iter. Eval: 0.0003 s/iter. Total: 0.0534 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:34:27 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.844769 (0.053520 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:34:27 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050704 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:34:27 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:34:27 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:34:27 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.34s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.314\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.710\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.219\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.196\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.391\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.164\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.450\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.488\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.460\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.531\n",
            "\u001b[32m[01/11 02:34:28 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.445 | 70.988 | 21.859 | 0.000 | 19.615 | 39.117 |\n",
            "\u001b[32m[01/11 02:34:28 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.445 |\n",
            "\u001b[32m[01/11 02:34:28 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:34:28 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:34:28 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:34:28 d2.evaluation.testing]: \u001b[0mcopypaste: 31.4452,70.9877,21.8590,0.0000,19.6150,39.1174\n",
            "\u001b[32m[01/11 02:34:28 d2.utils.events]: \u001b[0m eta: 0:38:41  iter: 1979  total_loss: 0.7003  loss_cls: 0.1859  loss_box_reg: 0.4304  loss_rpn_cls: 0.02847  loss_rpn_loc: 0.0269    time: 0.7717  last_time: 0.7739  data_time: 0.0320  last_data_time: 0.0267   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:34:36 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:34:36 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:34:36 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:34:36 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:34:36 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:34:36 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:34:36 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:34:36 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:34:37 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0514 s/iter. Eval: 0.0002 s/iter. Total: 0.0527 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:34:42 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0016 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0508 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:34:47 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0018 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:34:49 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.505991 (0.052108 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:34:49 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049299 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:34:49 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:34:49 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:34:49 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.302\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.706\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.190\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.189\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.374\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.161\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.464\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.441\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.503\n",
            "\u001b[32m[01/11 02:34:50 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.229 | 70.551 | 18.973 | 0.000 | 18.920 | 37.372 |\n",
            "\u001b[32m[01/11 02:34:50 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.229 |\n",
            "\u001b[32m[01/11 02:34:50 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:34:50 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:34:50 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:34:50 d2.evaluation.testing]: \u001b[0mcopypaste: 30.2291,70.5513,18.9731,0.0000,18.9198,37.3724\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:34:57 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:34:57 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:34:57 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:34:57 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:34:57 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:34:57 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:34:57 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:34:57 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:34:58 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0551 s/iter. Eval: 0.0003 s/iter. Total: 0.0565 s/iter. ETA=0:00:13\n",
            "\u001b[32m[01/11 02:35:03 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0017 s/iter. Inference: 0.0507 s/iter. Eval: 0.0003 s/iter. Total: 0.0527 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:35:08 d2.evaluation.evaluator]: \u001b[0mInference done 202/245. Dataloading: 0.0018 s/iter. Inference: 0.0506 s/iter. Eval: 0.0003 s/iter. Total: 0.0527 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:35:11 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.755035 (0.053146 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:35:11 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050350 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:35:11 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:35:11 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:35:11 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.30s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.309\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.704\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.212\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.192\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.382\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.445\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.472\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.448\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.512\n",
            "\u001b[32m[01/11 02:35:11 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.868 | 70.363 | 21.192 | 0.000 | 19.158 | 38.250 |\n",
            "\u001b[32m[01/11 02:35:11 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.868 |\n",
            "\u001b[32m[01/11 02:35:11 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:35:11 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:35:11 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:35:11 d2.evaluation.testing]: \u001b[0mcopypaste: 30.8679,70.3633,21.1920,0.0000,19.1580,38.2498\n",
            "\u001b[32m[01/11 02:35:11 d2.utils.events]: \u001b[0m eta: 0:38:25  iter: 1999  total_loss: 0.6071  loss_cls: 0.1434  loss_box_reg: 0.429  loss_rpn_cls: 0.01824  loss_rpn_loc: 0.01674    time: 0.7717  last_time: 0.7622  data_time: 0.0330  last_data_time: 0.0210   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:35:20 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:35:20 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:35:20 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:35:20 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:35:20 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:35:20 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:35:20 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:35:20 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:35:20 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0501 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:35:26 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0016 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:35:31 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0016 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:35:33 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.482415 (0.052010 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:35:33 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049390 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:35:33 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:35:33 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:35:33 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.309\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.700\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.223\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.185\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.385\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.164\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.466\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.444\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.504\n",
            "\u001b[32m[01/11 02:35:33 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.948 | 70.032 | 22.301 | 0.000 | 18.478 | 38.475 |\n",
            "\u001b[32m[01/11 02:35:33 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.948 |\n",
            "\u001b[32m[01/11 02:35:33 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:35:33 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:35:33 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:35:33 d2.evaluation.testing]: \u001b[0mcopypaste: 30.9481,70.0323,22.3011,0.0000,18.4780,38.4752\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:35:41 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:35:41 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:35:41 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:35:41 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:35:41 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:35:41 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:35:41 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:35:41 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:35:42 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0488 s/iter. Eval: 0.0003 s/iter. Total: 0.0500 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:35:47 d2.evaluation.evaluator]: \u001b[0mInference done 111/245. Dataloading: 0.0013 s/iter. Inference: 0.0486 s/iter. Eval: 0.0003 s/iter. Total: 0.0503 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:35:52 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0018 s/iter. Inference: 0.0488 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:35:54 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.385702 (0.051607 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:35:54 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048785 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:35:54 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:35:54 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:35:54 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.310\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.710\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.184\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.388\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.165\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.438\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.466\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.437\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.509\n",
            "\u001b[32m[01/11 02:35:54 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.026 | 71.001 | 21.699 | 0.000 | 18.434 | 38.754 |\n",
            "\u001b[32m[01/11 02:35:54 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.026 |\n",
            "\u001b[32m[01/11 02:35:54 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:35:54 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:35:54 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:35:54 d2.evaluation.testing]: \u001b[0mcopypaste: 31.0256,71.0014,21.6988,0.0000,18.4345,38.7537\n",
            "\u001b[32m[01/11 02:35:54 d2.utils.events]: \u001b[0m eta: 0:38:10  iter: 2019  total_loss: 0.6263  loss_cls: 0.1479  loss_box_reg: 0.4287  loss_rpn_cls: 0.0203  loss_rpn_loc: 0.01612    time: 0.7716  last_time: 0.7040  data_time: 0.0341  last_data_time: 0.0244   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:36:03 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:36:03 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:36:03 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:36:03 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:36:03 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:36:03 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:36:03 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:36:03 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:36:04 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0020 s/iter. Inference: 0.0506 s/iter. Eval: 0.0002 s/iter. Total: 0.0528 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:36:09 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0018 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:36:14 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0017 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:36:16 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.709036 (0.052954 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:36:16 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050167 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:36:16 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:36:16 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:36:16 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.310\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.707\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.220\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.184\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.386\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.165\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.441\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.470\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.449\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.508\n",
            "\u001b[32m[01/11 02:36:17 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.026 | 70.675 | 22.033 | 0.000 | 18.398 | 38.618 |\n",
            "\u001b[32m[01/11 02:36:17 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.026 |\n",
            "\u001b[32m[01/11 02:36:17 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:36:17 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:36:17 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:36:17 d2.evaluation.testing]: \u001b[0mcopypaste: 31.0255,70.6746,22.0327,0.0000,18.3984,38.6176\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:36:24 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:36:24 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:36:24 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:36:24 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:36:24 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:36:24 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:36:24 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:36:24 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:36:25 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0528 s/iter. Eval: 0.0003 s/iter. Total: 0.0542 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:36:30 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0016 s/iter. Inference: 0.0509 s/iter. Eval: 0.0003 s/iter. Total: 0.0528 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:36:35 d2.evaluation.evaluator]: \u001b[0mInference done 202/245. Dataloading: 0.0015 s/iter. Inference: 0.0507 s/iter. Eval: 0.0003 s/iter. Total: 0.0526 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:36:38 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.704781 (0.052937 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:36:38 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050415 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:36:38 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:36:38 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:36:38 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.30s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.707\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.190\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.396\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.166\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.448\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.485\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.029\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.458\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.526\n",
            "\u001b[32m[01/11 02:36:38 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.663 | 70.720 | 21.732 | 0.009 | 18.965 | 39.619 |\n",
            "\u001b[32m[01/11 02:36:38 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.663 |\n",
            "\u001b[32m[01/11 02:36:38 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:36:38 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:36:38 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:36:38 d2.evaluation.testing]: \u001b[0mcopypaste: 31.6635,70.7198,21.7322,0.0092,18.9649,39.6192\n",
            "\u001b[32m[01/11 02:36:38 d2.utils.events]: \u001b[0m eta: 0:37:55  iter: 2039  total_loss: 0.6363  loss_cls: 0.152  loss_box_reg: 0.3984  loss_rpn_cls: 0.01796  loss_rpn_loc: 0.01342    time: 0.7717  last_time: 0.7723  data_time: 0.0327  last_data_time: 0.0280   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:36:46 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:36:46 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:36:46 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:36:46 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:36:46 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:36:46 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:36:46 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:36:46 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:36:47 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0019 s/iter. Inference: 0.0499 s/iter. Eval: 0.0002 s/iter. Total: 0.0521 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:36:52 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0021 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:36:58 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0018 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:37:00 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.432860 (0.051804 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:37:00 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049046 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:37:00 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:37:00 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:37:00 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.316\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.712\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.199\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.390\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.167\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.447\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.476\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.450\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.516\n",
            "\u001b[32m[01/11 02:37:00 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.647 | 71.164 | 21.529 | 0.003 | 19.898 | 38.966 |\n",
            "\u001b[32m[01/11 02:37:00 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.647 |\n",
            "\u001b[32m[01/11 02:37:00 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:37:00 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:37:00 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:37:00 d2.evaluation.testing]: \u001b[0mcopypaste: 31.6473,71.1641,21.5286,0.0033,19.8976,38.9661\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:37:08 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:37:08 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:37:08 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:37:08 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:37:08 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:37:08 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:37:08 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:37:08 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:37:09 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0505 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:37:14 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0020 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:37:19 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0020 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:37:21 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.504893 (0.052104 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:37:21 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049170 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:37:21 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:37:21 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:37:21 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.69s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.320\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.711\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.226\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.204\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.395\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.165\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.455\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.484\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.019\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.450\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.529\n",
            "\u001b[32m[01/11 02:37:22 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.026 | 71.060 | 22.557 | 0.005 | 20.354 | 39.459 |\n",
            "\u001b[32m[01/11 02:37:22 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.026 |\n",
            "\u001b[32m[01/11 02:37:22 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:37:22 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:37:22 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:37:22 d2.evaluation.testing]: \u001b[0mcopypaste: 32.0264,71.0599,22.5565,0.0046,20.3538,39.4593\n",
            "\u001b[32m[01/11 02:37:22 d2.utils.events]: \u001b[0m eta: 0:37:39  iter: 2059  total_loss: 0.6317  loss_cls: 0.1465  loss_box_reg: 0.4213  loss_rpn_cls: 0.0194  loss_rpn_loc: 0.01696    time: 0.7717  last_time: 0.7737  data_time: 0.0369  last_data_time: 0.0284   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:37:30 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:37:30 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:37:30 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:37:30 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:37:30 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:37:30 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:37:30 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:37:30 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:37:31 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0019 s/iter. Inference: 0.0508 s/iter. Eval: 0.0003 s/iter. Total: 0.0530 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:37:36 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0019 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:37:41 d2.evaluation.evaluator]: \u001b[0mInference done 202/245. Dataloading: 0.0020 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0526 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:37:44 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.792250 (0.053301 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:37:44 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050388 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:37:44 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:37:44 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:37:44 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.320\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.711\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.221\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.202\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.395\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.165\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.456\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.478\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.433\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.530\n",
            "\u001b[32m[01/11 02:37:44 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.023 | 71.059 | 22.110 | 0.012 | 20.173 | 39.526 |\n",
            "\u001b[32m[01/11 02:37:44 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.023 |\n",
            "\u001b[32m[01/11 02:37:44 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:37:44 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:37:44 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:37:44 d2.evaluation.testing]: \u001b[0mcopypaste: 32.0228,71.0590,22.1101,0.0119,20.1735,39.5258\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:37:52 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:37:52 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:37:52 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:37:52 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:37:52 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:37:52 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:37:52 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:37:52 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:37:53 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0505 s/iter. Eval: 0.0002 s/iter. Total: 0.0519 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:37:58 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0015 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:38:03 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0016 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:38:05 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.390109 (0.051625 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:38:05 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049034 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:38:05 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:38:05 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:38:05 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.305\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.697\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.186\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.379\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.438\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.462\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.426\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.508\n",
            "\u001b[32m[01/11 02:38:05 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.468 | 69.671 | 21.618 | 0.007 | 18.583 | 37.928 |\n",
            "\u001b[32m[01/11 02:38:05 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.468 |\n",
            "\u001b[32m[01/11 02:38:05 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:38:05 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:38:05 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:38:05 d2.evaluation.testing]: \u001b[0mcopypaste: 30.4677,69.6709,21.6184,0.0066,18.5829,37.9276\n",
            "\u001b[32m[01/11 02:38:05 d2.utils.events]: \u001b[0m eta: 0:37:24  iter: 2079  total_loss: 0.6596  loss_cls: 0.1565  loss_box_reg: 0.4487  loss_rpn_cls: 0.01766  loss_rpn_loc: 0.01517    time: 0.7718  last_time: 0.7680  data_time: 0.0340  last_data_time: 0.0219   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:38:13 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:38:13 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:38:13 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:38:13 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:38:13 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:38:13 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:38:13 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:38:13 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:38:15 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0028 s/iter. Inference: 0.0480 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:38:20 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0022 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:38:25 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0019 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:38:27 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.589739 (0.052457 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:38:27 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049554 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:38:27 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:38:27 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:38:27 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.29s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.699\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.238\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.191\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.396\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.450\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.482\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.449\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.528\n",
            "\u001b[32m[01/11 02:38:27 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.654 | 69.943 | 23.770 | 0.004 | 19.090 | 39.635 |\n",
            "\u001b[32m[01/11 02:38:27 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.654 |\n",
            "\u001b[32m[01/11 02:38:27 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:38:27 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:38:27 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:38:27 d2.evaluation.testing]: \u001b[0mcopypaste: 31.6543,69.9431,23.7697,0.0044,19.0895,39.6351\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:38:35 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:38:35 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:38:35 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:38:35 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:38:35 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:38:35 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:38:35 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:38:35 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:38:36 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0038 s/iter. Inference: 0.0482 s/iter. Eval: 0.0002 s/iter. Total: 0.0522 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:38:41 d2.evaluation.evaluator]: \u001b[0mInference done 111/245. Dataloading: 0.0016 s/iter. Inference: 0.0485 s/iter. Eval: 0.0003 s/iter. Total: 0.0505 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:38:46 d2.evaluation.evaluator]: \u001b[0mInference done 210/245. Dataloading: 0.0017 s/iter. Inference: 0.0485 s/iter. Eval: 0.0002 s/iter. Total: 0.0506 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:38:48 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.302732 (0.051261 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:38:48 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048459 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:38:48 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:38:48 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:38:48 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.309\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.703\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.214\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.190\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.384\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.163\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.440\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.464\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.505\n",
            "\u001b[32m[01/11 02:38:48 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.922 | 70.274 | 21.391 | 0.002 | 18.960 | 38.420 |\n",
            "\u001b[32m[01/11 02:38:48 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.922 |\n",
            "\u001b[32m[01/11 02:38:48 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:38:48 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:38:48 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:38:48 d2.evaluation.testing]: \u001b[0mcopypaste: 30.9221,70.2744,21.3910,0.0022,18.9602,38.4200\n",
            "\u001b[32m[01/11 02:38:48 d2.utils.events]: \u001b[0m eta: 0:37:09  iter: 2099  total_loss: 0.6262  loss_cls: 0.1478  loss_box_reg: 0.404  loss_rpn_cls: 0.02601  loss_rpn_loc: 0.03262    time: 0.7718  last_time: 0.7783  data_time: 0.0327  last_data_time: 0.0286   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:38:57 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:38:57 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:38:57 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:38:57 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:38:57 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:38:57 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:38:57 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:38:57 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:38:58 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0479 s/iter. Eval: 0.0003 s/iter. Total: 0.0492 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:39:03 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0014 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:39:08 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0014 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:39:10 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.511979 (0.052133 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:39:10 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049656 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:39:10 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:39:10 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:39:10 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.314\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.708\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.226\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.193\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.389\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.166\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.446\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.470\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.434\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.517\n",
            "\u001b[32m[01/11 02:39:10 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.405 | 70.786 | 22.619 | 0.002 | 19.349 | 38.929 |\n",
            "\u001b[32m[01/11 02:39:10 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.405 |\n",
            "\u001b[32m[01/11 02:39:10 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:39:10 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:39:10 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:39:10 d2.evaluation.testing]: \u001b[0mcopypaste: 31.4051,70.7862,22.6188,0.0021,19.3491,38.9295\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:39:18 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:39:18 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:39:18 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:39:18 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:39:18 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:39:18 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:39:18 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:39:18 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:39:19 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0019 s/iter. Inference: 0.0541 s/iter. Eval: 0.0003 s/iter. Total: 0.0563 s/iter. ETA=0:00:13\n",
            "\u001b[32m[01/11 02:39:24 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0017 s/iter. Inference: 0.0490 s/iter. Eval: 0.0002 s/iter. Total: 0.0510 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:39:29 d2.evaluation.evaluator]: \u001b[0mInference done 209/245. Dataloading: 0.0018 s/iter. Inference: 0.0488 s/iter. Eval: 0.0002 s/iter. Total: 0.0509 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:39:31 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.349673 (0.051457 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:39:31 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048687 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:39:31 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:39:31 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:39:31 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.326\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.712\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.234\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.203\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.403\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.171\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.456\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.482\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.433\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.538\n",
            "\u001b[32m[01/11 02:39:32 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.582 | 71.236 | 23.443 | 0.000 | 20.300 | 40.326 |\n",
            "\u001b[32m[01/11 02:39:32 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.582 |\n",
            "\u001b[32m[01/11 02:39:32 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:39:32 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:39:32 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:39:32 d2.evaluation.testing]: \u001b[0mcopypaste: 32.5818,71.2357,23.4433,0.0000,20.2999,40.3264\n",
            "\u001b[32m[01/11 02:39:32 d2.utils.events]: \u001b[0m eta: 0:36:53  iter: 2119  total_loss: 0.6533  loss_cls: 0.1625  loss_box_reg: 0.4465  loss_rpn_cls: 0.02272  loss_rpn_loc: 0.01695    time: 0.7719  last_time: 0.7017  data_time: 0.0309  last_data_time: 0.0287   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:39:40 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:39:40 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:39:40 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:39:40 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:39:40 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:39:40 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:39:40 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:39:40 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:39:41 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0025 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0527 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:39:46 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0025 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:39:51 d2.evaluation.evaluator]: \u001b[0mInference done 210/245. Dataloading: 0.0021 s/iter. Inference: 0.0484 s/iter. Eval: 0.0003 s/iter. Total: 0.0508 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:39:53 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.349495 (0.051456 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:39:53 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048417 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:39:53 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:39:53 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:39:53 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.305\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.709\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.206\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.192\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.376\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.442\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.468\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.438\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.511\n",
            "\u001b[32m[01/11 02:39:53 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.466 | 70.858 | 20.604 | 0.000 | 19.224 | 37.636 |\n",
            "\u001b[32m[01/11 02:39:53 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.466 |\n",
            "\u001b[32m[01/11 02:39:53 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:39:53 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:39:53 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:39:53 d2.evaluation.testing]: \u001b[0mcopypaste: 30.4664,70.8582,20.6043,0.0000,19.2239,37.6364\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:40:01 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:40:01 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:40:01 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:40:01 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:40:01 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:40:01 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:40:01 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:40:01 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:40:02 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:40:07 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0016 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:40:12 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0017 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:40:14 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.682158 (0.052842 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:40:14 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050083 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:40:14 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:40:14 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:40:14 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.29s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.318\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.721\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.221\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.196\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.395\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.167\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.454\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.485\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.536\n",
            "\u001b[32m[01/11 02:40:15 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.835 | 72.084 | 22.105 | 0.002 | 19.618 | 39.505 |\n",
            "\u001b[32m[01/11 02:40:15 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.835 |\n",
            "\u001b[32m[01/11 02:40:15 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:40:15 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:40:15 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:40:15 d2.evaluation.testing]: \u001b[0mcopypaste: 31.8354,72.0839,22.1046,0.0018,19.6177,39.5049\n",
            "\u001b[32m[01/11 02:40:15 d2.utils.events]: \u001b[0m eta: 0:36:38  iter: 2139  total_loss: 0.6008  loss_cls: 0.1503  loss_box_reg: 0.4109  loss_rpn_cls: 0.01539  loss_rpn_loc: 0.01331    time: 0.7719  last_time: 0.7723  data_time: 0.0327  last_data_time: 0.0239   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:40:24 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:40:24 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:40:24 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:40:24 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:40:24 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:40:24 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:40:24 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:40:24 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:40:25 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0487 s/iter. Eval: 0.0002 s/iter. Total: 0.0499 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:40:30 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0014 s/iter. Inference: 0.0488 s/iter. Eval: 0.0003 s/iter. Total: 0.0505 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:40:35 d2.evaluation.evaluator]: \u001b[0mInference done 209/245. Dataloading: 0.0014 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0508 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:40:37 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.356091 (0.051484 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:40:37 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049097 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:40:37 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:40:37 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:40:37 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.29s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.328\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.711\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.252\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.203\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.409\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.171\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.462\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.490\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.445\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.544\n",
            "\u001b[32m[01/11 02:40:37 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.841 | 71.094 | 25.244 | 0.002 | 20.292 | 40.926 |\n",
            "\u001b[32m[01/11 02:40:37 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.841 |\n",
            "\u001b[32m[01/11 02:40:37 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:40:37 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:40:37 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:40:37 d2.evaluation.testing]: \u001b[0mcopypaste: 32.8407,71.0938,25.2442,0.0016,20.2917,40.9263\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:40:45 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:40:45 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:40:45 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:40:45 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:40:45 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:40:45 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:40:45 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:40:45 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:40:46 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0031 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0528 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:40:51 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0019 s/iter. Inference: 0.0502 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:40:56 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0019 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:40:58 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.620649 (0.052586 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:40:58 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049612 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:40:58 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:40:58 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:40:58 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.301\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.700\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.205\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.195\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.369\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.165\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.429\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.455\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.432\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.493\n",
            "\u001b[32m[01/11 02:40:58 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.145 | 69.959 | 20.490 | 0.000 | 19.489 | 36.950 |\n",
            "\u001b[32m[01/11 02:40:59 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.145 |\n",
            "\u001b[32m[01/11 02:40:59 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:40:59 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:40:59 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:40:59 d2.evaluation.testing]: \u001b[0mcopypaste: 30.1449,69.9591,20.4897,0.0000,19.4892,36.9499\n",
            "\u001b[32m[01/11 02:40:59 d2.utils.events]: \u001b[0m eta: 0:36:23  iter: 2159  total_loss: 0.6494  loss_cls: 0.1569  loss_box_reg: 0.4277  loss_rpn_cls: 0.02343  loss_rpn_loc: 0.02164    time: 0.7722  last_time: 0.7241  data_time: 0.0301  last_data_time: 0.0458   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:41:07 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:41:07 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:41:07 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:41:07 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:41:07 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:41:07 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:41:07 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:41:07 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:41:08 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0512 s/iter. Eval: 0.0003 s/iter. Total: 0.0525 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:41:13 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0019 s/iter. Inference: 0.0509 s/iter. Eval: 0.0003 s/iter. Total: 0.0531 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:41:18 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0020 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0526 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:41:21 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.864564 (0.053602 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:41:21 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050635 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:41:21 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:41:21 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:41:21 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.321\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.711\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.228\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.196\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.400\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.456\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.483\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.437\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.537\n",
            "\u001b[32m[01/11 02:41:21 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.124 | 71.054 | 22.751 | 0.001 | 19.569 | 40.006 |\n",
            "\u001b[32m[01/11 02:41:21 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.124 |\n",
            "\u001b[32m[01/11 02:41:21 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:41:21 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:41:21 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:41:21 d2.evaluation.testing]: \u001b[0mcopypaste: 32.1236,71.0537,22.7505,0.0014,19.5687,40.0056\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:41:29 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:41:29 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:41:29 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:41:29 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:41:29 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:41:29 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:41:29 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:41:29 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:41:30 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:41:35 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0018 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:41:40 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0019 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:41:42 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.568305 (0.052368 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:41:42 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049580 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:41:42 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:41:42 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:41:42 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.32s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.303\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.695\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.207\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.185\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.379\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.163\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.439\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.473\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.024\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.435\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.520\n",
            "\u001b[32m[01/11 02:41:43 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.265 | 69.550 | 20.692 | 0.007 | 18.487 | 37.851 |\n",
            "\u001b[32m[01/11 02:41:43 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.265 |\n",
            "\u001b[32m[01/11 02:41:43 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:41:43 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:41:43 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:41:43 d2.evaluation.testing]: \u001b[0mcopypaste: 30.2650,69.5499,20.6919,0.0071,18.4873,37.8514\n",
            "\u001b[32m[01/11 02:41:43 d2.utils.events]: \u001b[0m eta: 0:36:08  iter: 2179  total_loss: 0.5862  loss_cls: 0.1573  loss_box_reg: 0.4072  loss_rpn_cls: 0.01535  loss_rpn_loc: 0.01257    time: 0.7723  last_time: 0.7736  data_time: 0.0345  last_data_time: 0.0268   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:41:51 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:41:51 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:41:51 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:41:51 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:41:51 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:41:51 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:41:51 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:41:51 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:41:52 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0486 s/iter. Eval: 0.0003 s/iter. Total: 0.0498 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:41:57 d2.evaluation.evaluator]: \u001b[0mInference done 105/245. Dataloading: 0.0020 s/iter. Inference: 0.0507 s/iter. Eval: 0.0003 s/iter. Total: 0.0531 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:42:02 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0018 s/iter. Inference: 0.0500 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:42:04 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.687214 (0.052863 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:42:04 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050051 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:42:04 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:42:04 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:42:04 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.327\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.711\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.250\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.205\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.406\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.171\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.459\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.484\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.434\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.540\n",
            "\u001b[32m[01/11 02:42:05 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.746 | 71.110 | 25.033 | 0.002 | 20.465 | 40.568 |\n",
            "\u001b[32m[01/11 02:42:05 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.746 |\n",
            "\u001b[32m[01/11 02:42:05 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:42:05 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:42:05 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:42:05 d2.evaluation.testing]: \u001b[0mcopypaste: 32.7463,71.1101,25.0330,0.0015,20.4651,40.5676\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:42:12 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:42:12 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:42:12 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:42:12 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:42:12 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:42:12 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:42:12 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:42:12 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:42:13 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0035 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0527 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:42:19 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0015 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:42:24 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0015 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:42:26 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.564653 (0.052353 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:42:26 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049876 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:42:26 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:42:26 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:42:26 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.30s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.318\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.703\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.231\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.202\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.391\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.169\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.453\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.484\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.452\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.529\n",
            "\u001b[32m[01/11 02:42:26 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.800 | 70.308 | 23.137 | 0.002 | 20.224 | 39.107 |\n",
            "\u001b[32m[01/11 02:42:26 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.800 |\n",
            "\u001b[32m[01/11 02:42:26 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:42:26 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:42:26 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:42:26 d2.evaluation.testing]: \u001b[0mcopypaste: 31.7997,70.3080,23.1369,0.0025,20.2237,39.1070\n",
            "\u001b[32m[01/11 02:42:26 d2.utils.events]: \u001b[0m eta: 0:35:53  iter: 2199  total_loss: 0.6544  loss_cls: 0.1704  loss_box_reg: 0.4472  loss_rpn_cls: 0.02213  loss_rpn_loc: 0.02348    time: 0.7723  last_time: 0.7637  data_time: 0.0340  last_data_time: 0.0221   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:42:34 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:42:34 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:42:34 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:42:34 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:42:34 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:42:34 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:42:34 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:42:34 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:42:35 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0507 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:42:40 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0014 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0508 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:42:45 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0015 s/iter. Inference: 0.0500 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:42:48 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.538097 (0.052242 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:42:48 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049741 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:42:48 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:42:48 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:42:48 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.31s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.304\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.705\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.203\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.194\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.373\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.167\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.437\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.469\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.452\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.504\n",
            "\u001b[32m[01/11 02:42:48 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.405 | 70.525 | 20.253 | 0.002 | 19.377 | 37.272 |\n",
            "\u001b[32m[01/11 02:42:48 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.405 |\n",
            "\u001b[32m[01/11 02:42:48 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:42:48 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:42:48 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:42:48 d2.evaluation.testing]: \u001b[0mcopypaste: 30.4045,70.5254,20.2528,0.0022,19.3769,37.2722\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:42:56 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:42:56 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:42:56 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:42:56 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:42:56 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:42:56 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:42:56 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:42:56 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:42:57 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0517 s/iter. Eval: 0.0003 s/iter. Total: 0.0528 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:43:02 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0018 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:43:07 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0018 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:43:09 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.448326 (0.051868 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:43:09 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048987 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:43:09 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:43:09 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:43:09 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.707\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.220\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.202\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.390\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.444\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.474\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.446\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.517\n",
            "\u001b[32m[01/11 02:43:09 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.693 | 70.709 | 21.977 | 0.001 | 20.224 | 38.967 |\n",
            "\u001b[32m[01/11 02:43:09 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.693 |\n",
            "\u001b[32m[01/11 02:43:09 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:43:09 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:43:09 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:43:09 d2.evaluation.testing]: \u001b[0mcopypaste: 31.6929,70.7092,21.9767,0.0012,20.2236,38.9672\n",
            "\u001b[32m[01/11 02:43:09 d2.utils.events]: \u001b[0m eta: 0:35:37  iter: 2219  total_loss: 0.635  loss_cls: 0.1446  loss_box_reg: 0.4399  loss_rpn_cls: 0.01733  loss_rpn_loc: 0.01714    time: 0.7722  last_time: 0.7644  data_time: 0.0311  last_data_time: 0.0251   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:43:18 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:43:18 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:43:18 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:43:18 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:43:18 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:43:18 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:43:18 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:43:18 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:43:19 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0485 s/iter. Eval: 0.0002 s/iter. Total: 0.0496 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:43:24 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0021 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:43:29 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0020 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:43:31 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.539223 (0.052247 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:43:31 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049359 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:43:31 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:43:31 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:43:31 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.313\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.706\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.218\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.200\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.383\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.165\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.470\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.441\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.514\n",
            "\u001b[32m[01/11 02:43:32 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.254 | 70.573 | 21.815 | 0.000 | 19.956 | 38.337 |\n",
            "\u001b[32m[01/11 02:43:32 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.254 |\n",
            "\u001b[32m[01/11 02:43:32 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:43:32 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:43:32 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:43:32 d2.evaluation.testing]: \u001b[0mcopypaste: 31.2538,70.5734,21.8150,0.0000,19.9558,38.3374\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:43:39 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:43:39 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:43:39 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:43:39 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:43:39 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:43:39 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:43:39 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:43:39 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:43:40 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0013 s/iter. Inference: 0.0485 s/iter. Eval: 0.0003 s/iter. Total: 0.0501 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:43:45 d2.evaluation.evaluator]: \u001b[0mInference done 105/245. Dataloading: 0.0022 s/iter. Inference: 0.0508 s/iter. Eval: 0.0003 s/iter. Total: 0.0533 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:43:50 d2.evaluation.evaluator]: \u001b[0mInference done 202/245. Dataloading: 0.0019 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0526 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:43:53 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.763995 (0.053183 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:43:53 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050242 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:43:53 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:43:53 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:43:53 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.327\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.720\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.237\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.203\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.405\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.169\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.457\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.485\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.430\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.545\n",
            "\u001b[32m[01/11 02:43:53 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.685 | 72.004 | 23.723 | 0.002 | 20.305 | 40.495 |\n",
            "\u001b[32m[01/11 02:43:53 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.685 |\n",
            "\u001b[32m[01/11 02:43:53 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:43:53 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:43:53 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:43:53 d2.evaluation.testing]: \u001b[0mcopypaste: 32.6853,72.0040,23.7230,0.0018,20.3050,40.4955\n",
            "\u001b[32m[01/11 02:43:53 d2.utils.events]: \u001b[0m eta: 0:35:22  iter: 2239  total_loss: 0.6283  loss_cls: 0.1575  loss_box_reg: 0.4121  loss_rpn_cls: 0.01829  loss_rpn_loc: 0.01558    time: 0.7723  last_time: 0.7021  data_time: 0.0334  last_data_time: 0.0255   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:44:01 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:44:01 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:44:01 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:44:01 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:44:01 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:44:01 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:44:01 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:44:01 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:44:02 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:44:07 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0020 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:44:13 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0019 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:44:15 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.452934 (0.051887 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:44:15 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048944 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:44:15 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:44:15 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:44:15 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.321\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.707\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.231\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.199\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.397\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.166\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.453\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.482\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.439\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.533\n",
            "\u001b[32m[01/11 02:44:15 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.108 | 70.650 | 23.108 | 0.004 | 19.891 | 39.665 |\n",
            "\u001b[32m[01/11 02:44:15 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.108 |\n",
            "\u001b[32m[01/11 02:44:15 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:44:15 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:44:15 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:44:15 d2.evaluation.testing]: \u001b[0mcopypaste: 32.1084,70.6504,23.1081,0.0037,19.8907,39.6649\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:44:23 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:44:23 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:44:23 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:44:23 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:44:23 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:44:23 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:44:23 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:44:23 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:44:24 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0490 s/iter. Eval: 0.0002 s/iter. Total: 0.0502 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:44:29 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0022 s/iter. Inference: 0.0500 s/iter. Eval: 0.0003 s/iter. Total: 0.0526 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:44:34 d2.evaluation.evaluator]: \u001b[0mInference done 202/245. Dataloading: 0.0021 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0525 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:44:36 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.806334 (0.053360 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:44:36 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049990 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:44:36 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:44:36 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:44:36 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.308\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.703\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.218\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.203\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.376\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.163\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.437\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.463\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.434\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.504\n",
            "\u001b[32m[01/11 02:44:36 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.769 | 70.319 | 21.821 | 0.000 | 20.306 | 37.586 |\n",
            "\u001b[32m[01/11 02:44:36 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.769 |\n",
            "\u001b[32m[01/11 02:44:36 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:44:36 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:44:36 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:44:36 d2.evaluation.testing]: \u001b[0mcopypaste: 30.7691,70.3191,21.8215,0.0000,20.3060,37.5864\n",
            "\u001b[32m[01/11 02:44:36 d2.utils.events]: \u001b[0m eta: 0:35:07  iter: 2259  total_loss: 0.6231  loss_cls: 0.1414  loss_box_reg: 0.4269  loss_rpn_cls: 0.02185  loss_rpn_loc: 0.02026    time: 0.7722  last_time: 0.7710  data_time: 0.0319  last_data_time: 0.0239   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:44:45 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:44:45 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:44:45 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:44:45 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:44:45 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:44:45 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:44:45 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:44:45 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:44:46 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0488 s/iter. Eval: 0.0002 s/iter. Total: 0.0502 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:44:51 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0018 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:44:56 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0017 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:44:58 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.470615 (0.051961 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:44:58 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049379 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:44:58 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:44:58 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:44:58 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.322\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.712\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.236\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.201\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.398\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.459\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.489\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.452\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.536\n",
            "\u001b[32m[01/11 02:44:58 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.187 | 71.184 | 23.629 | 0.003 | 20.060 | 39.761 |\n",
            "\u001b[32m[01/11 02:44:58 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.187 |\n",
            "\u001b[32m[01/11 02:44:58 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:44:58 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:44:58 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:44:58 d2.evaluation.testing]: \u001b[0mcopypaste: 32.1870,71.1839,23.6295,0.0033,20.0603,39.7614\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:45:06 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:45:06 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:45:06 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:45:06 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:45:06 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:45:06 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:45:06 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:45:06 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:45:07 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0472 s/iter. Eval: 0.0002 s/iter. Total: 0.0484 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:45:12 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0015 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0509 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:45:17 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0016 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:45:19 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.573025 (0.052388 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:45:19 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049803 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:45:19 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:45:19 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:45:19 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.316\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.708\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.229\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.201\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.389\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.166\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.453\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.476\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.521\n",
            "\u001b[32m[01/11 02:45:20 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.592 | 70.808 | 22.877 | 0.013 | 20.065 | 38.892 |\n",
            "\u001b[32m[01/11 02:45:20 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.592 |\n",
            "\u001b[32m[01/11 02:45:20 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:45:20 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:45:20 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:45:20 d2.evaluation.testing]: \u001b[0mcopypaste: 31.5923,70.8081,22.8773,0.0130,20.0654,38.8920\n",
            "\u001b[32m[01/11 02:45:20 d2.utils.events]: \u001b[0m eta: 0:34:51  iter: 2279  total_loss: 0.6512  loss_cls: 0.1643  loss_box_reg: 0.4058  loss_rpn_cls: 0.02341  loss_rpn_loc: 0.02463    time: 0.7723  last_time: 0.7695  data_time: 0.0315  last_data_time: 0.0245   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:45:28 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:45:28 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:45:28 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:45:28 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:45:28 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:45:28 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:45:28 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:45:28 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:45:29 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0013 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:45:34 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0021 s/iter. Inference: 0.0507 s/iter. Eval: 0.0003 s/iter. Total: 0.0531 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:45:39 d2.evaluation.evaluator]: \u001b[0mInference done 201/245. Dataloading: 0.0019 s/iter. Inference: 0.0507 s/iter. Eval: 0.0003 s/iter. Total: 0.0529 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:45:42 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.831383 (0.053464 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:45:42 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050588 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:45:42 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:45:42 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:45:42 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.302\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.698\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.210\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.196\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.370\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.161\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.438\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.459\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.434\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.499\n",
            "\u001b[32m[01/11 02:45:42 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.195 | 69.802 | 20.997 | 0.000 | 19.582 | 37.049 |\n",
            "\u001b[32m[01/11 02:45:42 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.195 |\n",
            "\u001b[32m[01/11 02:45:42 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:45:42 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:45:42 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:45:42 d2.evaluation.testing]: \u001b[0mcopypaste: 30.1954,69.8022,20.9968,0.0000,19.5823,37.0491\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:45:50 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:45:50 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:45:50 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:45:50 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:45:50 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:45:50 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:45:50 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:45:50 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:45:51 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0488 s/iter. Eval: 0.0002 s/iter. Total: 0.0501 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:45:56 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0016 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:46:01 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0015 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:46:03 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.460589 (0.051919 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:46:03 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049391 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:46:03 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:46:03 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:46:03 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.309\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.703\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.219\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.191\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.382\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.165\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.440\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.463\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.431\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.508\n",
            "\u001b[32m[01/11 02:46:03 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.884 | 70.330 | 21.864 | 0.000 | 19.110 | 38.185 |\n",
            "\u001b[32m[01/11 02:46:03 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.884 |\n",
            "\u001b[32m[01/11 02:46:03 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:46:03 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:46:03 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:46:03 d2.evaluation.testing]: \u001b[0mcopypaste: 30.8840,70.3300,21.8636,0.0000,19.1100,38.1848\n",
            "\u001b[32m[01/11 02:46:03 d2.utils.events]: \u001b[0m eta: 0:34:36  iter: 2299  total_loss: 0.5838  loss_cls: 0.1408  loss_box_reg: 0.393  loss_rpn_cls: 0.01653  loss_rpn_loc: 0.01631    time: 0.7723  last_time: 0.7630  data_time: 0.0316  last_data_time: 0.0263   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:46:11 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:46:11 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:46:11 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:46:11 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:46:11 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:46:11 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:46:11 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:46:11 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:46:12 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0013 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0509 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:46:17 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0019 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:46:22 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0021 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:46:25 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.589879 (0.052458 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:46:25 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049367 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:46:25 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:46:25 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:46:25 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.41s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.29s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.302\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.690\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.200\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.181\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.378\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.163\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.433\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.468\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.446\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.507\n",
            "\u001b[32m[01/11 02:46:26 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.159 | 68.985 | 19.960 | 0.000 | 18.052 | 37.754 |\n",
            "\u001b[32m[01/11 02:46:26 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.159 |\n",
            "\u001b[32m[01/11 02:46:26 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:46:26 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:46:26 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:46:26 d2.evaluation.testing]: \u001b[0mcopypaste: 30.1587,68.9855,19.9596,0.0000,18.0519,37.7542\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:46:33 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:46:33 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:46:33 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:46:33 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:46:33 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:46:33 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:46:33 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:46:33 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:46:34 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0488 s/iter. Eval: 0.0002 s/iter. Total: 0.0501 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:46:39 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0017 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:46:44 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0018 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:46:47 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.515270 (0.052147 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:46:47 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049344 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:46:47 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:46:47 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:46:47 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.328\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.707\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.250\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.202\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.409\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.458\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.489\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.435\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.548\n",
            "\u001b[32m[01/11 02:46:47 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.780 | 70.698 | 24.951 | 0.000 | 20.238 | 40.887 |\n",
            "\u001b[32m[01/11 02:46:47 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.780 |\n",
            "\u001b[32m[01/11 02:46:47 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:46:47 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:46:47 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:46:47 d2.evaluation.testing]: \u001b[0mcopypaste: 32.7801,70.6982,24.9511,0.0000,20.2380,40.8867\n",
            "\u001b[32m[01/11 02:46:47 d2.utils.events]: \u001b[0m eta: 0:34:20  iter: 2319  total_loss: 0.6373  loss_cls: 0.1499  loss_box_reg: 0.4306  loss_rpn_cls: 0.01491  loss_rpn_loc: 0.01535    time: 0.7723  last_time: 0.7248  data_time: 0.0322  last_data_time: 0.0268   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:46:55 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:46:55 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:46:55 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:46:55 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:46:55 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:46:55 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:46:55 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:46:55 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:46:56 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0503 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:47:01 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0019 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:47:06 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0018 s/iter. Inference: 0.0490 s/iter. Eval: 0.0002 s/iter. Total: 0.0511 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:47:08 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.438976 (0.051829 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:47:08 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048956 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:47:08 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:47:09 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:47:09 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.708\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.223\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.197\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.398\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.169\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.447\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.481\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.534\n",
            "\u001b[32m[01/11 02:47:09 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.941 | 70.839 | 22.281 | 0.000 | 19.689 | 39.786 |\n",
            "\u001b[32m[01/11 02:47:09 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.941 |\n",
            "\u001b[32m[01/11 02:47:09 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:47:09 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:47:09 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:47:09 d2.evaluation.testing]: \u001b[0mcopypaste: 31.9409,70.8394,22.2812,0.0000,19.6893,39.7859\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:47:16 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:47:16 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:47:16 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:47:16 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:47:16 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:47:16 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:47:16 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:47:16 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:47:17 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0025 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:47:23 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0025 s/iter. Inference: 0.0487 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:47:28 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0021 s/iter. Inference: 0.0488 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:47:30 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.433062 (0.051804 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:47:30 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048739 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:47:30 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:47:30 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:47:30 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.29s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.320\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.708\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.235\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.198\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.399\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.454\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.488\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.456\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.534\n",
            "\u001b[32m[01/11 02:47:30 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.040 | 70.810 | 23.512 | 0.007 | 19.792 | 39.871 |\n",
            "\u001b[32m[01/11 02:47:30 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.040 |\n",
            "\u001b[32m[01/11 02:47:30 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:47:30 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:47:30 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:47:30 d2.evaluation.testing]: \u001b[0mcopypaste: 32.0399,70.8104,23.5124,0.0068,19.7925,39.8710\n",
            "\u001b[32m[01/11 02:47:30 d2.utils.events]: \u001b[0m eta: 0:34:05  iter: 2339  total_loss: 0.6574  loss_cls: 0.1621  loss_box_reg: 0.4386  loss_rpn_cls: 0.02427  loss_rpn_loc: 0.02814    time: 0.7722  last_time: 0.7707  data_time: 0.0329  last_data_time: 0.0224   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:47:38 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:47:38 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:47:38 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:47:38 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:47:38 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:47:38 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:47:38 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:47:38 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:47:39 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0484 s/iter. Eval: 0.0002 s/iter. Total: 0.0498 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:47:44 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0019 s/iter. Inference: 0.0502 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:47:49 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0018 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:47:51 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.547443 (0.052281 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:47:51 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049459 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:47:51 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:47:51 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:47:51 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.29s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.315\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.709\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.222\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.193\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.390\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.448\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.481\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.458\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.520\n",
            "\u001b[32m[01/11 02:47:52 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.512 | 70.900 | 22.152 | 0.002 | 19.337 | 39.034 |\n",
            "\u001b[32m[01/11 02:47:52 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.512 |\n",
            "\u001b[32m[01/11 02:47:52 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:47:52 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:47:52 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:47:52 d2.evaluation.testing]: \u001b[0mcopypaste: 31.5115,70.9001,22.1525,0.0015,19.3371,39.0342\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:48:00 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:48:00 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:48:00 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:48:00 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:48:00 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:48:00 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:48:00 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:48:00 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:48:01 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0035 s/iter. Inference: 0.0487 s/iter. Eval: 0.0002 s/iter. Total: 0.0525 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:48:06 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0022 s/iter. Inference: 0.0488 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:48:11 d2.evaluation.evaluator]: \u001b[0mInference done 211/245. Dataloading: 0.0019 s/iter. Inference: 0.0482 s/iter. Eval: 0.0003 s/iter. Total: 0.0504 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:48:12 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.232268 (0.050968 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:48:12 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048167 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:48:13 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:48:13 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:48:13 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.329\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.720\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.236\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.201\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.408\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.462\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.493\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.450\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.545\n",
            "\u001b[32m[01/11 02:48:13 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.874 | 71.970 | 23.559 | 0.004 | 20.110 | 40.805 |\n",
            "\u001b[32m[01/11 02:48:13 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.874 |\n",
            "\u001b[32m[01/11 02:48:13 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:48:13 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:48:13 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:48:13 d2.evaluation.testing]: \u001b[0mcopypaste: 32.8743,71.9702,23.5595,0.0038,20.1102,40.8047\n",
            "\u001b[32m[01/11 02:48:13 d2.utils.events]: \u001b[0m eta: 0:33:49  iter: 2359  total_loss: 0.6028  loss_cls: 0.1629  loss_box_reg: 0.4011  loss_rpn_cls: 0.02387  loss_rpn_loc: 0.01894    time: 0.7721  last_time: 0.7097  data_time: 0.0307  last_data_time: 0.0294   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:48:21 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:48:21 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:48:21 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:48:21 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:48:21 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:48:21 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:48:21 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:48:21 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:48:22 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0027 s/iter. Inference: 0.0497 s/iter. Eval: 0.0002 s/iter. Total: 0.0526 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:48:27 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0017 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:48:32 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0016 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:48:35 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.643063 (0.052679 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:48:35 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050055 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:48:35 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:48:35 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:48:35 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.305\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.697\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.208\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.189\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.376\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.160\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.469\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.448\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.507\n",
            "\u001b[32m[01/11 02:48:35 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.537 | 69.739 | 20.789 | 0.001 | 18.936 | 37.582 |\n",
            "\u001b[32m[01/11 02:48:35 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.537 |\n",
            "\u001b[32m[01/11 02:48:35 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:48:35 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:48:35 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:48:35 d2.evaluation.testing]: \u001b[0mcopypaste: 30.5373,69.7392,20.7889,0.0014,18.9358,37.5822\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:48:42 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:48:42 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:48:42 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:48:42 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:48:42 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:48:42 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:48:42 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:48:42 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:48:44 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0475 s/iter. Eval: 0.0003 s/iter. Total: 0.0488 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:48:49 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0019 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:48:54 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0023 s/iter. Inference: 0.0488 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:48:56 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.551370 (0.052297 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:48:56 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048737 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:48:56 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:48:56 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:48:56 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.309\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.703\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.219\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.195\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.381\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.165\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.442\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.468\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.439\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.511\n",
            "\u001b[32m[01/11 02:48:56 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.896 | 70.293 | 21.861 | 0.002 | 19.537 | 38.116 |\n",
            "\u001b[32m[01/11 02:48:56 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.896 |\n",
            "\u001b[32m[01/11 02:48:56 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:48:56 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:48:56 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:48:56 d2.evaluation.testing]: \u001b[0mcopypaste: 30.8960,70.2932,21.8608,0.0018,19.5372,38.1159\n",
            "\u001b[32m[01/11 02:48:56 d2.utils.events]: \u001b[0m eta: 0:33:34  iter: 2379  total_loss: 0.6315  loss_cls: 0.1525  loss_box_reg: 0.4566  loss_rpn_cls: 0.02253  loss_rpn_loc: 0.01977    time: 0.7721  last_time: 0.6967  data_time: 0.0333  last_data_time: 0.0263   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:49:04 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:49:04 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:49:04 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:49:04 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:49:04 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:49:04 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:49:04 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:49:04 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:49:05 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0031 s/iter. Inference: 0.0482 s/iter. Eval: 0.0002 s/iter. Total: 0.0515 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:49:10 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0017 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:49:16 d2.evaluation.evaluator]: \u001b[0mInference done 209/245. Dataloading: 0.0017 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0509 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:49:17 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.345386 (0.051439 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:49:17 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048786 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:49:18 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:49:18 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:49:18 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.46s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.311\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.703\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.214\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.195\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.385\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.164\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.445\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.470\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.430\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.519\n",
            "\u001b[32m[01/11 02:49:18 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.139 | 70.312 | 21.355 | 0.000 | 19.528 | 38.519 |\n",
            "\u001b[32m[01/11 02:49:18 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.139 |\n",
            "\u001b[32m[01/11 02:49:18 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:49:18 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:49:18 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:49:18 d2.evaluation.testing]: \u001b[0mcopypaste: 31.1388,70.3118,21.3551,0.0000,19.5281,38.5189\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:49:26 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:49:26 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:49:26 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:49:26 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:49:26 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:49:26 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:49:26 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:49:26 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:49:27 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0023 s/iter. Inference: 0.0503 s/iter. Eval: 0.0002 s/iter. Total: 0.0529 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:49:32 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0016 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:49:37 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0017 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:49:39 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.599281 (0.052497 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:49:39 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049817 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:49:39 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:49:39 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:49:39 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.325\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.707\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.236\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.205\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.399\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.169\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.458\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.488\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.462\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.529\n",
            "\u001b[32m[01/11 02:49:40 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.476 | 70.703 | 23.629 | 0.003 | 20.515 | 39.934 |\n",
            "\u001b[32m[01/11 02:49:40 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.476 |\n",
            "\u001b[32m[01/11 02:49:40 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:49:40 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:49:40 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:49:40 d2.evaluation.testing]: \u001b[0mcopypaste: 32.4758,70.7028,23.6288,0.0026,20.5152,39.9336\n",
            "\u001b[32m[01/11 02:49:40 d2.utils.events]: \u001b[0m eta: 0:33:19  iter: 2399  total_loss: 0.6574  loss_cls: 0.1543  loss_box_reg: 0.4354  loss_rpn_cls: 0.02742  loss_rpn_loc: 0.01797    time: 0.7720  last_time: 0.6675  data_time: 0.0342  last_data_time: 0.0317   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:49:48 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:49:48 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:49:48 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:49:48 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:49:48 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:49:48 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:49:48 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:49:48 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:49:49 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:49:54 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0018 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:49:59 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0016 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:50:01 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.402955 (0.051679 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:50:01 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048943 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:50:01 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:50:01 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:50:01 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.306\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.705\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.222\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.194\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.377\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.160\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.469\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.451\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.505\n",
            "\u001b[32m[01/11 02:50:02 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.639 | 70.467 | 22.204 | 0.000 | 19.401 | 37.745 |\n",
            "\u001b[32m[01/11 02:50:02 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.639 |\n",
            "\u001b[32m[01/11 02:50:02 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:50:02 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:50:02 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:50:02 d2.evaluation.testing]: \u001b[0mcopypaste: 30.6389,70.4671,22.2043,0.0000,19.4008,37.7451\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:50:09 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:50:09 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:50:09 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:50:09 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:50:09 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:50:09 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:50:09 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:50:09 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:50:10 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0025 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:50:15 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0017 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:50:20 d2.evaluation.evaluator]: \u001b[0mInference done 210/245. Dataloading: 0.0017 s/iter. Inference: 0.0488 s/iter. Eval: 0.0003 s/iter. Total: 0.0508 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:50:22 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.396558 (0.051652 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:50:22 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048929 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:50:22 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:50:22 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:50:22 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.334\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.728\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.254\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.215\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.413\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.460\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.489\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.435\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.548\n",
            "\u001b[32m[01/11 02:50:23 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.440 | 72.780 | 25.377 | 0.003 | 21.510 | 41.270 |\n",
            "\u001b[32m[01/11 02:50:23 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.440 |\n",
            "\u001b[32m[01/11 02:50:23 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:50:23 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:50:23 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:50:23 d2.evaluation.testing]: \u001b[0mcopypaste: 33.4401,72.7804,25.3775,0.0034,21.5102,41.2700\n",
            "\u001b[32m[01/11 02:50:23 d2.utils.events]: \u001b[0m eta: 0:33:03  iter: 2419  total_loss: 0.6206  loss_cls: 0.1532  loss_box_reg: 0.431  loss_rpn_cls: 0.02179  loss_rpn_loc: 0.01925    time: 0.7721  last_time: 0.7800  data_time: 0.0351  last_data_time: 0.0350   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:50:31 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:50:31 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:50:31 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:50:31 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:50:31 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:50:31 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:50:31 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:50:31 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:50:32 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0480 s/iter. Eval: 0.0003 s/iter. Total: 0.0492 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:50:37 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0018 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:50:42 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0016 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:50:44 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.403604 (0.051682 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:50:44 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049074 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:50:44 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:50:44 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:50:44 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.307\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.707\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.220\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.192\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.377\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.163\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.434\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.464\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.435\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.507\n",
            "\u001b[32m[01/11 02:50:44 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.651 | 70.673 | 22.041 | 0.000 | 19.247 | 37.679 |\n",
            "\u001b[32m[01/11 02:50:44 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.651 |\n",
            "\u001b[32m[01/11 02:50:44 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:50:44 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:50:44 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:50:44 d2.evaluation.testing]: \u001b[0mcopypaste: 30.6513,70.6726,22.0412,0.0000,19.2473,37.6792\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:50:52 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:50:52 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:50:52 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:50:52 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:50:52 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:50:52 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:50:52 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:50:52 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:50:53 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0040 s/iter. Inference: 0.0521 s/iter. Eval: 0.0003 s/iter. Total: 0.0565 s/iter. ETA=0:00:13\n",
            "\u001b[32m[01/11 02:50:58 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0018 s/iter. Inference: 0.0504 s/iter. Eval: 0.0003 s/iter. Total: 0.0525 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:51:03 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0021 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:51:06 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.655423 (0.052731 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:51:06 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049679 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:51:06 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:51:06 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:51:06 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.31s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.330\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.709\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.248\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.199\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.412\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.171\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.458\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.499\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.029\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.449\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.555\n",
            "\u001b[32m[01/11 02:51:06 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.014 | 70.914 | 24.754 | 0.012 | 19.868 | 41.238 |\n",
            "\u001b[32m[01/11 02:51:06 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.014 |\n",
            "\u001b[32m[01/11 02:51:06 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:51:06 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:51:06 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:51:06 d2.evaluation.testing]: \u001b[0mcopypaste: 33.0142,70.9139,24.7539,0.0120,19.8684,41.2379\n",
            "\u001b[32m[01/11 02:51:06 d2.utils.events]: \u001b[0m eta: 0:32:48  iter: 2439  total_loss: 0.5756  loss_cls: 0.1385  loss_box_reg: 0.4011  loss_rpn_cls: 0.01675  loss_rpn_loc: 0.01276    time: 0.7721  last_time: 0.7889  data_time: 0.0327  last_data_time: 0.0355   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:51:14 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:51:14 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:51:14 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:51:14 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:51:14 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:51:14 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:51:14 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:51:14 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:51:15 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0541 s/iter. Eval: 0.0002 s/iter. Total: 0.0555 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:51:20 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0023 s/iter. Inference: 0.0499 s/iter. Eval: 0.0002 s/iter. Total: 0.0525 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:51:25 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0022 s/iter. Inference: 0.0493 s/iter. Eval: 0.0002 s/iter. Total: 0.0518 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:51:28 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.636773 (0.052653 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:51:28 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049521 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:51:28 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:51:28 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:51:28 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.324\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.713\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.240\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.200\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.402\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.169\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.450\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.482\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.431\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.538\n",
            "\u001b[32m[01/11 02:51:28 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.425 | 71.315 | 24.035 | 0.000 | 19.992 | 40.177 |\n",
            "\u001b[32m[01/11 02:51:28 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.425 |\n",
            "\u001b[32m[01/11 02:51:28 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:51:28 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:51:28 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:51:28 d2.evaluation.testing]: \u001b[0mcopypaste: 32.4248,71.3154,24.0354,0.0000,19.9922,40.1773\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:51:36 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:51:36 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:51:36 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:51:36 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:51:36 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:51:36 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:51:36 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:51:36 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:51:37 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0506 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:51:42 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0021 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:51:47 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0019 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:51:49 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.401081 (0.051671 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:51:49 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048859 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:51:49 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:51:49 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:51:49 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.22s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.299\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.704\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.194\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.193\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.366\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.163\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.426\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.448\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.419\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.489\n",
            "\u001b[32m[01/11 02:51:49 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.893 | 70.395 | 19.420 | 0.000 | 19.310 | 36.644 |\n",
            "\u001b[32m[01/11 02:51:49 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.893 |\n",
            "\u001b[32m[01/11 02:51:49 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:51:49 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:51:49 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:51:49 d2.evaluation.testing]: \u001b[0mcopypaste: 29.8927,70.3948,19.4199,0.0000,19.3096,36.6442\n",
            "\u001b[32m[01/11 02:51:49 d2.utils.events]: \u001b[0m eta: 0:32:33  iter: 2459  total_loss: 0.6369  loss_cls: 0.1604  loss_box_reg: 0.4091  loss_rpn_cls: 0.0188  loss_rpn_loc: 0.01725    time: 0.7722  last_time: 0.7807  data_time: 0.0359  last_data_time: 0.0320   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:51:57 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:51:57 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:51:58 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:51:58 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:51:58 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:51:58 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:51:58 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:51:58 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:51:59 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0517 s/iter. Eval: 0.0003 s/iter. Total: 0.0531 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:52:04 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0016 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:52:09 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0020 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:52:11 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.523863 (0.052183 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:52:11 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049264 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:52:11 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:52:11 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:52:11 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.41s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.327\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.722\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.238\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.210\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.402\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.171\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.455\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.489\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.455\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.535\n",
            "\u001b[32m[01/11 02:52:12 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.677 | 72.176 | 23.825 | 0.003 | 20.962 | 40.196 |\n",
            "\u001b[32m[01/11 02:52:12 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.677 |\n",
            "\u001b[32m[01/11 02:52:12 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:52:12 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:52:12 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:52:12 d2.evaluation.testing]: \u001b[0mcopypaste: 32.6768,72.1756,23.8253,0.0028,20.9618,40.1964\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:52:19 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:52:19 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:52:19 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:52:19 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:52:19 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:52:19 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:52:19 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:52:19 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:52:20 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0472 s/iter. Eval: 0.0003 s/iter. Total: 0.0483 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:52:25 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0017 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:52:30 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0016 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:52:33 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.531220 (0.052213 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:52:33 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049339 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:52:33 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:52:33 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:52:33 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.30s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.323\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.715\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.237\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.210\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.396\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.453\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.490\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.461\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.532\n",
            "\u001b[32m[01/11 02:52:33 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.336 | 71.521 | 23.669 | 0.003 | 21.047 | 39.587 |\n",
            "\u001b[32m[01/11 02:52:33 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.336 |\n",
            "\u001b[32m[01/11 02:52:33 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:52:33 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:52:33 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:52:33 d2.evaluation.testing]: \u001b[0mcopypaste: 32.3357,71.5212,23.6685,0.0029,21.0472,39.5869\n",
            "\u001b[32m[01/11 02:52:33 d2.utils.events]: \u001b[0m eta: 0:32:18  iter: 2479  total_loss: 0.6538  loss_cls: 0.1632  loss_box_reg: 0.4335  loss_rpn_cls: 0.02347  loss_rpn_loc: 0.02803    time: 0.7722  last_time: 0.7733  data_time: 0.0338  last_data_time: 0.0302   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:52:42 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:52:42 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:52:42 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:52:42 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:52:42 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:52:42 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:52:42 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:52:42 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:52:43 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0505 s/iter. Eval: 0.0002 s/iter. Total: 0.0518 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:52:48 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0015 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:52:53 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0015 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:52:55 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.518441 (0.052160 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:52:55 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049684 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:52:55 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:52:55 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:52:55 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.312\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.713\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.221\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.201\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.383\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.163\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.445\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.476\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.446\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.520\n",
            "\u001b[32m[01/11 02:52:55 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.220 | 71.288 | 22.080 | 0.000 | 20.105 | 38.325 |\n",
            "\u001b[32m[01/11 02:52:55 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.220 |\n",
            "\u001b[32m[01/11 02:52:55 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:52:55 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:52:55 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:52:55 d2.evaluation.testing]: \u001b[0mcopypaste: 31.2197,71.2875,22.0795,0.0000,20.1048,38.3253\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:53:03 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:53:03 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:53:03 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:53:03 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:53:03 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:53:03 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:53:03 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:53:03 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:53:04 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0476 s/iter. Eval: 0.0002 s/iter. Total: 0.0490 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:53:09 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0017 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:53:14 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0017 s/iter. Inference: 0.0490 s/iter. Eval: 0.0002 s/iter. Total: 0.0510 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:53:16 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.451673 (0.051882 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:53:16 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049127 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:53:16 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:53:16 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:53:16 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.320\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.715\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.227\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.205\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.394\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.167\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.454\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.479\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.435\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.531\n",
            "\u001b[32m[01/11 02:53:16 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.016 | 71.460 | 22.737 | 0.002 | 20.500 | 39.406 |\n",
            "\u001b[32m[01/11 02:53:16 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.016 |\n",
            "\u001b[32m[01/11 02:53:16 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:53:16 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:53:16 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:53:16 d2.evaluation.testing]: \u001b[0mcopypaste: 32.0161,71.4601,22.7375,0.0021,20.4997,39.4061\n",
            "\u001b[32m[01/11 02:53:16 d2.utils.events]: \u001b[0m eta: 0:32:02  iter: 2499  total_loss: 0.6306  loss_cls: 0.1582  loss_box_reg: 0.4275  loss_rpn_cls: 0.02164  loss_rpn_loc: 0.01703    time: 0.7722  last_time: 0.7682  data_time: 0.0341  last_data_time: 0.0239   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:53:24 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:53:24 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:53:24 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:53:24 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:53:24 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:53:24 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:53:24 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:53:24 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:53:25 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0492 s/iter. Eval: 0.0002 s/iter. Total: 0.0505 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:53:31 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0022 s/iter. Inference: 0.0504 s/iter. Eval: 0.0003 s/iter. Total: 0.0530 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:53:36 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0018 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:53:38 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.660998 (0.052754 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:53:38 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049846 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:53:38 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:53:38 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:53:38 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.22s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.311\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.707\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.231\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.197\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.381\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.166\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.440\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.462\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.430\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.507\n",
            "\u001b[32m[01/11 02:53:38 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.059 | 70.675 | 23.063 | 0.002 | 19.657 | 38.084 |\n",
            "\u001b[32m[01/11 02:53:38 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.059 |\n",
            "\u001b[32m[01/11 02:53:38 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:53:38 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:53:38 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:53:38 d2.evaluation.testing]: \u001b[0mcopypaste: 31.0594,70.6752,23.0634,0.0023,19.6570,38.0839\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:53:46 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:53:46 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:53:46 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:53:46 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:53:46 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:53:46 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:53:46 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:53:46 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:53:47 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0536 s/iter. Eval: 0.0003 s/iter. Total: 0.0548 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:53:52 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0016 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:53:57 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0018 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:53:59 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.595296 (0.052480 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:53:59 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049613 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:53:59 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:53:59 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:53:59 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.315\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.702\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.236\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.192\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.391\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.167\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.450\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.479\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.526\n",
            "\u001b[32m[01/11 02:53:59 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.476 | 70.170 | 23.615 | 0.002 | 19.161 | 39.124 |\n",
            "\u001b[32m[01/11 02:53:59 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.476 |\n",
            "\u001b[32m[01/11 02:53:59 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:53:59 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:53:59 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:53:59 d2.evaluation.testing]: \u001b[0mcopypaste: 31.4757,70.1705,23.6155,0.0019,19.1608,39.1240\n",
            "\u001b[32m[01/11 02:53:59 d2.utils.events]: \u001b[0m eta: 0:31:47  iter: 2519  total_loss: 0.613  loss_cls: 0.1442  loss_box_reg: 0.4256  loss_rpn_cls: 0.02053  loss_rpn_loc: 0.01157    time: 0.7721  last_time: 0.7175  data_time: 0.0289  last_data_time: 0.0288   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:54:08 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:54:08 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:54:08 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:54:08 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:54:08 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:54:08 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:54:08 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:54:08 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:54:09 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0031 s/iter. Inference: 0.0484 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:54:14 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0016 s/iter. Inference: 0.0508 s/iter. Eval: 0.0003 s/iter. Total: 0.0527 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:54:19 d2.evaluation.evaluator]: \u001b[0mInference done 197/245. Dataloading: 0.0016 s/iter. Inference: 0.0518 s/iter. Eval: 0.0003 s/iter. Total: 0.0538 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:54:21 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.940313 (0.053918 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:54:21 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.051157 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:54:21 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:54:21 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:54:21 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.323\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.709\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.245\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.209\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.397\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.169\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.455\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.481\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.533\n",
            "\u001b[32m[01/11 02:54:22 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.275 | 70.923 | 24.524 | 0.000 | 20.900 | 39.661 |\n",
            "\u001b[32m[01/11 02:54:22 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.275 |\n",
            "\u001b[32m[01/11 02:54:22 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:54:22 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:54:22 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:54:22 d2.evaluation.testing]: \u001b[0mcopypaste: 32.2755,70.9232,24.5239,0.0000,20.9001,39.6611\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:54:29 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:54:29 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:54:29 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:54:29 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:54:29 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:54:29 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:54:29 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:54:29 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:54:30 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0474 s/iter. Eval: 0.0002 s/iter. Total: 0.0487 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:54:35 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0016 s/iter. Inference: 0.0485 s/iter. Eval: 0.0003 s/iter. Total: 0.0504 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:54:40 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0016 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:54:42 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.413055 (0.051721 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:54:42 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048984 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:54:43 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:54:43 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:54:43 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.328\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.721\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.240\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.212\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.403\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.171\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.461\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.486\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.438\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.541\n",
            "\u001b[32m[01/11 02:54:43 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.778 | 72.058 | 24.043 | 0.009 | 21.154 | 40.255 |\n",
            "\u001b[32m[01/11 02:54:43 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.778 |\n",
            "\u001b[32m[01/11 02:54:43 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:54:43 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:54:43 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:54:43 d2.evaluation.testing]: \u001b[0mcopypaste: 32.7780,72.0583,24.0428,0.0090,21.1536,40.2547\n",
            "\u001b[32m[01/11 02:54:43 d2.utils.events]: \u001b[0m eta: 0:31:31  iter: 2539  total_loss: 0.6194  loss_cls: 0.1568  loss_box_reg: 0.4221  loss_rpn_cls: 0.01552  loss_rpn_loc: 0.01295    time: 0.7720  last_time: 0.7692  data_time: 0.0334  last_data_time: 0.0293   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:54:51 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:54:51 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:54:51 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:54:51 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:54:51 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:54:51 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:54:51 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:54:51 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:54:52 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0016 s/iter. Inference: 0.0523 s/iter. Eval: 0.0003 s/iter. Total: 0.0542 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:54:57 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0026 s/iter. Inference: 0.0488 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:55:02 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0023 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:55:04 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.523162 (0.052180 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:55:04 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048945 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:55:04 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:55:04 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:55:04 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.311\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.704\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.221\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.197\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.382\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.167\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.470\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.511\n",
            "\u001b[32m[01/11 02:55:05 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.138 | 70.353 | 22.078 | 0.003 | 19.667 | 38.209 |\n",
            "\u001b[32m[01/11 02:55:05 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.138 |\n",
            "\u001b[32m[01/11 02:55:05 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:55:05 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:55:05 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:55:05 d2.evaluation.testing]: \u001b[0mcopypaste: 31.1383,70.3532,22.0780,0.0029,19.6672,38.2094\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:55:12 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:55:12 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:55:12 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:55:12 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:55:12 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:55:12 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:55:12 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:55:12 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:55:13 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0015 s/iter. Inference: 0.0524 s/iter. Eval: 0.0003 s/iter. Total: 0.0541 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:55:18 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0019 s/iter. Inference: 0.0504 s/iter. Eval: 0.0003 s/iter. Total: 0.0526 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:55:24 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0016 s/iter. Inference: 0.0500 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:55:26 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:13.173206 (0.054888 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:55:26 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.052197 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:55:26 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:55:26 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:55:26 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.307\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.710\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.193\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.379\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.164\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.438\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.466\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.441\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.506\n",
            "\u001b[32m[01/11 02:55:27 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.669 | 70.969 | 21.646 | 0.011 | 19.312 | 37.930 |\n",
            "\u001b[32m[01/11 02:55:27 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.669 |\n",
            "\u001b[32m[01/11 02:55:27 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:55:27 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:55:27 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:55:27 d2.evaluation.testing]: \u001b[0mcopypaste: 30.6690,70.9692,21.6457,0.0114,19.3117,37.9296\n",
            "\u001b[32m[01/11 02:55:27 d2.utils.events]: \u001b[0m eta: 0:31:16  iter: 2559  total_loss: 0.617  loss_cls: 0.1641  loss_box_reg: 0.4232  loss_rpn_cls: 0.02249  loss_rpn_loc: 0.02179    time: 0.7721  last_time: 0.7722  data_time: 0.0329  last_data_time: 0.0317   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:55:35 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:55:35 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:55:35 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:55:35 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:55:35 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:55:35 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:55:35 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:55:35 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:55:36 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0484 s/iter. Eval: 0.0002 s/iter. Total: 0.0498 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:55:41 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0016 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:55:46 d2.evaluation.evaluator]: \u001b[0mInference done 209/245. Dataloading: 0.0016 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0509 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:55:48 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.358496 (0.051494 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:55:48 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048918 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:55:48 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:55:48 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:55:48 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.336\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.717\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.258\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.219\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.414\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.463\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.495\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.019\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.442\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.553\n",
            "\u001b[32m[01/11 02:55:49 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.598 | 71.685 | 25.819 | 0.006 | 21.874 | 41.388 |\n",
            "\u001b[32m[01/11 02:55:49 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.598 |\n",
            "\u001b[32m[01/11 02:55:49 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:55:49 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:55:49 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:55:49 d2.evaluation.testing]: \u001b[0mcopypaste: 33.5977,71.6847,25.8187,0.0056,21.8737,41.3883\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:55:56 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:55:56 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:55:56 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:55:56 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:55:56 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:55:56 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:55:56 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:55:56 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:55:57 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0046 s/iter. Inference: 0.0484 s/iter. Eval: 0.0003 s/iter. Total: 0.0532 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:56:02 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0019 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:56:07 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0018 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:56:09 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.427633 (0.051782 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:56:09 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048970 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:56:09 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:56:09 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:56:09 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.29s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.316\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.708\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.219\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.197\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.392\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.167\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.446\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.476\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.522\n",
            "\u001b[32m[01/11 02:56:10 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.646 | 70.841 | 21.903 | 0.000 | 19.668 | 39.156 |\n",
            "\u001b[32m[01/11 02:56:10 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.646 |\n",
            "\u001b[32m[01/11 02:56:10 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:56:10 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:56:10 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:56:10 d2.evaluation.testing]: \u001b[0mcopypaste: 31.6458,70.8406,21.9032,0.0000,19.6682,39.1563\n",
            "\u001b[32m[01/11 02:56:10 d2.utils.events]: \u001b[0m eta: 0:31:00  iter: 2579  total_loss: 0.6374  loss_cls: 0.1592  loss_box_reg: 0.4273  loss_rpn_cls: 0.02196  loss_rpn_loc: 0.01536    time: 0.7720  last_time: 0.7653  data_time: 0.0333  last_data_time: 0.0249   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:56:18 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:56:18 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:56:18 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:56:18 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:56:18 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:56:18 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:56:18 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:56:18 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:56:19 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0503 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:56:24 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0014 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:56:29 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0015 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:56:31 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.510561 (0.052127 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:56:31 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049453 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:56:31 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:56:31 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:56:31 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.324\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.709\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.222\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.205\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.400\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.169\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.453\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.480\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.440\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.529\n",
            "\u001b[32m[01/11 02:56:31 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.364 | 70.948 | 22.230 | 0.000 | 20.475 | 39.976 |\n",
            "\u001b[32m[01/11 02:56:31 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.364 |\n",
            "\u001b[32m[01/11 02:56:31 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:56:31 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:56:31 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:56:31 d2.evaluation.testing]: \u001b[0mcopypaste: 32.3640,70.9484,22.2295,0.0000,20.4746,39.9763\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:56:39 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:56:39 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:56:39 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:56:39 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:56:39 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:56:39 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:56:39 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:56:39 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:56:40 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0487 s/iter. Eval: 0.0002 s/iter. Total: 0.0501 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:56:45 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0017 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0509 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:56:50 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0019 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:56:52 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.410536 (0.051711 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:56:52 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048855 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:56:52 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:56:52 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:56:52 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.325\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.710\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.239\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.205\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.402\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.171\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.456\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.486\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.450\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.535\n",
            "\u001b[32m[01/11 02:56:53 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.533 | 71.028 | 23.871 | 0.000 | 20.526 | 40.168 |\n",
            "\u001b[32m[01/11 02:56:53 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.533 |\n",
            "\u001b[32m[01/11 02:56:53 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:56:53 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:56:53 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:56:53 d2.evaluation.testing]: \u001b[0mcopypaste: 32.5335,71.0285,23.8708,0.0000,20.5259,40.1683\n",
            "\u001b[32m[01/11 02:56:53 d2.utils.events]: \u001b[0m eta: 0:30:45  iter: 2599  total_loss: 0.6481  loss_cls: 0.1606  loss_box_reg: 0.4257  loss_rpn_cls: 0.02627  loss_rpn_loc: 0.02102    time: 0.7719  last_time: 0.7759  data_time: 0.0330  last_data_time: 0.0282   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:57:01 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:57:01 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:57:01 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:57:01 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:57:01 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:57:01 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:57:01 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:57:01 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:57:02 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0020 s/iter. Inference: 0.0543 s/iter. Eval: 0.0003 s/iter. Total: 0.0566 s/iter. ETA=0:00:13\n",
            "\u001b[32m[01/11 02:57:07 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0014 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:57:12 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0016 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:57:14 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.656901 (0.052737 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:57:14 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050116 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:57:14 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:57:14 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:57:14 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.29s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.316\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.703\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.225\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.198\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.392\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.169\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.448\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.475\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.442\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.521\n",
            "\u001b[32m[01/11 02:57:15 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.625 | 70.329 | 22.478 | 0.000 | 19.842 | 39.205 |\n",
            "\u001b[32m[01/11 02:57:15 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.625 |\n",
            "\u001b[32m[01/11 02:57:15 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:57:15 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:57:15 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:57:15 d2.evaluation.testing]: \u001b[0mcopypaste: 31.6255,70.3285,22.4778,0.0000,19.8418,39.2052\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:57:22 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:57:22 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:57:22 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:57:22 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:57:22 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:57:22 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:57:22 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:57:22 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:57:24 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0491 s/iter. Eval: 0.0002 s/iter. Total: 0.0504 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:57:29 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0022 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:57:34 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0023 s/iter. Inference: 0.0497 s/iter. Eval: 0.0002 s/iter. Total: 0.0522 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:57:36 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.756301 (0.053151 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:57:36 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049979 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:57:36 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:57:36 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:57:36 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.336\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.721\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.245\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.213\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.416\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.464\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.485\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.431\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.544\n",
            "\u001b[32m[01/11 02:57:36 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.596 | 72.075 | 24.509 | 0.000 | 21.306 | 41.607 |\n",
            "\u001b[32m[01/11 02:57:36 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.596 |\n",
            "\u001b[32m[01/11 02:57:36 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:57:36 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:57:36 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:57:36 d2.evaluation.testing]: \u001b[0mcopypaste: 33.5957,72.0751,24.5089,0.0000,21.3064,41.6068\n",
            "\u001b[32m[01/11 02:57:36 d2.utils.events]: \u001b[0m eta: 0:30:30  iter: 2619  total_loss: 0.6531  loss_cls: 0.1576  loss_box_reg: 0.4307  loss_rpn_cls: 0.0192  loss_rpn_loc: 0.01554    time: 0.7720  last_time: 0.6572  data_time: 0.0367  last_data_time: 0.0245   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:57:45 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:57:45 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:57:45 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:57:45 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:57:45 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:57:45 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:57:45 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:57:45 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:57:46 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0504 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:57:51 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0021 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:57:56 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0020 s/iter. Inference: 0.0500 s/iter. Eval: 0.0003 s/iter. Total: 0.0523 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:57:58 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.674920 (0.052812 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:57:58 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049743 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:57:58 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:57:58 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:57:58 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.29s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.329\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.708\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.245\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.207\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.408\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.462\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.488\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.441\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.543\n",
            "\u001b[32m[01/11 02:57:58 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.874 | 70.849 | 24.549 | 0.000 | 20.688 | 40.827 |\n",
            "\u001b[32m[01/11 02:57:58 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.874 |\n",
            "\u001b[32m[01/11 02:57:58 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:57:58 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:57:58 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:57:58 d2.evaluation.testing]: \u001b[0mcopypaste: 32.8741,70.8489,24.5486,0.0000,20.6879,40.8271\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:58:06 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:58:06 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:58:06 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:58:06 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:58:06 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:58:06 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:58:06 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:58:06 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:58:07 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0486 s/iter. Eval: 0.0002 s/iter. Total: 0.0499 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:58:12 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0019 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:58:17 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0020 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:58:19 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.587251 (0.052447 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:58:19 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049475 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:58:19 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:58:19 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:58:19 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.71s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.295\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.699\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.191\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.188\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.363\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.161\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.432\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.460\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.495\n",
            "\u001b[32m[01/11 02:58:20 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.516 | 69.929 | 19.140 | 0.006 | 18.840 | 36.260 |\n",
            "\u001b[32m[01/11 02:58:20 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.516 |\n",
            "\u001b[32m[01/11 02:58:20 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:58:20 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:58:20 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:58:20 d2.evaluation.testing]: \u001b[0mcopypaste: 29.5162,69.9287,19.1395,0.0063,18.8396,36.2605\n",
            "\u001b[32m[01/11 02:58:20 d2.utils.events]: \u001b[0m eta: 0:30:14  iter: 2639  total_loss: 0.6192  loss_cls: 0.1696  loss_box_reg: 0.4225  loss_rpn_cls: 0.01643  loss_rpn_loc: 0.01368    time: 0.7719  last_time: 0.7618  data_time: 0.0380  last_data_time: 0.0253   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:58:28 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:58:28 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:58:28 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:58:28 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:58:28 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:58:28 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:58:28 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:58:28 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:58:29 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0036 s/iter. Inference: 0.0474 s/iter. Eval: 0.0002 s/iter. Total: 0.0512 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:58:35 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0022 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:58:40 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0023 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:58:42 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.626182 (0.052609 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:58:42 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049374 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:58:42 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:58:42 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:58:42 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.701\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.232\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.194\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.393\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.167\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.451\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.474\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.430\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.526\n",
            "\u001b[32m[01/11 02:58:42 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.745 | 70.085 | 23.180 | 0.000 | 19.377 | 39.301 |\n",
            "\u001b[32m[01/11 02:58:42 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.745 |\n",
            "\u001b[32m[01/11 02:58:42 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:58:42 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:58:42 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:58:42 d2.evaluation.testing]: \u001b[0mcopypaste: 31.7455,70.0853,23.1802,0.0000,19.3772,39.3014\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:58:50 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:58:50 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:58:50 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:58:50 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:58:50 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:58:50 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:58:50 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:58:50 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:58:51 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0518 s/iter. Eval: 0.0003 s/iter. Total: 0.0530 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:58:56 d2.evaluation.evaluator]: \u001b[0mInference done 105/245. Dataloading: 0.0014 s/iter. Inference: 0.0518 s/iter. Eval: 0.0003 s/iter. Total: 0.0535 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:59:01 d2.evaluation.evaluator]: \u001b[0mInference done 202/245. Dataloading: 0.0018 s/iter. Inference: 0.0504 s/iter. Eval: 0.0003 s/iter. Total: 0.0526 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:59:03 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.702463 (0.052927 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:59:03 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050112 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:59:03 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:59:03 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:59:03 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.322\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.710\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.247\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.206\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.395\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.166\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.454\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.479\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.429\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.535\n",
            "\u001b[32m[01/11 02:59:04 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.174 | 71.032 | 24.676 | 0.000 | 20.621 | 39.482 |\n",
            "\u001b[32m[01/11 02:59:04 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.174 |\n",
            "\u001b[32m[01/11 02:59:04 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:59:04 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:59:04 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:59:04 d2.evaluation.testing]: \u001b[0mcopypaste: 32.1745,71.0318,24.6762,0.0000,20.6215,39.4822\n",
            "\u001b[32m[01/11 02:59:04 d2.utils.events]: \u001b[0m eta: 0:29:59  iter: 2659  total_loss: 0.6459  loss_cls: 0.1633  loss_box_reg: 0.4164  loss_rpn_cls: 0.02192  loss_rpn_loc: 0.0266    time: 0.7720  last_time: 0.7703  data_time: 0.0307  last_data_time: 0.0267   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:59:12 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:59:12 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:59:12 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:59:12 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:59:12 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:59:12 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:59:12 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:59:12 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:59:13 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0509 s/iter. Eval: 0.0002 s/iter. Total: 0.0523 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 02:59:18 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0016 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 02:59:23 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0017 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 02:59:25 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.414473 (0.051727 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:59:25 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048949 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:59:25 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:59:25 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:59:25 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.300\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.707\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.191\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.192\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.365\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.163\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.461\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.496\n",
            "\u001b[32m[01/11 02:59:26 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.990 | 70.737 | 19.076 | 0.003 | 19.238 | 36.526 |\n",
            "\u001b[32m[01/11 02:59:26 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.990 |\n",
            "\u001b[32m[01/11 02:59:26 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:59:26 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:59:26 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:59:26 d2.evaluation.testing]: \u001b[0mcopypaste: 29.9905,70.7373,19.0755,0.0034,19.2381,36.5259\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:59:33 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:59:33 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:59:33 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:59:33 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:59:33 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:59:33 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:59:33 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:59:33 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:59:34 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 02:59:39 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0018 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 02:59:45 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0020 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 02:59:47 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.467184 (0.051947 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:59:47 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048996 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 02:59:47 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 02:59:47 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 02:59:47 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.713\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.222\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.200\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.392\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.169\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.452\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.481\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.019\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.432\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.535\n",
            "\u001b[32m[01/11 02:59:47 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.689 | 71.319 | 22.172 | 0.005 | 19.999 | 39.227 |\n",
            "\u001b[32m[01/11 02:59:47 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.689 |\n",
            "\u001b[32m[01/11 02:59:47 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 02:59:47 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 02:59:47 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 02:59:47 d2.evaluation.testing]: \u001b[0mcopypaste: 31.6889,71.3187,22.1719,0.0054,19.9990,39.2270\n",
            "\u001b[32m[01/11 02:59:47 d2.utils.events]: \u001b[0m eta: 0:29:44  iter: 2679  total_loss: 0.6273  loss_cls: 0.1653  loss_box_reg: 0.4281  loss_rpn_cls: 0.02112  loss_rpn_loc: 0.01765    time: 0.7720  last_time: 0.7795  data_time: 0.0318  last_data_time: 0.0228   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 02:59:55 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 02:59:55 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 02:59:55 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 02:59:55 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 02:59:55 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 02:59:55 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 02:59:55 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 02:59:55 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 02:59:56 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0488 s/iter. Eval: 0.0003 s/iter. Total: 0.0502 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:00:01 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0022 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:00:06 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0022 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:00:08 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.549338 (0.052289 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:00:08 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049130 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:00:08 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:00:08 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:00:08 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.716\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.200\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.391\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.450\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.484\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.533\n",
            "\u001b[32m[01/11 03:00:09 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.700 | 71.554 | 21.645 | 0.006 | 20.035 | 39.074 |\n",
            "\u001b[32m[01/11 03:00:09 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.700 |\n",
            "\u001b[32m[01/11 03:00:09 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:00:09 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:00:09 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:00:09 d2.evaluation.testing]: \u001b[0mcopypaste: 31.6995,71.5535,21.6453,0.0063,20.0352,39.0738\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:00:16 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:00:16 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:00:16 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:00:16 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:00:16 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:00:16 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:00:16 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:00:17 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:00:17 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0015 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:00:23 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0018 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:00:28 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0018 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:00:30 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.550553 (0.052294 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:00:30 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049542 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:00:30 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:00:30 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:00:30 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.307\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.709\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.212\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.196\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.379\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.165\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.437\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.465\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.432\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.509\n",
            "\u001b[32m[01/11 03:00:30 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.719 | 70.877 | 21.216 | 0.005 | 19.619 | 37.911 |\n",
            "\u001b[32m[01/11 03:00:30 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.719 |\n",
            "\u001b[32m[01/11 03:00:30 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:00:30 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:00:30 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:00:30 d2.evaluation.testing]: \u001b[0mcopypaste: 30.7186,70.8774,21.2156,0.0046,19.6192,37.9108\n",
            "\u001b[32m[01/11 03:00:30 d2.utils.events]: \u001b[0m eta: 0:29:28  iter: 2699  total_loss: 0.6104  loss_cls: 0.1518  loss_box_reg: 0.4354  loss_rpn_cls: 0.01505  loss_rpn_loc: 0.0142    time: 0.7720  last_time: 0.6930  data_time: 0.0319  last_data_time: 0.0219   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:00:38 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:00:38 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:00:38 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:00:38 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:00:38 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:00:38 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:00:38 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:00:38 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:00:39 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:00:44 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0016 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:00:50 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0016 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:00:52 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.713365 (0.052972 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:00:52 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050322 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:00:52 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:00:52 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:00:52 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.322\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.716\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.222\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.206\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.397\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.454\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.483\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.534\n",
            "\u001b[32m[01/11 03:00:52 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.213 | 71.645 | 22.222 | 0.004 | 20.623 | 39.667 |\n",
            "\u001b[32m[01/11 03:00:52 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.213 |\n",
            "\u001b[32m[01/11 03:00:52 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:00:52 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:00:52 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:00:52 d2.evaluation.testing]: \u001b[0mcopypaste: 32.2128,71.6447,22.2218,0.0041,20.6230,39.6666\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:01:00 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:01:00 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:01:00 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:01:00 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:01:00 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:01:00 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:01:00 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:01:00 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:01:01 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:01:06 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0014 s/iter. Inference: 0.0506 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:01:11 d2.evaluation.evaluator]: \u001b[0mInference done 202/245. Dataloading: 0.0015 s/iter. Inference: 0.0507 s/iter. Eval: 0.0003 s/iter. Total: 0.0526 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:01:13 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.739756 (0.053082 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:01:13 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050412 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:01:14 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:01:14 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:01:14 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.31s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.311\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.713\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.219\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.195\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.386\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.169\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.444\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.482\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.458\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.522\n",
            "\u001b[32m[01/11 03:01:14 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.113 | 71.335 | 21.890 | 0.001 | 19.468 | 38.592 |\n",
            "\u001b[32m[01/11 03:01:14 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.113 |\n",
            "\u001b[32m[01/11 03:01:14 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:01:14 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:01:14 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:01:14 d2.evaluation.testing]: \u001b[0mcopypaste: 31.1134,71.3348,21.8901,0.0010,19.4683,38.5917\n",
            "\u001b[32m[01/11 03:01:14 d2.utils.events]: \u001b[0m eta: 0:29:13  iter: 2719  total_loss: 0.6649  loss_cls: 0.1598  loss_box_reg: 0.4141  loss_rpn_cls: 0.01712  loss_rpn_loc: 0.02085    time: 0.7721  last_time: 0.7658  data_time: 0.0335  last_data_time: 0.0234   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:01:22 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:01:22 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:01:22 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:01:22 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:01:22 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:01:22 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:01:22 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:01:22 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:01:24 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0525 s/iter. Eval: 0.0002 s/iter. Total: 0.0537 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:01:29 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0014 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:01:34 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0016 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:01:36 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:13.015109 (0.054230 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:01:36 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.051641 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:01:36 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:01:36 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:01:36 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.307\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.710\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.212\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.202\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.377\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.166\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.437\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.464\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.431\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.508\n",
            "\u001b[32m[01/11 03:01:37 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.729 | 71.014 | 21.187 | 0.000 | 20.196 | 37.665 |\n",
            "\u001b[32m[01/11 03:01:37 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.729 |\n",
            "\u001b[32m[01/11 03:01:37 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:01:37 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:01:37 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:01:37 d2.evaluation.testing]: \u001b[0mcopypaste: 30.7289,71.0142,21.1867,0.0000,20.1958,37.6653\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:01:44 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:01:44 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:01:45 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:01:45 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:01:45 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:01:45 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:01:45 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:01:45 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:01:45 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0022 s/iter. Inference: 0.0482 s/iter. Eval: 0.0003 s/iter. Total: 0.0506 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:01:51 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0015 s/iter. Inference: 0.0505 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:01:56 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0015 s/iter. Inference: 0.0500 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:01:58 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.545202 (0.052272 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:01:58 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049737 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:01:58 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:01:58 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:01:58 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.337\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.719\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.261\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.214\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.418\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.179\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.465\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.488\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.424\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.553\n",
            "\u001b[32m[01/11 03:01:58 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.742 | 71.863 | 26.141 | 0.002 | 21.416 | 41.821 |\n",
            "\u001b[32m[01/11 03:01:58 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.742 |\n",
            "\u001b[32m[01/11 03:01:58 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:01:58 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:01:58 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:01:58 d2.evaluation.testing]: \u001b[0mcopypaste: 33.7421,71.8630,26.1408,0.0017,21.4160,41.8208\n",
            "\u001b[32m[01/11 03:01:58 d2.utils.events]: \u001b[0m eta: 0:28:58  iter: 2739  total_loss: 0.6281  loss_cls: 0.1582  loss_box_reg: 0.4105  loss_rpn_cls: 0.02318  loss_rpn_loc: 0.0181    time: 0.7721  last_time: 0.8036  data_time: 0.0334  last_data_time: 0.0313   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:02:07 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:02:07 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:02:07 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:02:07 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:02:07 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:02:07 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:02:07 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:02:07 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:02:08 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0483 s/iter. Eval: 0.0002 s/iter. Total: 0.0496 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:02:13 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0018 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:02:18 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0021 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:02:20 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.658189 (0.052742 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:02:20 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049583 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:02:20 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:02:20 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:02:20 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.323\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.713\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.228\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.204\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.399\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.170\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.447\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.476\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.435\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.527\n",
            "\u001b[32m[01/11 03:02:20 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.302 | 71.335 | 22.842 | 0.001 | 20.411 | 39.863 |\n",
            "\u001b[32m[01/11 03:02:20 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.302 |\n",
            "\u001b[32m[01/11 03:02:20 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:02:20 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:02:20 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:02:20 d2.evaluation.testing]: \u001b[0mcopypaste: 32.3016,71.3352,22.8418,0.0014,20.4113,39.8635\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:02:28 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:02:28 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:02:28 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:02:28 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:02:28 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:02:28 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:02:28 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:02:28 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:02:29 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0501 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:02:34 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0014 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:02:39 d2.evaluation.evaluator]: \u001b[0mInference done 209/245. Dataloading: 0.0015 s/iter. Inference: 0.0490 s/iter. Eval: 0.0002 s/iter. Total: 0.0508 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:02:41 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.339600 (0.051415 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:02:41 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048852 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:02:41 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:02:41 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:02:41 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.306\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.707\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.193\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.375\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.170\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.460\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.439\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.497\n",
            "\u001b[32m[01/11 03:02:41 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.648 | 70.683 | 21.575 | 0.005 | 19.348 | 37.487 |\n",
            "\u001b[32m[01/11 03:02:41 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.648 |\n",
            "\u001b[32m[01/11 03:02:41 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:02:41 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:02:41 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:02:41 d2.evaluation.testing]: \u001b[0mcopypaste: 30.6479,70.6829,21.5745,0.0051,19.3483,37.4866\n",
            "\u001b[32m[01/11 03:02:41 d2.utils.events]: \u001b[0m eta: 0:28:43  iter: 2759  total_loss: 0.6077  loss_cls: 0.153  loss_box_reg: 0.3991  loss_rpn_cls: 0.02053  loss_rpn_loc: 0.02143    time: 0.7722  last_time: 0.7743  data_time: 0.0330  last_data_time: 0.0218   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:02:50 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:02:50 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:02:50 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:02:50 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:02:50 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:02:50 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:02:50 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:02:50 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:02:51 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0033 s/iter. Inference: 0.0481 s/iter. Eval: 0.0002 s/iter. Total: 0.0517 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:02:56 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0020 s/iter. Inference: 0.0489 s/iter. Eval: 0.0002 s/iter. Total: 0.0512 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:03:01 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0020 s/iter. Inference: 0.0488 s/iter. Eval: 0.0002 s/iter. Total: 0.0512 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:03:03 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.446609 (0.051861 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:03:03 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048891 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:03:03 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:03:03 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:03:03 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.323\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.712\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.233\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.202\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.397\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.170\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.451\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.472\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.423\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.526\n",
            "\u001b[32m[01/11 03:03:03 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.316 | 71.151 | 23.329 | 0.002 | 20.207 | 39.747 |\n",
            "\u001b[32m[01/11 03:03:03 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.316 |\n",
            "\u001b[32m[01/11 03:03:03 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:03:03 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:03:03 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:03:03 d2.evaluation.testing]: \u001b[0mcopypaste: 32.3161,71.1507,23.3288,0.0017,20.2066,39.7474\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:03:11 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:03:11 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:03:11 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:03:11 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:03:11 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:03:11 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:03:11 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:03:11 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:03:12 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0499 s/iter. Eval: 0.0002 s/iter. Total: 0.0511 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:03:17 d2.evaluation.evaluator]: \u001b[0mInference done 112/245. Dataloading: 0.0018 s/iter. Inference: 0.0478 s/iter. Eval: 0.0003 s/iter. Total: 0.0500 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:03:22 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0017 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:03:24 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.507617 (0.052115 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:03:24 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049352 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:03:24 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:03:24 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:03:24 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.286\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.699\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.190\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.182\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.351\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.157\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.412\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.442\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.424\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.476\n",
            "\u001b[32m[01/11 03:03:25 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 28.574 | 69.918 | 18.967 | 0.007 | 18.223 | 35.089 |\n",
            "\u001b[32m[01/11 03:03:25 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 28.574 |\n",
            "\u001b[32m[01/11 03:03:25 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:03:25 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:03:25 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:03:25 d2.evaluation.testing]: \u001b[0mcopypaste: 28.5739,69.9185,18.9668,0.0068,18.2226,35.0886\n",
            "\u001b[32m[01/11 03:03:25 d2.utils.events]: \u001b[0m eta: 0:28:27  iter: 2779  total_loss: 0.6398  loss_cls: 0.1568  loss_box_reg: 0.4287  loss_rpn_cls: 0.02246  loss_rpn_loc: 0.01582    time: 0.7722  last_time: 0.7648  data_time: 0.0315  last_data_time: 0.0210   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:03:33 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:03:33 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:03:33 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:03:33 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:03:33 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:03:33 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:03:33 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:03:33 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:03:34 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0506 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:03:39 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0015 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:03:44 d2.evaluation.evaluator]: \u001b[0mInference done 209/245. Dataloading: 0.0016 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0509 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:03:46 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.362763 (0.051512 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:03:46 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048883 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:03:46 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:03:46 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:03:46 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.329\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.710\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.245\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.204\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.408\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.454\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.487\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.440\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.541\n",
            "\u001b[32m[01/11 03:03:47 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.881 | 70.956 | 24.495 | 0.001 | 20.370 | 40.783 |\n",
            "\u001b[32m[01/11 03:03:47 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.881 |\n",
            "\u001b[32m[01/11 03:03:47 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:03:47 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:03:47 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:03:47 d2.evaluation.testing]: \u001b[0mcopypaste: 32.8813,70.9558,24.4951,0.0010,20.3699,40.7828\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:03:54 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:03:54 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:03:54 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:03:54 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:03:54 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:03:54 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:03:54 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:03:54 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:03:55 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:04:00 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0016 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:04:05 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0018 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:04:07 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.555495 (0.052315 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:04:07 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049349 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:04:07 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:04:07 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:04:07 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.338\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.718\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.251\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.216\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.417\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.174\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.462\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.490\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.438\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.549\n",
            "\u001b[32m[01/11 03:04:08 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.829 | 71.775 | 25.099 | 0.000 | 21.628 | 41.697 |\n",
            "\u001b[32m[01/11 03:04:08 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.829 |\n",
            "\u001b[32m[01/11 03:04:08 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:04:08 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:04:08 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:04:08 d2.evaluation.testing]: \u001b[0mcopypaste: 33.8287,71.7750,25.0991,0.0000,21.6284,41.6973\n",
            "\u001b[32m[01/11 03:04:08 d2.utils.events]: \u001b[0m eta: 0:28:12  iter: 2799  total_loss: 0.6069  loss_cls: 0.1466  loss_box_reg: 0.4152  loss_rpn_cls: 0.01662  loss_rpn_loc: 0.01961    time: 0.7722  last_time: 0.7144  data_time: 0.0310  last_data_time: 0.0253   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:04:16 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:04:16 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:04:16 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:04:16 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:04:16 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:04:16 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:04:16 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:04:16 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:04:17 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0008 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0505 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:04:22 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0017 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:04:27 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0019 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:04:29 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.456376 (0.051902 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:04:29 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049066 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:04:29 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:04:29 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:04:29 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.31s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.322\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.717\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.224\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.209\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.396\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.167\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.448\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.483\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.448\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.530\n",
            "\u001b[32m[01/11 03:04:29 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.159 | 71.656 | 22.414 | 0.000 | 20.877 | 39.583 |\n",
            "\u001b[32m[01/11 03:04:29 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.159 |\n",
            "\u001b[32m[01/11 03:04:29 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:04:29 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:04:29 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:04:29 d2.evaluation.testing]: \u001b[0mcopypaste: 32.1586,71.6562,22.4143,0.0000,20.8770,39.5834\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:04:37 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:04:37 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:04:37 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:04:37 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:04:37 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:04:37 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:04:37 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:04:37 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:04:38 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0542 s/iter. Eval: 0.0002 s/iter. Total: 0.0555 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:04:43 d2.evaluation.evaluator]: \u001b[0mInference done 104/245. Dataloading: 0.0016 s/iter. Inference: 0.0523 s/iter. Eval: 0.0003 s/iter. Total: 0.0542 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:04:48 d2.evaluation.evaluator]: \u001b[0mInference done 193/245. Dataloading: 0.0016 s/iter. Inference: 0.0534 s/iter. Eval: 0.0003 s/iter. Total: 0.0553 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:04:51 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:13.245672 (0.055190 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:04:51 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.052631 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:04:51 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:04:51 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:04:51 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.29s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.337\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.724\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.247\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.212\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.418\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.174\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.468\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.494\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.423\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.564\n",
            "\u001b[32m[01/11 03:04:52 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.722 | 72.372 | 24.698 | 0.001 | 21.171 | 41.778 |\n",
            "\u001b[32m[01/11 03:04:52 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.722 |\n",
            "\u001b[32m[01/11 03:04:52 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:04:52 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:04:52 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:04:52 d2.evaluation.testing]: \u001b[0mcopypaste: 33.7224,72.3716,24.6981,0.0012,21.1707,41.7778\n",
            "\u001b[32m[01/11 03:04:52 d2.utils.events]: \u001b[0m eta: 0:27:56  iter: 2819  total_loss: 0.6436  loss_cls: 0.1574  loss_box_reg: 0.4306  loss_rpn_cls: 0.02341  loss_rpn_loc: 0.01568    time: 0.7721  last_time: 0.7764  data_time: 0.0331  last_data_time: 0.0300   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:05:00 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:05:00 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:05:00 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:05:00 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:05:00 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:05:00 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:05:00 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:05:00 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:05:01 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0477 s/iter. Eval: 0.0002 s/iter. Total: 0.0490 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:05:06 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0017 s/iter. Inference: 0.0498 s/iter. Eval: 0.0002 s/iter. Total: 0.0519 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:05:11 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0017 s/iter. Inference: 0.0494 s/iter. Eval: 0.0002 s/iter. Total: 0.0514 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:05:13 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.476357 (0.051985 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:05:13 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049338 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:05:13 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:05:13 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:05:13 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.299\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.708\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.210\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.192\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.365\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.159\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.430\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.459\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.441\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.495\n",
            "\u001b[32m[01/11 03:05:13 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.923 | 70.840 | 20.983 | 0.000 | 19.162 | 36.507 |\n",
            "\u001b[32m[01/11 03:05:13 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.923 |\n",
            "\u001b[32m[01/11 03:05:13 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:05:13 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:05:13 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:05:13 d2.evaluation.testing]: \u001b[0mcopypaste: 29.9228,70.8400,20.9829,0.0000,19.1618,36.5069\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:05:21 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:05:21 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:05:21 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:05:21 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:05:21 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:05:21 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:05:21 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:05:21 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:05:22 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0506 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:05:27 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0019 s/iter. Inference: 0.0488 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:05:32 d2.evaluation.evaluator]: \u001b[0mInference done 209/245. Dataloading: 0.0017 s/iter. Inference: 0.0487 s/iter. Eval: 0.0003 s/iter. Total: 0.0508 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:05:34 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.341731 (0.051424 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:05:34 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048751 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:05:34 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:05:34 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:05:34 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.312\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.711\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.211\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.202\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.382\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.164\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.441\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.468\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.435\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.513\n",
            "\u001b[32m[01/11 03:05:34 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.214 | 71.121 | 21.146 | 0.000 | 20.194 | 38.189 |\n",
            "\u001b[32m[01/11 03:05:34 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.214 |\n",
            "\u001b[32m[01/11 03:05:34 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:05:34 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:05:34 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:05:34 d2.evaluation.testing]: \u001b[0mcopypaste: 31.2144,71.1213,21.1461,0.0000,20.1944,38.1891\n",
            "\u001b[32m[01/11 03:05:34 d2.utils.events]: \u001b[0m eta: 0:27:41  iter: 2839  total_loss: 0.5572  loss_cls: 0.1532  loss_box_reg: 0.3798  loss_rpn_cls: 0.01629  loss_rpn_loc: 0.01076    time: 0.7721  last_time: 0.7802  data_time: 0.0323  last_data_time: 0.0337   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:05:43 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:05:43 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:05:43 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:05:43 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:05:43 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:05:43 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:05:43 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:05:43 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:05:44 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0512 s/iter. Eval: 0.0002 s/iter. Total: 0.0527 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:05:49 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0019 s/iter. Inference: 0.0500 s/iter. Eval: 0.0003 s/iter. Total: 0.0523 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:05:54 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0017 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:05:56 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.447501 (0.051865 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:05:56 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049187 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:05:56 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:05:56 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:05:56 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.316\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.714\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.208\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.384\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.169\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.450\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.473\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.445\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.516\n",
            "\u001b[32m[01/11 03:05:56 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.602 | 71.399 | 21.641 | 0.000 | 20.844 | 38.450 |\n",
            "\u001b[32m[01/11 03:05:56 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.602 |\n",
            "\u001b[32m[01/11 03:05:56 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:05:56 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:05:56 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:05:56 d2.evaluation.testing]: \u001b[0mcopypaste: 31.6016,71.3991,21.6412,0.0000,20.8435,38.4497\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:06:04 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:06:04 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:06:04 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:06:04 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:06:04 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:06:04 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:06:04 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:06:04 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:06:05 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0025 s/iter. Inference: 0.0481 s/iter. Eval: 0.0002 s/iter. Total: 0.0508 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:06:10 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0021 s/iter. Inference: 0.0486 s/iter. Eval: 0.0002 s/iter. Total: 0.0511 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:06:15 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0018 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:06:17 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.696127 (0.052901 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:06:17 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050111 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:06:17 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:06:17 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:06:17 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.309\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.715\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.200\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.202\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.376\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.169\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.440\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.464\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.440\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.504\n",
            "\u001b[32m[01/11 03:06:18 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.946 | 71.546 | 20.047 | 0.002 | 20.244 | 37.596 |\n",
            "\u001b[32m[01/11 03:06:18 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.946 |\n",
            "\u001b[32m[01/11 03:06:18 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:06:18 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:06:18 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:06:18 d2.evaluation.testing]: \u001b[0mcopypaste: 30.9456,71.5462,20.0472,0.0015,20.2436,37.5960\n",
            "\u001b[32m[01/11 03:06:18 d2.utils.events]: \u001b[0m eta: 0:27:25  iter: 2859  total_loss: 0.6579  loss_cls: 0.1609  loss_box_reg: 0.4294  loss_rpn_cls: 0.02067  loss_rpn_loc: 0.01866    time: 0.7721  last_time: 0.7075  data_time: 0.0380  last_data_time: 0.0298   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:06:26 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:06:26 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:06:26 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:06:26 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:06:26 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:06:26 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:06:26 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:06:26 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:06:27 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0470 s/iter. Eval: 0.0002 s/iter. Total: 0.0483 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:06:32 d2.evaluation.evaluator]: \u001b[0mInference done 111/245. Dataloading: 0.0018 s/iter. Inference: 0.0480 s/iter. Eval: 0.0003 s/iter. Total: 0.0501 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:06:37 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0019 s/iter. Inference: 0.0486 s/iter. Eval: 0.0003 s/iter. Total: 0.0508 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:06:39 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.367641 (0.051532 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:06:39 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048586 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:06:39 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:06:39 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:06:39 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.301\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.711\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.207\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.196\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.366\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.165\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.434\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.463\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.446\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.497\n",
            "\u001b[32m[01/11 03:06:39 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.144 | 71.146 | 20.680 | 0.000 | 19.625 | 36.605 |\n",
            "\u001b[32m[01/11 03:06:39 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.144 |\n",
            "\u001b[32m[01/11 03:06:39 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:06:39 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:06:39 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:06:39 d2.evaluation.testing]: \u001b[0mcopypaste: 30.1437,71.1463,20.6804,0.0000,19.6245,36.6045\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:06:47 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:06:47 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:06:47 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:06:47 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:06:47 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:06:47 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:06:47 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:06:47 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:06:48 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0018 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:06:53 d2.evaluation.evaluator]: \u001b[0mInference done 112/245. Dataloading: 0.0014 s/iter. Inference: 0.0482 s/iter. Eval: 0.0003 s/iter. Total: 0.0499 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:06:58 d2.evaluation.evaluator]: \u001b[0mInference done 209/245. Dataloading: 0.0014 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0508 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:07:00 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.294803 (0.051228 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:07:00 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048922 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:07:00 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:07:00 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:07:00 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.318\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.705\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.230\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.201\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.391\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.170\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.455\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.476\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.432\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.529\n",
            "\u001b[32m[01/11 03:07:01 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.756 | 70.538 | 22.958 | 0.000 | 20.096 | 39.068 |\n",
            "\u001b[32m[01/11 03:07:01 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.756 |\n",
            "\u001b[32m[01/11 03:07:01 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:07:01 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:07:01 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:07:01 d2.evaluation.testing]: \u001b[0mcopypaste: 31.7564,70.5383,22.9584,0.0000,20.0959,39.0684\n",
            "\u001b[32m[01/11 03:07:01 d2.utils.events]: \u001b[0m eta: 0:27:10  iter: 2879  total_loss: 0.6462  loss_cls: 0.153  loss_box_reg: 0.4199  loss_rpn_cls: 0.0224  loss_rpn_loc: 0.01561    time: 0.7721  last_time: 0.7653  data_time: 0.0321  last_data_time: 0.0139   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:07:09 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:07:09 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:07:09 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:07:09 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:07:09 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:07:09 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:07:09 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:07:09 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:07:10 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0482 s/iter. Eval: 0.0002 s/iter. Total: 0.0493 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:07:15 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0015 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0508 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:07:20 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0016 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:07:22 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.469693 (0.051957 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:07:22 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049359 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:07:22 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:07:22 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:07:22 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.322\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.715\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.229\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.203\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.396\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.170\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.455\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.480\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.444\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.527\n",
            "\u001b[32m[01/11 03:07:22 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.231 | 71.530 | 22.897 | 0.000 | 20.268 | 39.581 |\n",
            "\u001b[32m[01/11 03:07:22 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.231 |\n",
            "\u001b[32m[01/11 03:07:22 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:07:22 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:07:22 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:07:22 d2.evaluation.testing]: \u001b[0mcopypaste: 32.2309,71.5297,22.8971,0.0000,20.2678,39.5807\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:07:30 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:07:30 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:07:30 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:07:30 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:07:30 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:07:30 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:07:30 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:07:30 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:07:31 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0044 s/iter. Inference: 0.0521 s/iter. Eval: 0.0002 s/iter. Total: 0.0567 s/iter. ETA=0:00:13\n",
            "\u001b[32m[01/11 03:07:36 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0018 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0523 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:07:42 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0016 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:07:44 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.602871 (0.052512 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:07:44 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049870 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:07:44 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:07:44 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:07:44 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.43s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.305\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.711\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.198\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.195\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.373\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.165\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.438\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.465\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.444\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.503\n",
            "\u001b[32m[01/11 03:07:44 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.467 | 71.134 | 19.776 | 0.001 | 19.498 | 37.331 |\n",
            "\u001b[32m[01/11 03:07:44 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.467 |\n",
            "\u001b[32m[01/11 03:07:44 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:07:44 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:07:44 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:07:44 d2.evaluation.testing]: \u001b[0mcopypaste: 30.4673,71.1337,19.7758,0.0011,19.4977,37.3313\n",
            "\u001b[32m[01/11 03:07:45 d2.utils.events]: \u001b[0m eta: 0:26:55  iter: 2899  total_loss: 0.5883  loss_cls: 0.1473  loss_box_reg: 0.3996  loss_rpn_cls: 0.02135  loss_rpn_loc: 0.01661    time: 0.7722  last_time: 0.7734  data_time: 0.0332  last_data_time: 0.0314   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:07:53 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:07:53 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:07:53 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:07:53 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:07:53 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:07:53 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:07:53 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:07:53 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:07:54 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0486 s/iter. Eval: 0.0003 s/iter. Total: 0.0499 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:07:59 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0017 s/iter. Inference: 0.0504 s/iter. Eval: 0.0003 s/iter. Total: 0.0525 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:08:04 d2.evaluation.evaluator]: \u001b[0mInference done 200/245. Dataloading: 0.0020 s/iter. Inference: 0.0509 s/iter. Eval: 0.0003 s/iter. Total: 0.0531 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:08:06 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.836990 (0.053487 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:08:06 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050591 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:08:06 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:08:06 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:08:06 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.320\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.711\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.230\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.205\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.394\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.171\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.447\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.473\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.432\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.524\n",
            "\u001b[32m[01/11 03:08:07 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.987 | 71.075 | 22.970 | 0.000 | 20.489 | 39.376 |\n",
            "\u001b[32m[01/11 03:08:07 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.987 |\n",
            "\u001b[32m[01/11 03:08:07 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:08:07 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:08:07 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:08:07 d2.evaluation.testing]: \u001b[0mcopypaste: 31.9874,71.0748,22.9703,0.0000,20.4894,39.3757\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:08:14 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:08:14 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:08:14 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:08:14 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:08:14 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:08:14 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:08:14 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:08:14 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:08:15 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0015 s/iter. Inference: 0.0506 s/iter. Eval: 0.0003 s/iter. Total: 0.0523 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:08:20 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0020 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:08:25 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0018 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:08:27 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.430143 (0.051792 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:08:27 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049065 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:08:27 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:08:27 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:08:27 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.322\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.710\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.236\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.201\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.397\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.171\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.450\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.483\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.439\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.535\n",
            "\u001b[32m[01/11 03:08:28 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.174 | 70.952 | 23.587 | 0.001 | 20.091 | 39.704 |\n",
            "\u001b[32m[01/11 03:08:28 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.174 |\n",
            "\u001b[32m[01/11 03:08:28 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:08:28 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:08:28 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:08:28 d2.evaluation.testing]: \u001b[0mcopypaste: 32.1736,70.9521,23.5872,0.0012,20.0906,39.7039\n",
            "\u001b[32m[01/11 03:08:28 d2.utils.events]: \u001b[0m eta: 0:26:39  iter: 2919  total_loss: 0.6236  loss_cls: 0.1635  loss_box_reg: 0.4051  loss_rpn_cls: 0.02202  loss_rpn_loc: 0.02343    time: 0.7721  last_time: 0.7671  data_time: 0.0327  last_data_time: 0.0249   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:08:36 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:08:36 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:08:36 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:08:36 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:08:36 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:08:36 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:08:36 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:08:36 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:08:37 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0516 s/iter. Eval: 0.0003 s/iter. Total: 0.0531 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:08:42 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0018 s/iter. Inference: 0.0509 s/iter. Eval: 0.0003 s/iter. Total: 0.0530 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:08:47 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0017 s/iter. Inference: 0.0505 s/iter. Eval: 0.0003 s/iter. Total: 0.0526 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:08:49 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.752248 (0.053134 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:08:49 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050495 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:08:50 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:08:50 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:08:50 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.311\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.708\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.218\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.196\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.382\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.166\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.441\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.467\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.437\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.511\n",
            "\u001b[32m[01/11 03:08:50 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.113 | 70.798 | 21.762 | 0.000 | 19.630 | 38.223 |\n",
            "\u001b[32m[01/11 03:08:50 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.113 |\n",
            "\u001b[32m[01/11 03:08:50 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:08:50 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:08:50 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:08:50 d2.evaluation.testing]: \u001b[0mcopypaste: 31.1128,70.7984,21.7621,0.0000,19.6297,38.2233\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:08:58 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:08:58 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:08:58 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:08:58 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:08:58 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:08:58 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:08:58 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:08:58 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:08:59 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0041 s/iter. Inference: 0.0494 s/iter. Eval: 0.0002 s/iter. Total: 0.0538 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:09:04 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0024 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:09:09 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0019 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:09:11 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.605769 (0.052524 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:09:11 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049602 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:09:11 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:09:11 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:09:11 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.328\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.719\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.243\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.206\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.404\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.456\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.480\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.432\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.535\n",
            "\u001b[32m[01/11 03:09:11 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.786 | 71.886 | 24.335 | 0.000 | 20.563 | 40.392 |\n",
            "\u001b[32m[01/11 03:09:11 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.786 |\n",
            "\u001b[32m[01/11 03:09:11 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:09:11 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:09:11 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:09:11 d2.evaluation.testing]: \u001b[0mcopypaste: 32.7862,71.8856,24.3355,0.0000,20.5632,40.3915\n",
            "\u001b[32m[01/11 03:09:11 d2.utils.events]: \u001b[0m eta: 0:26:24  iter: 2939  total_loss: 0.6289  loss_cls: 0.1624  loss_box_reg: 0.4226  loss_rpn_cls: 0.01904  loss_rpn_loc: 0.01599    time: 0.7721  last_time: 0.7624  data_time: 0.0340  last_data_time: 0.0222   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:09:20 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:09:20 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:09:20 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:09:20 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:09:20 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:09:20 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:09:20 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:09:20 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:09:21 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0504 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:09:26 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0015 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:09:31 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0016 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:09:33 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.434991 (0.051812 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:09:33 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049117 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:09:33 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:09:33 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:09:33 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.326\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.713\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.253\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.208\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.403\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.455\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.483\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.434\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.540\n",
            "\u001b[32m[01/11 03:09:33 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.642 | 71.350 | 25.287 | 0.000 | 20.805 | 40.261 |\n",
            "\u001b[32m[01/11 03:09:33 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.642 |\n",
            "\u001b[32m[01/11 03:09:33 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:09:33 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:09:33 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:09:33 d2.evaluation.testing]: \u001b[0mcopypaste: 32.6419,71.3495,25.2868,0.0000,20.8047,40.2607\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:09:41 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:09:41 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:09:41 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:09:41 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:09:41 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:09:41 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:09:41 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:09:41 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:09:42 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0038 s/iter. Inference: 0.0484 s/iter. Eval: 0.0002 s/iter. Total: 0.0525 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:09:47 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0017 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:09:52 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0016 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0509 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:09:54 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.382365 (0.051593 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:09:54 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048884 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:09:54 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:09:54 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:09:54 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.329\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.718\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.248\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.211\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.405\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.451\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.486\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.450\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.532\n",
            "\u001b[32m[01/11 03:09:54 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.899 | 71.771 | 24.797 | 0.002 | 21.086 | 40.548 |\n",
            "\u001b[32m[01/11 03:09:54 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.899 |\n",
            "\u001b[32m[01/11 03:09:54 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:09:54 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:09:54 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:09:54 d2.evaluation.testing]: \u001b[0mcopypaste: 32.8993,71.7714,24.7967,0.0019,21.0865,40.5482\n",
            "\u001b[32m[01/11 03:09:54 d2.utils.events]: \u001b[0m eta: 0:26:08  iter: 2959  total_loss: 0.6459  loss_cls: 0.1666  loss_box_reg: 0.4167  loss_rpn_cls: 0.01953  loss_rpn_loc: 0.01942    time: 0.7721  last_time: 0.6890  data_time: 0.0334  last_data_time: 0.0218   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:10:03 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:10:03 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:10:03 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:10:03 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:10:03 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:10:03 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:10:03 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:10:03 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:10:04 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0022 s/iter. Inference: 0.0487 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:10:09 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0015 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:10:14 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0015 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:10:16 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.329399 (0.051372 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:10:16 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048983 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:10:16 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:10:16 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:10:16 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.337\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.725\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.250\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.209\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.417\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.460\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.491\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.444\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.546\n",
            "\u001b[32m[01/11 03:10:16 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.653 | 72.541 | 24.982 | 0.005 | 20.919 | 41.653 |\n",
            "\u001b[32m[01/11 03:10:16 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.653 |\n",
            "\u001b[32m[01/11 03:10:16 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:10:16 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:10:16 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:10:16 d2.evaluation.testing]: \u001b[0mcopypaste: 33.6526,72.5412,24.9821,0.0047,20.9188,41.6532\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:10:24 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:10:24 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:10:24 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:10:24 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:10:24 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:10:24 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:10:24 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:10:24 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:10:25 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0503 s/iter. Eval: 0.0002 s/iter. Total: 0.0516 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:10:30 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0015 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:10:35 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0019 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:10:37 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.629425 (0.052623 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:10:37 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049722 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:10:37 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:10:37 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:10:37 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.313\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.711\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.207\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.204\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.382\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.439\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.468\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.437\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.512\n",
            "\u001b[32m[01/11 03:10:38 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.281 | 71.126 | 20.750 | 0.001 | 20.389 | 38.230 |\n",
            "\u001b[32m[01/11 03:10:38 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.281 |\n",
            "\u001b[32m[01/11 03:10:38 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:10:38 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:10:38 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:10:38 d2.evaluation.testing]: \u001b[0mcopypaste: 31.2811,71.1258,20.7495,0.0009,20.3894,38.2296\n",
            "\u001b[32m[01/11 03:10:38 d2.utils.events]: \u001b[0m eta: 0:25:53  iter: 2979  total_loss: 0.5922  loss_cls: 0.1581  loss_box_reg: 0.4014  loss_rpn_cls: 0.01449  loss_rpn_loc: 0.01527    time: 0.7721  last_time: 0.7115  data_time: 0.0312  last_data_time: 0.0355   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:10:46 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:10:46 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:10:46 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:10:46 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:10:46 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:10:46 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:10:46 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:10:46 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:10:47 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:10:52 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0021 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:10:57 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0020 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:11:00 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.935882 (0.053900 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:11:00 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.051039 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:11:00 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:11:00 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:11:00 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.323\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.719\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.230\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.210\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.395\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.448\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.473\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.432\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.522\n",
            "\u001b[32m[01/11 03:11:00 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.311 | 71.907 | 22.967 | 0.011 | 20.981 | 39.548 |\n",
            "\u001b[32m[01/11 03:11:00 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.311 |\n",
            "\u001b[32m[01/11 03:11:00 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:11:00 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:11:00 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:11:00 d2.evaluation.testing]: \u001b[0mcopypaste: 32.3111,71.9067,22.9666,0.0111,20.9811,39.5478\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:11:07 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:11:07 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:11:07 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:11:07 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:11:07 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:11:07 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:11:07 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:11:07 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:11:08 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0024 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:11:13 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0016 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0509 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:11:18 d2.evaluation.evaluator]: \u001b[0mInference done 209/245. Dataloading: 0.0016 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0508 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:11:20 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.336428 (0.051402 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:11:20 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048757 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:11:20 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:11:20 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:11:20 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.715\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.206\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.390\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.166\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.469\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.434\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.515\n",
            "\u001b[32m[01/11 03:11:21 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.749 | 71.461 | 21.671 | 0.015 | 20.589 | 38.971 |\n",
            "\u001b[32m[01/11 03:11:21 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.749 |\n",
            "\u001b[32m[01/11 03:11:21 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:11:21 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:11:21 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:11:21 d2.evaluation.testing]: \u001b[0mcopypaste: 31.7494,71.4614,21.6715,0.0148,20.5891,38.9705\n",
            "\u001b[32m[01/11 03:11:21 d2.utils.events]: \u001b[0m eta: 0:25:38  iter: 2999  total_loss: 0.6099  loss_cls: 0.1472  loss_box_reg: 0.4233  loss_rpn_cls: 0.01874  loss_rpn_loc: 0.01746    time: 0.7721  last_time: 0.7648  data_time: 0.0339  last_data_time: 0.0232   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:11:29 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:11:29 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:11:29 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:11:29 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:11:29 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:11:29 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:11:29 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:11:29 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:11:30 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0033 s/iter. Inference: 0.0502 s/iter. Eval: 0.0003 s/iter. Total: 0.0537 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:11:35 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0014 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:11:40 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0018 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:11:42 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.478413 (0.051993 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:11:42 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049151 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:11:42 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:11:42 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:11:42 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.315\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.704\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.229\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.202\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.390\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.170\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.472\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.435\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.519\n",
            "\u001b[32m[01/11 03:11:43 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.530 | 70.369 | 22.872 | 0.012 | 20.228 | 38.952 |\n",
            "\u001b[32m[01/11 03:11:43 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.530 |\n",
            "\u001b[32m[01/11 03:11:43 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:11:43 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:11:43 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:11:43 d2.evaluation.testing]: \u001b[0mcopypaste: 31.5297,70.3692,22.8719,0.0116,20.2280,38.9523\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:11:50 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:11:50 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:11:50 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:11:50 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:11:50 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:11:50 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:11:50 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:11:50 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:11:51 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0483 s/iter. Eval: 0.0003 s/iter. Total: 0.0495 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:11:56 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0024 s/iter. Inference: 0.0488 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:12:01 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0019 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:12:04 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.550459 (0.052294 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:12:04 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049375 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:12:04 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:12:04 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:12:04 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.333\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.718\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.249\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.217\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.410\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.177\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.461\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.490\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.445\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.544\n",
            "\u001b[32m[01/11 03:12:04 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.316 | 71.824 | 24.900 | 0.004 | 21.703 | 40.960 |\n",
            "\u001b[32m[01/11 03:12:04 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.316 |\n",
            "\u001b[32m[01/11 03:12:04 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:12:04 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:12:04 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:12:04 d2.evaluation.testing]: \u001b[0mcopypaste: 33.3162,71.8239,24.9000,0.0044,21.7034,40.9595\n",
            "\u001b[32m[01/11 03:12:04 d2.utils.events]: \u001b[0m eta: 0:25:22  iter: 3019  total_loss: 0.6256  loss_cls: 0.1527  loss_box_reg: 0.4132  loss_rpn_cls: 0.02703  loss_rpn_loc: 0.02259    time: 0.7721  last_time: 0.7031  data_time: 0.0362  last_data_time: 0.0243   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:12:12 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:12:12 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:12:12 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:12:12 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:12:12 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:12:12 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:12:12 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:12:12 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:12:13 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0509 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:12:18 d2.evaluation.evaluator]: \u001b[0mInference done 104/245. Dataloading: 0.0015 s/iter. Inference: 0.0519 s/iter. Eval: 0.0003 s/iter. Total: 0.0538 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:12:23 d2.evaluation.evaluator]: \u001b[0mInference done 201/245. Dataloading: 0.0016 s/iter. Inference: 0.0508 s/iter. Eval: 0.0003 s/iter. Total: 0.0527 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:12:26 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.772571 (0.053219 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:12:26 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050565 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:12:26 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:12:26 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:12:26 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.326\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.713\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.238\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.212\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.402\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.457\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.490\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.448\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.540\n",
            "\u001b[32m[01/11 03:12:26 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.632 | 71.306 | 23.805 | 0.006 | 21.209 | 40.154 |\n",
            "\u001b[32m[01/11 03:12:26 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.632 |\n",
            "\u001b[32m[01/11 03:12:26 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:12:26 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:12:26 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:12:26 d2.evaluation.testing]: \u001b[0mcopypaste: 32.6323,71.3063,23.8046,0.0060,21.2085,40.1545\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:12:34 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:12:34 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:12:34 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:12:34 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:12:34 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:12:34 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:12:34 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:12:34 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:12:35 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0015 s/iter. Inference: 0.0530 s/iter. Eval: 0.0003 s/iter. Total: 0.0548 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:12:40 d2.evaluation.evaluator]: \u001b[0mInference done 103/245. Dataloading: 0.0016 s/iter. Inference: 0.0525 s/iter. Eval: 0.0003 s/iter. Total: 0.0545 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:12:45 d2.evaluation.evaluator]: \u001b[0mInference done 199/245. Dataloading: 0.0018 s/iter. Inference: 0.0512 s/iter. Eval: 0.0003 s/iter. Total: 0.0533 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:12:48 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.880912 (0.053670 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:12:48 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050943 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:12:48 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:12:48 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:12:48 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.313\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.718\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.208\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.381\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.167\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.445\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.472\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.447\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.513\n",
            "\u001b[32m[01/11 03:12:48 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.263 | 71.790 | 21.547 | 0.002 | 20.826 | 38.054 |\n",
            "\u001b[32m[01/11 03:12:48 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.263 |\n",
            "\u001b[32m[01/11 03:12:48 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:12:48 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:12:48 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:12:48 d2.evaluation.testing]: \u001b[0mcopypaste: 31.2627,71.7901,21.5472,0.0015,20.8263,38.0545\n",
            "\u001b[32m[01/11 03:12:48 d2.utils.events]: \u001b[0m eta: 0:25:07  iter: 3039  total_loss: 0.6442  loss_cls: 0.1567  loss_box_reg: 0.4145  loss_rpn_cls: 0.02426  loss_rpn_loc: 0.02389    time: 0.7721  last_time: 0.7801  data_time: 0.0319  last_data_time: 0.0293   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:12:56 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:12:56 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:12:57 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:12:57 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:12:57 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:12:57 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:12:57 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:12:57 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:12:58 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0520 s/iter. Eval: 0.0003 s/iter. Total: 0.0534 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:13:03 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0016 s/iter. Inference: 0.0500 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:13:08 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0017 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:13:10 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.598205 (0.052493 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:13:10 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049848 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:13:10 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:13:10 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:13:10 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.22s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.712\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.225\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.206\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.388\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.169\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.447\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.468\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.425\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.519\n",
            "\u001b[32m[01/11 03:13:10 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.732 | 71.206 | 22.532 | 0.002 | 20.619 | 38.782 |\n",
            "\u001b[32m[01/11 03:13:10 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.732 |\n",
            "\u001b[32m[01/11 03:13:10 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:13:10 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:13:10 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:13:10 d2.evaluation.testing]: \u001b[0mcopypaste: 31.7318,71.2064,22.5321,0.0019,20.6190,38.7819\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:13:18 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:13:18 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:13:18 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:13:18 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:13:18 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:13:18 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:13:18 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:13:18 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:13:19 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0509 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:13:24 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0024 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:13:29 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0021 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:13:31 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.631375 (0.052631 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:13:31 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049508 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:13:31 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:13:31 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:13:31 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.311\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.707\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.228\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.207\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.380\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.447\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.477\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.446\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.521\n",
            "\u001b[32m[01/11 03:13:32 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.110 | 70.684 | 22.829 | 0.000 | 20.691 | 38.024 |\n",
            "\u001b[32m[01/11 03:13:32 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.110 |\n",
            "\u001b[32m[01/11 03:13:32 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:13:32 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:13:32 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:13:32 d2.evaluation.testing]: \u001b[0mcopypaste: 31.1102,70.6842,22.8290,0.0000,20.6905,38.0238\n",
            "\u001b[32m[01/11 03:13:32 d2.utils.events]: \u001b[0m eta: 0:24:51  iter: 3059  total_loss: 0.605  loss_cls: 0.1365  loss_box_reg: 0.4062  loss_rpn_cls: 0.01943  loss_rpn_loc: 0.01689    time: 0.7722  last_time: 0.7652  data_time: 0.0352  last_data_time: 0.0252   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:13:40 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:13:40 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:13:40 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:13:40 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:13:40 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:13:40 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:13:40 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:13:40 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:13:41 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:13:46 d2.evaluation.evaluator]: \u001b[0mInference done 105/245. Dataloading: 0.0015 s/iter. Inference: 0.0517 s/iter. Eval: 0.0003 s/iter. Total: 0.0536 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:13:51 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0015 s/iter. Inference: 0.0507 s/iter. Eval: 0.0003 s/iter. Total: 0.0525 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:13:53 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.734436 (0.053060 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:13:53 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050523 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:13:53 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:13:53 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:13:53 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.68s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.299\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.696\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.210\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.190\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.368\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.163\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.435\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.460\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.435\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.500\n",
            "\u001b[32m[01/11 03:13:54 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.924 | 69.628 | 21.012 | 0.000 | 19.004 | 36.834 |\n",
            "\u001b[32m[01/11 03:13:54 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.924 |\n",
            "\u001b[32m[01/11 03:13:54 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:13:54 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:13:54 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:13:54 d2.evaluation.testing]: \u001b[0mcopypaste: 29.9245,69.6281,21.0121,0.0000,19.0043,36.8344\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:14:02 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:14:02 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:14:02 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:14:02 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:14:02 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:14:02 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:14:02 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:14:02 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:14:03 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0015 s/iter. Inference: 0.0511 s/iter. Eval: 0.0003 s/iter. Total: 0.0529 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:14:08 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0018 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:14:13 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0018 s/iter. Inference: 0.0491 s/iter. Eval: 0.0002 s/iter. Total: 0.0512 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:14:15 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.429800 (0.051791 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:14:15 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048975 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:14:15 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:14:15 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:14:15 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.315\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.707\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.232\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.207\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.383\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.449\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.471\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.440\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.515\n",
            "\u001b[32m[01/11 03:14:15 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.495 | 70.655 | 23.216 | 0.004 | 20.747 | 38.284 |\n",
            "\u001b[32m[01/11 03:14:15 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.495 |\n",
            "\u001b[32m[01/11 03:14:15 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:14:15 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:14:15 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:14:15 d2.evaluation.testing]: \u001b[0mcopypaste: 31.4950,70.6553,23.2157,0.0038,20.7470,38.2837\n",
            "\u001b[32m[01/11 03:14:15 d2.utils.events]: \u001b[0m eta: 0:24:36  iter: 3079  total_loss: 0.596  loss_cls: 0.16  loss_box_reg: 0.4131  loss_rpn_cls: 0.01771  loss_rpn_loc: 0.01378    time: 0.7722  last_time: 0.7660  data_time: 0.0323  last_data_time: 0.0252   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:14:24 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:14:24 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:14:24 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:14:24 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:14:24 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:14:24 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:14:24 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:14:24 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:14:25 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0021 s/iter. Inference: 0.0492 s/iter. Eval: 0.0007 s/iter. Total: 0.0520 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:14:30 d2.evaluation.evaluator]: \u001b[0mInference done 111/245. Dataloading: 0.0016 s/iter. Inference: 0.0486 s/iter. Eval: 0.0003 s/iter. Total: 0.0505 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:14:35 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0015 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:14:37 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.436907 (0.051820 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:14:37 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049197 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:14:37 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:14:37 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:14:37 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.324\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.705\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.243\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.211\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.399\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.457\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.485\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.430\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.544\n",
            "\u001b[32m[01/11 03:14:37 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.366 | 70.507 | 24.335 | 0.004 | 21.072 | 39.905 |\n",
            "\u001b[32m[01/11 03:14:37 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.366 |\n",
            "\u001b[32m[01/11 03:14:37 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:14:37 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:14:37 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:14:37 d2.evaluation.testing]: \u001b[0mcopypaste: 32.3655,70.5071,24.3354,0.0043,21.0716,39.9047\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:14:45 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:14:45 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:14:45 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:14:45 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:14:45 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:14:45 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:14:45 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:14:45 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:14:46 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0504 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:14:51 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0016 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:14:56 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0021 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:14:58 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.549676 (0.052290 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:14:58 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049172 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:14:58 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:14:58 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:14:58 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.303\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.701\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.210\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.194\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.372\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.165\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.433\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.457\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.425\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.500\n",
            "\u001b[32m[01/11 03:14:59 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.322 | 70.051 | 21.015 | 0.002 | 19.363 | 37.235 |\n",
            "\u001b[32m[01/11 03:14:59 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.322 |\n",
            "\u001b[32m[01/11 03:14:59 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:14:59 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:14:59 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:14:59 d2.evaluation.testing]: \u001b[0mcopypaste: 30.3222,70.0507,21.0152,0.0016,19.3631,37.2345\n",
            "\u001b[32m[01/11 03:14:59 d2.utils.events]: \u001b[0m eta: 0:24:21  iter: 3099  total_loss: 0.6472  loss_cls: 0.1656  loss_box_reg: 0.4253  loss_rpn_cls: 0.01597  loss_rpn_loc: 0.0183    time: 0.7722  last_time: 0.7754  data_time: 0.0358  last_data_time: 0.0345   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:15:07 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:15:07 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:15:07 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:15:07 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:15:07 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:15:07 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:15:07 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:15:07 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:15:08 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0034 s/iter. Inference: 0.0484 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:15:13 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0019 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:15:18 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0017 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:15:20 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.481499 (0.052006 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:15:20 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049348 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:15:20 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:15:20 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:15:20 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.325\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.712\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.241\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.203\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.403\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.170\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.452\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.478\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.418\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.539\n",
            "\u001b[32m[01/11 03:15:20 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.516 | 71.245 | 24.135 | 0.001 | 20.282 | 40.332 |\n",
            "\u001b[32m[01/11 03:15:20 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.516 |\n",
            "\u001b[32m[01/11 03:15:20 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:15:21 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:15:21 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:15:21 d2.evaluation.testing]: \u001b[0mcopypaste: 32.5162,71.2446,24.1351,0.0015,20.2817,40.3319\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:15:28 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:15:28 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:15:28 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:15:28 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:15:28 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:15:28 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:15:28 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:15:28 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:15:29 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0523 s/iter. Eval: 0.0003 s/iter. Total: 0.0535 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:15:34 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0016 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:15:39 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0016 s/iter. Inference: 0.0500 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:15:41 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.572913 (0.052387 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:15:41 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049681 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:15:41 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:15:41 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:15:42 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.328\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.708\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.249\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.206\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.405\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.174\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.455\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.478\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.421\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.537\n",
            "\u001b[32m[01/11 03:15:42 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.762 | 70.786 | 24.917 | 0.009 | 20.582 | 40.541 |\n",
            "\u001b[32m[01/11 03:15:42 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.762 |\n",
            "\u001b[32m[01/11 03:15:42 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:15:42 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:15:42 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:15:42 d2.evaluation.testing]: \u001b[0mcopypaste: 32.7615,70.7864,24.9169,0.0093,20.5817,40.5412\n",
            "\u001b[32m[01/11 03:15:42 d2.utils.events]: \u001b[0m eta: 0:24:05  iter: 3119  total_loss: 0.6371  loss_cls: 0.1592  loss_box_reg: 0.4363  loss_rpn_cls: 0.01486  loss_rpn_loc: 0.01529    time: 0.7722  last_time: 0.7794  data_time: 0.0344  last_data_time: 0.0368   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:15:50 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:15:50 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:15:50 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:15:50 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:15:50 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:15:50 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:15:50 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:15:50 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:15:51 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:15:56 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0026 s/iter. Inference: 0.0500 s/iter. Eval: 0.0003 s/iter. Total: 0.0529 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:16:01 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0020 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:16:03 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.706741 (0.052945 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:16:03 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049981 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:16:03 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:16:03 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:16:03 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.316\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.701\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.192\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.395\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.446\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.470\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.422\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.524\n",
            "\u001b[32m[01/11 03:16:04 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.634 | 70.072 | 21.708 | 0.008 | 19.207 | 39.499 |\n",
            "\u001b[32m[01/11 03:16:04 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.634 |\n",
            "\u001b[32m[01/11 03:16:04 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:16:04 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:16:04 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:16:04 d2.evaluation.testing]: \u001b[0mcopypaste: 31.6342,70.0718,21.7080,0.0080,19.2075,39.4994\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:16:11 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:16:11 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:16:11 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:16:11 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:16:11 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:16:11 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:16:11 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:16:11 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:16:12 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0511 s/iter. Eval: 0.0002 s/iter. Total: 0.0523 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:16:17 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0015 s/iter. Inference: 0.0505 s/iter. Eval: 0.0003 s/iter. Total: 0.0523 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:16:22 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0017 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:16:25 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.669197 (0.052788 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:16:25 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050058 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:16:25 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:16:25 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:16:25 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.324\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.711\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.231\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.205\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.400\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.452\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.478\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.434\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.530\n",
            "\u001b[32m[01/11 03:16:25 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.424 | 71.056 | 23.108 | 0.000 | 20.531 | 40.038 |\n",
            "\u001b[32m[01/11 03:16:25 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.424 |\n",
            "\u001b[32m[01/11 03:16:25 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:16:25 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:16:25 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:16:25 d2.evaluation.testing]: \u001b[0mcopypaste: 32.4240,71.0559,23.1081,0.0000,20.5308,40.0377\n",
            "\u001b[32m[01/11 03:16:25 d2.utils.events]: \u001b[0m eta: 0:23:50  iter: 3139  total_loss: 0.6059  loss_cls: 0.1526  loss_box_reg: 0.4031  loss_rpn_cls: 0.01371  loss_rpn_loc: 0.01767    time: 0.7721  last_time: 0.7786  data_time: 0.0333  last_data_time: 0.0315   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:16:33 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:16:33 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:16:33 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:16:33 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:16:33 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:16:33 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:16:33 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:16:33 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:16:34 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0008 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0502 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:16:39 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0017 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:16:44 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0016 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:16:47 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.580633 (0.052419 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:16:47 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049620 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:16:47 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:16:47 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:16:47 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.312\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.697\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.209\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.200\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.382\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.171\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.463\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.432\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.506\n",
            "\u001b[32m[01/11 03:16:47 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.150 | 69.743 | 20.946 | 0.000 | 20.032 | 38.205 |\n",
            "\u001b[32m[01/11 03:16:47 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.150 |\n",
            "\u001b[32m[01/11 03:16:47 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:16:47 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:16:47 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:16:47 d2.evaluation.testing]: \u001b[0mcopypaste: 31.1501,69.7427,20.9459,0.0000,20.0324,38.2051\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:16:55 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:16:55 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:16:55 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:16:55 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:16:55 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:16:55 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:16:55 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:16:55 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:16:56 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0014 s/iter. Inference: 0.1291 s/iter. Eval: 0.0002 s/iter. Total: 0.1308 s/iter. ETA=0:00:30\n",
            "\u001b[32m[01/11 03:17:01 d2.evaluation.evaluator]: \u001b[0mInference done 111/245. Dataloading: 0.0016 s/iter. Inference: 0.0531 s/iter. Eval: 0.0003 s/iter. Total: 0.0550 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:17:06 d2.evaluation.evaluator]: \u001b[0mInference done 210/245. Dataloading: 0.0017 s/iter. Inference: 0.0510 s/iter. Eval: 0.0003 s/iter. Total: 0.0530 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:17:08 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.794956 (0.053312 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:17:08 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050628 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:17:08 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:17:08 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:17:08 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.327\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.709\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.231\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.206\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.404\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.458\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.481\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.433\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.535\n",
            "\u001b[32m[01/11 03:17:08 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.719 | 70.862 | 23.119 | 0.010 | 20.629 | 40.440 |\n",
            "\u001b[32m[01/11 03:17:08 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.719 |\n",
            "\u001b[32m[01/11 03:17:08 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:17:08 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:17:08 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:17:08 d2.evaluation.testing]: \u001b[0mcopypaste: 32.7187,70.8624,23.1191,0.0103,20.6287,40.4402\n",
            "\u001b[32m[01/11 03:17:08 d2.utils.events]: \u001b[0m eta: 0:23:34  iter: 3159  total_loss: 0.6435  loss_cls: 0.1781  loss_box_reg: 0.431  loss_rpn_cls: 0.01875  loss_rpn_loc: 0.0241    time: 0.7720  last_time: 0.7067  data_time: 0.0320  last_data_time: 0.0291   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:17:17 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:17:17 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:17:17 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:17:17 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:17:17 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:17:17 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:17:17 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:17:17 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:17:18 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0561 s/iter. Eval: 0.0003 s/iter. Total: 0.0575 s/iter. ETA=0:00:13\n",
            "\u001b[32m[01/11 03:17:23 d2.evaluation.evaluator]: \u001b[0mInference done 105/245. Dataloading: 0.0019 s/iter. Inference: 0.0515 s/iter. Eval: 0.0003 s/iter. Total: 0.0537 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:17:28 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0018 s/iter. Inference: 0.0505 s/iter. Eval: 0.0003 s/iter. Total: 0.0526 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:17:31 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.729272 (0.053039 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:17:31 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050229 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:17:31 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:17:31 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:17:31 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.29s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.335\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.718\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.245\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.220\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.411\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.464\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.493\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.452\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.544\n",
            "\u001b[32m[01/11 03:17:31 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.461 | 71.824 | 24.500 | 0.001 | 22.039 | 41.071 |\n",
            "\u001b[32m[01/11 03:17:31 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.461 |\n",
            "\u001b[32m[01/11 03:17:31 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:17:31 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:17:31 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:17:31 d2.evaluation.testing]: \u001b[0mcopypaste: 33.4611,71.8240,24.4998,0.0014,22.0390,41.0709\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:17:39 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:17:39 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:17:39 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:17:39 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:17:39 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:17:39 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:17:39 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:17:39 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:17:40 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0483 s/iter. Eval: 0.0002 s/iter. Total: 0.0496 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:17:45 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0026 s/iter. Inference: 0.0500 s/iter. Eval: 0.0003 s/iter. Total: 0.0529 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:17:50 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0024 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0523 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:17:52 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.616464 (0.052569 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:17:52 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049424 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:17:52 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:17:52 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:17:52 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.710\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.214\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.388\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.171\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.452\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.485\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.019\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.469\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.520\n",
            "\u001b[32m[01/11 03:17:52 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.905 | 71.007 | 21.689 | 0.003 | 21.364 | 38.763 |\n",
            "\u001b[32m[01/11 03:17:52 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.905 |\n",
            "\u001b[32m[01/11 03:17:52 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:17:52 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:17:52 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:17:52 d2.evaluation.testing]: \u001b[0mcopypaste: 31.9046,71.0069,21.6886,0.0030,21.3644,38.7631\n",
            "\u001b[32m[01/11 03:17:52 d2.utils.events]: \u001b[0m eta: 0:23:19  iter: 3179  total_loss: 0.6143  loss_cls: 0.1621  loss_box_reg: 0.4048  loss_rpn_cls: 0.0223  loss_rpn_loc: 0.02352    time: 0.7721  last_time: 0.7689  data_time: 0.0343  last_data_time: 0.0239   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:18:00 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:18:00 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:18:00 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:18:00 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:18:00 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:18:00 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:18:00 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:18:00 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:18:01 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0035 s/iter. Inference: 0.0490 s/iter. Eval: 0.0002 s/iter. Total: 0.0528 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:18:06 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0016 s/iter. Inference: 0.0502 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:18:11 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0015 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:18:14 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.490033 (0.052042 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:18:14 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049479 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:18:14 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:18:14 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:18:14 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.322\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.717\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.216\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.390\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.169\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.453\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.480\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.452\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.522\n",
            "\u001b[32m[01/11 03:18:14 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.205 | 71.738 | 21.536 | 0.002 | 21.551 | 39.043 |\n",
            "\u001b[32m[01/11 03:18:14 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.205 |\n",
            "\u001b[32m[01/11 03:18:14 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:18:14 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:18:14 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:18:14 d2.evaluation.testing]: \u001b[0mcopypaste: 32.2054,71.7380,21.5361,0.0024,21.5506,39.0425\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:18:22 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:18:22 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:18:22 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:18:22 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:18:22 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:18:22 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:18:22 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:18:22 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:18:23 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0485 s/iter. Eval: 0.0002 s/iter. Total: 0.0498 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:18:28 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0018 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:18:33 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0018 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:18:35 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.521645 (0.052174 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:18:35 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049325 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:18:35 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:18:35 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:18:35 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.333\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.727\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.241\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.219\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.408\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.459\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.486\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.433\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.544\n",
            "\u001b[32m[01/11 03:18:35 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.278 | 72.667 | 24.089 | 0.015 | 21.924 | 40.767 |\n",
            "\u001b[32m[01/11 03:18:35 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.278 |\n",
            "\u001b[32m[01/11 03:18:35 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:18:35 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:18:35 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:18:35 d2.evaluation.testing]: \u001b[0mcopypaste: 33.2775,72.6668,24.0890,0.0146,21.9242,40.7665\n",
            "\u001b[32m[01/11 03:18:35 d2.utils.events]: \u001b[0m eta: 0:23:04  iter: 3199  total_loss: 0.6188  loss_cls: 0.1501  loss_box_reg: 0.4201  loss_rpn_cls: 0.01754  loss_rpn_loc: 0.01235    time: 0.7720  last_time: 0.7689  data_time: 0.0332  last_data_time: 0.0247   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:18:43 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:18:43 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:18:43 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:18:43 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:18:43 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:18:43 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:18:43 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:18:43 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:18:44 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0488 s/iter. Eval: 0.0002 s/iter. Total: 0.0501 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:18:49 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0022 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:18:54 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0020 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:18:56 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.389196 (0.051622 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:18:56 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048838 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:18:57 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:18:57 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:18:57 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.321\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.720\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.222\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.209\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.392\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.171\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.452\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.482\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.531\n",
            "\u001b[32m[01/11 03:18:57 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.122 | 71.976 | 22.202 | 0.002 | 20.852 | 39.248 |\n",
            "\u001b[32m[01/11 03:18:57 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.122 |\n",
            "\u001b[32m[01/11 03:18:57 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:18:57 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:18:57 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:18:57 d2.evaluation.testing]: \u001b[0mcopypaste: 32.1225,71.9764,22.2018,0.0021,20.8520,39.2479\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:19:04 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:19:04 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:19:04 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:19:04 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:19:04 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:19:04 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:19:04 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:19:04 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:19:05 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0480 s/iter. Eval: 0.0002 s/iter. Total: 0.0494 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:19:11 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0023 s/iter. Inference: 0.0488 s/iter. Eval: 0.0002 s/iter. Total: 0.0515 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:19:16 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0020 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:19:18 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.636119 (0.052650 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:19:18 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049609 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:19:18 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:19:18 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:19:18 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.315\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.708\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.218\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.198\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.386\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.446\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.471\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.437\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.518\n",
            "\u001b[32m[01/11 03:19:18 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.524 | 70.850 | 21.752 | 0.000 | 19.846 | 38.577 |\n",
            "\u001b[32m[01/11 03:19:18 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.524 |\n",
            "\u001b[32m[01/11 03:19:18 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:19:18 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:19:18 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:19:18 d2.evaluation.testing]: \u001b[0mcopypaste: 31.5242,70.8498,21.7522,0.0000,19.8460,38.5766\n",
            "\u001b[32m[01/11 03:19:18 d2.utils.events]: \u001b[0m eta: 0:22:48  iter: 3219  total_loss: 0.6216  loss_cls: 0.1593  loss_box_reg: 0.4094  loss_rpn_cls: 0.01453  loss_rpn_loc: 0.015    time: 0.7719  last_time: 0.7677  data_time: 0.0302  last_data_time: 0.0248   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:19:26 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:19:26 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:19:26 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:19:26 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:19:26 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:19:26 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:19:26 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:19:26 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:19:27 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0472 s/iter. Eval: 0.0002 s/iter. Total: 0.0486 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:19:33 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0017 s/iter. Inference: 0.0488 s/iter. Eval: 0.0003 s/iter. Total: 0.0508 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:19:38 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0017 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:19:40 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.484044 (0.052017 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:19:40 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049244 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:19:40 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:19:40 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:19:40 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.315\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.712\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.198\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.386\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.165\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.448\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.474\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.448\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.516\n",
            "\u001b[32m[01/11 03:19:40 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.492 | 71.171 | 21.747 | 0.000 | 19.784 | 38.554 |\n",
            "\u001b[32m[01/11 03:19:40 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.492 |\n",
            "\u001b[32m[01/11 03:19:40 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:19:40 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:19:40 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:19:40 d2.evaluation.testing]: \u001b[0mcopypaste: 31.4921,71.1712,21.7467,0.0000,19.7839,38.5537\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:19:48 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:19:48 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:19:48 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:19:48 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:19:48 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:19:48 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:19:48 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:19:48 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:19:49 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0034 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0540 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:19:54 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0019 s/iter. Inference: 0.0488 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:19:59 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0020 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:20:01 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.569413 (0.052373 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:20:01 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049419 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:20:01 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:20:01 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:20:01 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.69s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.329\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.720\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.240\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.217\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.401\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.171\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.460\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.487\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.447\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.537\n",
            "\u001b[32m[01/11 03:20:02 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.945 | 71.984 | 23.980 | 0.011 | 21.701 | 40.135 |\n",
            "\u001b[32m[01/11 03:20:02 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.945 |\n",
            "\u001b[32m[01/11 03:20:02 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:20:02 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:20:02 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:20:02 d2.evaluation.testing]: \u001b[0mcopypaste: 32.9447,71.9844,23.9799,0.0114,21.7006,40.1353\n",
            "\u001b[32m[01/11 03:20:02 d2.utils.events]: \u001b[0m eta: 0:22:33  iter: 3239  total_loss: 0.585  loss_cls: 0.1376  loss_box_reg: 0.4006  loss_rpn_cls: 0.01454  loss_rpn_loc: 0.01363    time: 0.7719  last_time: 0.6926  data_time: 0.0337  last_data_time: 0.0228   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:20:10 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:20:10 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:20:10 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:20:10 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:20:10 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:20:10 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:20:10 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:20:10 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:20:11 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0512 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:20:16 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0016 s/iter. Inference: 0.0500 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:20:21 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0017 s/iter. Inference: 0.0496 s/iter. Eval: 0.0002 s/iter. Total: 0.0516 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:20:23 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.510197 (0.052126 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:20:23 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049444 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:20:23 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:20:23 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:20:23 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.22s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.318\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.711\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.224\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.207\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.389\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.169\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.445\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.469\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.439\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.512\n",
            "\u001b[32m[01/11 03:20:24 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.843 | 71.104 | 22.420 | 0.002 | 20.657 | 38.868 |\n",
            "\u001b[32m[01/11 03:20:24 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.843 |\n",
            "\u001b[32m[01/11 03:20:24 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:20:24 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:20:24 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:20:24 d2.evaluation.testing]: \u001b[0mcopypaste: 31.8426,71.1037,22.4199,0.0021,20.6570,38.8679\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:20:31 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:20:31 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:20:31 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:20:31 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:20:31 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:20:31 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:20:31 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:20:31 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:20:32 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0014 s/iter. Inference: 0.0486 s/iter. Eval: 0.0003 s/iter. Total: 0.0502 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:20:37 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0015 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:20:42 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0018 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:20:45 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.529409 (0.052206 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:20:45 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049256 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:20:45 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:20:45 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:20:45 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.710\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.218\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.203\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.389\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.171\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.448\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.473\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.440\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.519\n",
            "\u001b[32m[01/11 03:20:45 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.692 | 71.028 | 21.808 | 0.000 | 20.332 | 38.915 |\n",
            "\u001b[32m[01/11 03:20:45 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.692 |\n",
            "\u001b[32m[01/11 03:20:45 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:20:45 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:20:45 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:20:45 d2.evaluation.testing]: \u001b[0mcopypaste: 31.6918,71.0284,21.8078,0.0000,20.3317,38.9151\n",
            "\u001b[32m[01/11 03:20:45 d2.utils.events]: \u001b[0m eta: 0:22:17  iter: 3259  total_loss: 0.6177  loss_cls: 0.1482  loss_box_reg: 0.4192  loss_rpn_cls: 0.01483  loss_rpn_loc: 0.01451    time: 0.7719  last_time: 0.8003  data_time: 0.0306  last_data_time: 0.0549   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:20:53 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:20:53 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:20:53 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:20:53 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:20:53 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:20:53 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:20:53 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:20:53 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:20:54 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0485 s/iter. Eval: 0.0003 s/iter. Total: 0.0498 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:20:59 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0025 s/iter. Inference: 0.0487 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:21:04 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0024 s/iter. Inference: 0.0488 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:21:06 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.513643 (0.052140 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:21:06 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048776 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:21:06 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:21:06 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:21:06 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.333\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.723\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.252\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.210\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.412\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.174\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.465\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.492\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.445\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.547\n",
            "\u001b[32m[01/11 03:21:07 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.335 | 72.330 | 25.178 | 0.009 | 20.967 | 41.221 |\n",
            "\u001b[32m[01/11 03:21:07 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.335 |\n",
            "\u001b[32m[01/11 03:21:07 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:21:07 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:21:07 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:21:07 d2.evaluation.testing]: \u001b[0mcopypaste: 33.3349,72.3303,25.1775,0.0088,20.9673,41.2212\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:21:14 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:21:14 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:21:14 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:21:14 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:21:14 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:21:14 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:21:14 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:21:14 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:21:15 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0512 s/iter. Eval: 0.0003 s/iter. Total: 0.0526 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:21:20 d2.evaluation.evaluator]: \u001b[0mInference done 104/245. Dataloading: 0.0017 s/iter. Inference: 0.0520 s/iter. Eval: 0.0003 s/iter. Total: 0.0540 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:21:25 d2.evaluation.evaluator]: \u001b[0mInference done 200/245. Dataloading: 0.0016 s/iter. Inference: 0.0513 s/iter. Eval: 0.0003 s/iter. Total: 0.0531 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:21:28 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.802993 (0.053346 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:21:28 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050785 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:21:28 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:21:28 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:21:28 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.325\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.715\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.246\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.202\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.402\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.174\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.452\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.482\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.435\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.535\n",
            "\u001b[32m[01/11 03:21:28 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.473 | 71.459 | 24.618 | 0.005 | 20.242 | 40.184 |\n",
            "\u001b[32m[01/11 03:21:28 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.473 |\n",
            "\u001b[32m[01/11 03:21:28 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:21:28 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:21:28 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:21:28 d2.evaluation.testing]: \u001b[0mcopypaste: 32.4731,71.4585,24.6184,0.0048,20.2419,40.1842\n",
            "\u001b[32m[01/11 03:21:28 d2.utils.events]: \u001b[0m eta: 0:22:02  iter: 3279  total_loss: 0.6502  loss_cls: 0.1622  loss_box_reg: 0.4305  loss_rpn_cls: 0.02455  loss_rpn_loc: 0.02209    time: 0.7718  last_time: 0.7042  data_time: 0.0346  last_data_time: 0.0251   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:21:37 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:21:37 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:21:37 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:21:37 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:21:37 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:21:37 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:21:37 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:21:37 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:21:38 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0025 s/iter. Inference: 0.0485 s/iter. Eval: 0.0002 s/iter. Total: 0.0512 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:21:43 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0015 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:21:48 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0015 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:21:50 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.645687 (0.052690 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:21:50 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050116 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:21:50 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:21:50 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:21:50 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.30s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.318\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.712\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.233\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.210\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.389\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.170\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.449\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.481\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.452\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.524\n",
            "\u001b[32m[01/11 03:21:50 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.788 | 71.195 | 23.255 | 0.005 | 21.037 | 38.870 |\n",
            "\u001b[32m[01/11 03:21:50 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.788 |\n",
            "\u001b[32m[01/11 03:21:50 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:21:50 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:21:50 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:21:50 d2.evaluation.testing]: \u001b[0mcopypaste: 31.7879,71.1949,23.2553,0.0054,21.0366,38.8699\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:21:58 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:21:58 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:21:58 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:21:58 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:21:58 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:21:58 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:21:58 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:21:58 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:21:59 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0486 s/iter. Eval: 0.0002 s/iter. Total: 0.0499 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:22:04 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0017 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:22:09 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0016 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:22:11 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.538603 (0.052244 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:22:11 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049618 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:22:11 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:22:11 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:22:11 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.314\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.711\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.221\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.206\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.385\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.169\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.473\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.444\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.516\n",
            "\u001b[32m[01/11 03:22:12 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.389 | 71.115 | 22.075 | 0.000 | 20.606 | 38.521 |\n",
            "\u001b[32m[01/11 03:22:12 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.389 |\n",
            "\u001b[32m[01/11 03:22:12 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:22:12 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:22:12 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:22:12 d2.evaluation.testing]: \u001b[0mcopypaste: 31.3891,71.1150,22.0749,0.0000,20.6063,38.5209\n",
            "\u001b[32m[01/11 03:22:12 d2.utils.events]: \u001b[0m eta: 0:21:47  iter: 3299  total_loss: 0.5988  loss_cls: 0.1536  loss_box_reg: 0.3965  loss_rpn_cls: 0.02519  loss_rpn_loc: 0.01828    time: 0.7718  last_time: 0.6956  data_time: 0.0375  last_data_time: 0.0237   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:22:20 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:22:20 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:22:20 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:22:20 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:22:20 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:22:20 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:22:20 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:22:20 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:22:21 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:22:26 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0016 s/iter. Inference: 0.0500 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:22:31 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0019 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:22:33 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.576449 (0.052402 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:22:33 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049532 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:22:33 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:22:33 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:22:33 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.323\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.717\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.229\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.209\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.399\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.447\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.481\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.433\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.536\n",
            "\u001b[32m[01/11 03:22:34 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.285 | 71.726 | 22.860 | 0.000 | 20.914 | 39.875 |\n",
            "\u001b[32m[01/11 03:22:34 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.285 |\n",
            "\u001b[32m[01/11 03:22:34 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:22:34 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:22:34 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:22:34 d2.evaluation.testing]: \u001b[0mcopypaste: 32.2849,71.7257,22.8595,0.0000,20.9145,39.8748\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:22:41 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:22:41 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:22:41 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:22:41 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:22:41 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:22:41 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:22:41 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:22:41 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:22:42 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0021 s/iter. Inference: 0.0505 s/iter. Eval: 0.0003 s/iter. Total: 0.0529 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:22:47 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0024 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:22:52 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0021 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:22:55 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.555657 (0.052315 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:22:55 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049253 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:22:55 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:22:55 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:22:55 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.308\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.708\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.213\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.199\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.378\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.441\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.464\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.427\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.510\n",
            "\u001b[32m[01/11 03:22:55 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.838 | 70.821 | 21.267 | 0.000 | 19.850 | 37.817 |\n",
            "\u001b[32m[01/11 03:22:55 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.838 |\n",
            "\u001b[32m[01/11 03:22:55 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:22:55 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:22:55 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:22:55 d2.evaluation.testing]: \u001b[0mcopypaste: 30.8381,70.8214,21.2671,0.0000,19.8500,37.8167\n",
            "\u001b[32m[01/11 03:22:55 d2.utils.events]: \u001b[0m eta: 0:21:31  iter: 3319  total_loss: 0.5904  loss_cls: 0.148  loss_box_reg: 0.3917  loss_rpn_cls: 0.02252  loss_rpn_loc: 0.02379    time: 0.7718  last_time: 0.7826  data_time: 0.0340  last_data_time: 0.0399   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:23:03 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:23:03 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:23:03 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:23:03 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:23:03 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:23:03 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:23:03 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:23:03 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:23:04 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0482 s/iter. Eval: 0.0002 s/iter. Total: 0.0496 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:23:09 d2.evaluation.evaluator]: \u001b[0mInference done 101/245. Dataloading: 0.0015 s/iter. Inference: 0.0535 s/iter. Eval: 0.0003 s/iter. Total: 0.0553 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:23:14 d2.evaluation.evaluator]: \u001b[0mInference done 200/245. Dataloading: 0.0016 s/iter. Inference: 0.0512 s/iter. Eval: 0.0003 s/iter. Total: 0.0530 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:23:17 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.838230 (0.053493 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:23:17 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050790 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:23:17 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:23:17 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:23:17 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.302\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.704\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.182\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.202\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.365\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.165\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.432\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.457\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.437\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.493\n",
            "\u001b[32m[01/11 03:23:17 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.172 | 70.430 | 18.155 | 0.000 | 20.210 | 36.510 |\n",
            "\u001b[32m[01/11 03:23:17 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.172 |\n",
            "\u001b[32m[01/11 03:23:17 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:23:17 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:23:17 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:23:17 d2.evaluation.testing]: \u001b[0mcopypaste: 30.1722,70.4303,18.1553,0.0000,20.2100,36.5097\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:23:25 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:23:25 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:23:25 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:23:25 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:23:25 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:23:25 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:23:25 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:23:25 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:23:26 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:23:31 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0016 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:23:36 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0017 s/iter. Inference: 0.0502 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:23:38 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.630744 (0.052628 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:23:38 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049940 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:23:38 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:23:38 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:23:38 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.315\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.716\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.207\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.211\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.384\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.169\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.447\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.470\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.515\n",
            "\u001b[32m[01/11 03:23:39 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.493 | 71.596 | 20.663 | 0.001 | 21.087 | 38.426 |\n",
            "\u001b[32m[01/11 03:23:39 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.493 |\n",
            "\u001b[32m[01/11 03:23:39 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:23:39 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:23:39 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:23:39 d2.evaluation.testing]: \u001b[0mcopypaste: 31.4935,71.5958,20.6632,0.0012,21.0871,38.4256\n",
            "\u001b[32m[01/11 03:23:39 d2.utils.events]: \u001b[0m eta: 0:21:16  iter: 3339  total_loss: 0.5886  loss_cls: 0.1414  loss_box_reg: 0.3983  loss_rpn_cls: 0.01803  loss_rpn_loc: 0.01474    time: 0.7718  last_time: 0.7748  data_time: 0.0321  last_data_time: 0.0295   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:23:47 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:23:47 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:23:47 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:23:47 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:23:47 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:23:47 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:23:47 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:23:47 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:23:48 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0017 s/iter. Inference: 0.0515 s/iter. Eval: 0.0003 s/iter. Total: 0.0534 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:23:53 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0017 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:23:58 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0017 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:24:00 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.484849 (0.052020 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:24:00 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049239 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:24:00 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:24:00 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:24:00 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.713\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.226\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.202\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.394\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.446\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.471\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.423\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.525\n",
            "\u001b[32m[01/11 03:24:00 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.872 | 71.301 | 22.564 | 0.000 | 20.230 | 39.374 |\n",
            "\u001b[32m[01/11 03:24:00 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.872 |\n",
            "\u001b[32m[01/11 03:24:00 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:24:00 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:24:00 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:24:00 d2.evaluation.testing]: \u001b[0mcopypaste: 31.8722,71.3006,22.5637,0.0000,20.2296,39.3739\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:24:08 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:24:08 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:24:08 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:24:08 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:24:08 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:24:08 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:24:08 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:24:08 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:24:09 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0024 s/iter. Inference: 0.0506 s/iter. Eval: 0.0003 s/iter. Total: 0.0533 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:24:14 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0020 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:24:19 d2.evaluation.evaluator]: \u001b[0mInference done 201/245. Dataloading: 0.0018 s/iter. Inference: 0.0507 s/iter. Eval: 0.0003 s/iter. Total: 0.0528 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:24:22 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.813521 (0.053390 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:24:22 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050507 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:24:22 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:24:22 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:24:22 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.314\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.711\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.219\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.209\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.386\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.169\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.444\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.469\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.429\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.518\n",
            "\u001b[32m[01/11 03:24:22 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.448 | 71.086 | 21.873 | 0.002 | 20.938 | 38.557 |\n",
            "\u001b[32m[01/11 03:24:22 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.448 |\n",
            "\u001b[32m[01/11 03:24:22 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:24:22 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:24:22 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:24:22 d2.evaluation.testing]: \u001b[0mcopypaste: 31.4481,71.0858,21.8725,0.0017,20.9377,38.5569\n",
            "\u001b[32m[01/11 03:24:22 d2.utils.events]: \u001b[0m eta: 0:21:01  iter: 3359  total_loss: 0.6248  loss_cls: 0.147  loss_box_reg: 0.4281  loss_rpn_cls: 0.0193  loss_rpn_loc: 0.01822    time: 0.7719  last_time: 0.7822  data_time: 0.0339  last_data_time: 0.0357   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:24:30 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:24:30 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:24:30 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:24:30 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:24:30 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:24:30 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:24:30 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:24:30 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:24:32 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0498 s/iter. Eval: 0.0002 s/iter. Total: 0.0513 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:24:37 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0016 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0523 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:24:42 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0015 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:24:44 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.568654 (0.052369 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:24:44 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049672 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:24:44 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:24:44 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:24:44 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.312\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.707\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.209\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.205\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.384\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.167\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.440\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.467\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.433\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.513\n",
            "\u001b[32m[01/11 03:24:44 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.177 | 70.691 | 20.923 | 0.002 | 20.538 | 38.369 |\n",
            "\u001b[32m[01/11 03:24:44 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.177 |\n",
            "\u001b[32m[01/11 03:24:44 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:24:44 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:24:44 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:24:44 d2.evaluation.testing]: \u001b[0mcopypaste: 31.1769,70.6906,20.9225,0.0021,20.5379,38.3686\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:24:52 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:24:52 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:24:52 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:24:52 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:24:52 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:24:52 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:24:52 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:24:52 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:24:53 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0019 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0525 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:24:58 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0027 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0528 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:25:03 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0023 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:25:05 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.576511 (0.052402 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:25:05 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049148 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:25:05 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:25:05 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:25:05 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.340\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.730\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.249\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.216\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.421\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.179\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.463\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.487\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.422\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.552\n",
            "\u001b[32m[01/11 03:25:06 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.996 | 73.003 | 24.856 | 0.000 | 21.633 | 42.088 |\n",
            "\u001b[32m[01/11 03:25:06 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.996 |\n",
            "\u001b[32m[01/11 03:25:06 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:25:06 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:25:06 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:25:06 d2.evaluation.testing]: \u001b[0mcopypaste: 33.9960,73.0030,24.8555,0.0000,21.6334,42.0884\n",
            "\u001b[32m[01/11 03:25:06 d2.utils.events]: \u001b[0m eta: 0:20:45  iter: 3379  total_loss: 0.628  loss_cls: 0.1406  loss_box_reg: 0.4189  loss_rpn_cls: 0.02091  loss_rpn_loc: 0.01732    time: 0.7719  last_time: 0.7746  data_time: 0.0319  last_data_time: 0.0236   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:25:14 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:25:14 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:25:14 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:25:14 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:25:14 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:25:14 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:25:14 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:25:14 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:25:15 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0506 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:25:20 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0017 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:25:25 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0017 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:25:27 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.493649 (0.052057 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:25:27 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049327 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:25:27 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:25:27 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:25:27 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.297\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.698\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.189\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.192\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.366\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.160\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.429\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.460\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.442\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.495\n",
            "\u001b[32m[01/11 03:25:27 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 29.727 | 69.804 | 18.926 | 0.000 | 19.203 | 36.569 |\n",
            "\u001b[32m[01/11 03:25:27 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 29.727 |\n",
            "\u001b[32m[01/11 03:25:27 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:25:27 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:25:27 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:25:27 d2.evaluation.testing]: \u001b[0mcopypaste: 29.7275,69.8036,18.9257,0.0000,19.2025,36.5687\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:25:35 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:25:35 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:25:35 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:25:35 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:25:35 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:25:35 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:25:35 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:25:35 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:25:36 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0504 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:25:41 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0017 s/iter. Inference: 0.0502 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:25:46 d2.evaluation.evaluator]: \u001b[0mInference done 201/245. Dataloading: 0.0019 s/iter. Inference: 0.0506 s/iter. Eval: 0.0003 s/iter. Total: 0.0529 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:25:49 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.786555 (0.053277 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:25:49 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050381 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:25:49 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:25:49 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:25:49 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.333\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.717\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.241\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.207\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.412\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.176\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.463\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.490\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.444\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.545\n",
            "\u001b[32m[01/11 03:25:49 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.283 | 71.671 | 24.148 | 0.000 | 20.711 | 41.155 |\n",
            "\u001b[32m[01/11 03:25:49 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.283 |\n",
            "\u001b[32m[01/11 03:25:49 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:25:49 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:25:49 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:25:49 d2.evaluation.testing]: \u001b[0mcopypaste: 33.2833,71.6712,24.1477,0.0000,20.7112,41.1550\n",
            "\u001b[32m[01/11 03:25:49 d2.utils.events]: \u001b[0m eta: 0:20:30  iter: 3399  total_loss: 0.6025  loss_cls: 0.1595  loss_box_reg: 0.3927  loss_rpn_cls: 0.02285  loss_rpn_loc: 0.02396    time: 0.7718  last_time: 0.7700  data_time: 0.0338  last_data_time: 0.0275   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:25:57 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:25:57 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:25:57 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:25:57 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:25:57 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:25:57 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:25:57 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:25:57 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:25:58 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0485 s/iter. Eval: 0.0002 s/iter. Total: 0.0498 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:26:03 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0021 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0523 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:26:08 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0020 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:26:11 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.749293 (0.053122 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:26:11 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050050 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:26:11 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:26:11 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:26:11 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.46s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.22s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.325\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.718\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.229\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.213\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.397\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.171\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.454\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.474\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.431\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.524\n",
            "\u001b[32m[01/11 03:26:11 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.496 | 71.832 | 22.851 | 0.010 | 21.252 | 39.696 |\n",
            "\u001b[32m[01/11 03:26:11 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.496 |\n",
            "\u001b[32m[01/11 03:26:12 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:26:12 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:26:12 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:26:12 d2.evaluation.testing]: \u001b[0mcopypaste: 32.4964,71.8319,22.8506,0.0099,21.2518,39.6955\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:26:19 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:26:19 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:26:19 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:26:19 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:26:19 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:26:19 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:26:19 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:26:19 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:26:20 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0483 s/iter. Eval: 0.0002 s/iter. Total: 0.0495 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:26:25 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0014 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:26:30 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0015 s/iter. Inference: 0.0506 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:26:33 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.701066 (0.052921 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:26:33 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050393 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:26:33 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:26:33 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:26:33 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.322\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.710\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.238\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.207\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.396\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.452\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.478\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.435\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.529\n",
            "\u001b[32m[01/11 03:26:33 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.160 | 70.964 | 23.815 | 0.000 | 20.658 | 39.648 |\n",
            "\u001b[32m[01/11 03:26:33 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.160 |\n",
            "\u001b[32m[01/11 03:26:33 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:26:33 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:26:33 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:26:33 d2.evaluation.testing]: \u001b[0mcopypaste: 32.1603,70.9635,23.8153,0.0000,20.6578,39.6477\n",
            "\u001b[32m[01/11 03:26:33 d2.utils.events]: \u001b[0m eta: 0:20:15  iter: 3419  total_loss: 0.5925  loss_cls: 0.1484  loss_box_reg: 0.4026  loss_rpn_cls: 0.01321  loss_rpn_loc: 0.01319    time: 0.7718  last_time: 0.7657  data_time: 0.0323  last_data_time: 0.0218   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:26:42 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:26:42 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:26:42 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:26:42 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:26:42 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:26:42 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:26:42 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:26:42 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:26:43 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0016 s/iter. Inference: 0.0510 s/iter. Eval: 0.0003 s/iter. Total: 0.0529 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:26:48 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0020 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:26:53 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0018 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:26:55 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.476471 (0.051985 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:26:55 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049218 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:26:55 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:26:55 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:26:55 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.329\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.724\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.252\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.204\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.408\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.175\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.455\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.482\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.425\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.542\n",
            "\u001b[32m[01/11 03:26:55 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.933 | 72.360 | 25.247 | 0.005 | 20.380 | 40.821 |\n",
            "\u001b[32m[01/11 03:26:55 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.933 |\n",
            "\u001b[32m[01/11 03:26:55 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:26:55 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:26:55 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:26:55 d2.evaluation.testing]: \u001b[0mcopypaste: 32.9330,72.3604,25.2467,0.0047,20.3797,40.8208\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:27:03 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:27:03 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:27:03 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:27:03 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:27:03 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:27:03 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:27:03 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:27:03 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:27:04 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0489 s/iter. Eval: 0.0002 s/iter. Total: 0.0500 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:27:09 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0016 s/iter. Inference: 0.0509 s/iter. Eval: 0.0003 s/iter. Total: 0.0529 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:27:14 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0016 s/iter. Inference: 0.0500 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:27:16 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.674673 (0.052811 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:27:16 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049978 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:27:16 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:27:16 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:27:16 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.320\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.715\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.226\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.207\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.393\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.171\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.446\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.476\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.438\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.524\n",
            "\u001b[32m[01/11 03:27:17 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.010 | 71.488 | 22.569 | 0.005 | 20.711 | 39.301 |\n",
            "\u001b[32m[01/11 03:27:17 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.010 |\n",
            "\u001b[32m[01/11 03:27:17 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:27:17 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:27:17 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:27:17 d2.evaluation.testing]: \u001b[0mcopypaste: 32.0099,71.4884,22.5692,0.0046,20.7113,39.3009\n",
            "\u001b[32m[01/11 03:27:17 d2.utils.events]: \u001b[0m eta: 0:19:59  iter: 3439  total_loss: 0.6752  loss_cls: 0.1633  loss_box_reg: 0.4502  loss_rpn_cls: 0.01952  loss_rpn_loc: 0.02467    time: 0.7718  last_time: 0.7019  data_time: 0.0353  last_data_time: 0.0256   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:27:25 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:27:25 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:27:25 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:27:25 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:27:25 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:27:25 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:27:25 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:27:25 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:27:26 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0479 s/iter. Eval: 0.0003 s/iter. Total: 0.0492 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:27:31 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0018 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:27:36 d2.evaluation.evaluator]: \u001b[0mInference done 202/245. Dataloading: 0.0017 s/iter. Inference: 0.0504 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:27:39 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.672588 (0.052802 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:27:39 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050000 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:27:39 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:27:39 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:27:39 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.29s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.327\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.719\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.227\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.209\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.403\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.176\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.460\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.492\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.454\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.541\n",
            "\u001b[32m[01/11 03:27:39 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.691 | 71.926 | 22.651 | 0.001 | 20.885 | 40.278 |\n",
            "\u001b[32m[01/11 03:27:39 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.691 |\n",
            "\u001b[32m[01/11 03:27:39 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:27:39 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:27:39 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:27:39 d2.evaluation.testing]: \u001b[0mcopypaste: 32.6915,71.9265,22.6507,0.0013,20.8854,40.2776\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:27:47 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:27:47 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:27:47 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:27:47 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:27:47 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:27:47 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:27:47 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:27:47 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:27:48 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0490 s/iter. Eval: 0.0002 s/iter. Total: 0.0502 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:27:53 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0018 s/iter. Inference: 0.0508 s/iter. Eval: 0.0003 s/iter. Total: 0.0529 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:27:58 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0016 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:28:00 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.496029 (0.052067 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:28:00 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049390 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:28:00 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:28:00 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:28:00 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.318\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.717\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.213\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.214\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.384\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.447\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.469\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.438\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.513\n",
            "\u001b[32m[01/11 03:28:00 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.850 | 71.710 | 21.327 | 0.000 | 21.443 | 38.383 |\n",
            "\u001b[32m[01/11 03:28:00 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.850 |\n",
            "\u001b[32m[01/11 03:28:00 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:28:00 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:28:00 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:28:00 d2.evaluation.testing]: \u001b[0mcopypaste: 31.8497,71.7098,21.3270,0.0000,21.4425,38.3828\n",
            "\u001b[32m[01/11 03:28:00 d2.utils.events]: \u001b[0m eta: 0:19:44  iter: 3459  total_loss: 0.6339  loss_cls: 0.1526  loss_box_reg: 0.4123  loss_rpn_cls: 0.02481  loss_rpn_loc: 0.01899    time: 0.7719  last_time: 0.7959  data_time: 0.0382  last_data_time: 0.0401   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:28:09 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:28:09 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:28:09 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:28:09 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:28:09 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:28:09 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:28:09 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:28:09 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:28:10 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0024 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:28:15 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0021 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:28:20 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0024 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:28:22 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.667287 (0.052780 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:28:22 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049464 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:28:22 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:28:22 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:28:22 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.313\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.707\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.198\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.383\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.169\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.442\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.467\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.439\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.509\n",
            "\u001b[32m[01/11 03:28:22 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.274 | 70.748 | 21.677 | 0.010 | 19.839 | 38.284 |\n",
            "\u001b[32m[01/11 03:28:22 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.274 |\n",
            "\u001b[32m[01/11 03:28:22 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:28:22 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:28:22 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:28:22 d2.evaluation.testing]: \u001b[0mcopypaste: 31.2743,70.7479,21.6769,0.0097,19.8395,38.2840\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:28:30 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:28:30 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:28:30 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:28:30 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:28:30 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:28:30 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:28:30 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:28:30 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:28:31 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0488 s/iter. Eval: 0.0002 s/iter. Total: 0.0500 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:28:36 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0018 s/iter. Inference: 0.0506 s/iter. Eval: 0.0003 s/iter. Total: 0.0527 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:28:41 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0017 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:28:43 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.543315 (0.052264 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:28:43 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049539 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:28:43 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:28:43 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:28:43 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.713\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.210\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.205\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.388\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.174\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.447\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.466\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.429\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.513\n",
            "\u001b[32m[01/11 03:28:44 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.736 | 71.261 | 20.967 | 0.001 | 20.461 | 38.794 |\n",
            "\u001b[32m[01/11 03:28:44 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.736 |\n",
            "\u001b[32m[01/11 03:28:44 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:28:44 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:28:44 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:28:44 d2.evaluation.testing]: \u001b[0mcopypaste: 31.7358,71.2615,20.9675,0.0013,20.4608,38.7941\n",
            "\u001b[32m[01/11 03:28:44 d2.utils.events]: \u001b[0m eta: 0:19:29  iter: 3479  total_loss: 0.5954  loss_cls: 0.1557  loss_box_reg: 0.3985  loss_rpn_cls: 0.01785  loss_rpn_loc: 0.01411    time: 0.7720  last_time: 0.7007  data_time: 0.0301  last_data_time: 0.0233   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:28:52 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:28:52 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:28:52 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:28:52 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:28:52 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:28:52 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:28:52 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:28:52 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:28:53 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:28:58 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0021 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:29:03 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0018 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:29:05 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.526532 (0.052194 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:29:05 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049409 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:29:05 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:29:05 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:29:05 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.327\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.711\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.239\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.206\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.402\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.458\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.483\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.439\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.536\n",
            "\u001b[32m[01/11 03:29:05 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.684 | 71.073 | 23.913 | 0.000 | 20.642 | 40.236 |\n",
            "\u001b[32m[01/11 03:29:05 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.684 |\n",
            "\u001b[32m[01/11 03:29:05 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:29:05 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:29:05 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:29:05 d2.evaluation.testing]: \u001b[0mcopypaste: 32.6837,71.0732,23.9134,0.0000,20.6420,40.2362\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:29:13 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:29:13 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:29:13 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:29:13 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:29:13 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:29:13 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:29:13 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:29:13 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:29:14 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0510 s/iter. Eval: 0.0003 s/iter. Total: 0.0523 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:29:19 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0018 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:29:24 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0020 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:29:26 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.533392 (0.052222 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:29:26 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049102 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:29:26 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:29:26 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:29:26 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.48s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.328\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.714\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.241\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.209\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.402\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.171\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.455\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.486\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.445\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.537\n",
            "\u001b[32m[01/11 03:29:27 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.761 | 71.438 | 24.054 | 0.001 | 20.929 | 40.195 |\n",
            "\u001b[32m[01/11 03:29:27 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.761 |\n",
            "\u001b[32m[01/11 03:29:27 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:29:27 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:29:27 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:29:27 d2.evaluation.testing]: \u001b[0mcopypaste: 32.7607,71.4381,24.0537,0.0015,20.9286,40.1946\n",
            "\u001b[32m[01/11 03:29:27 d2.utils.events]: \u001b[0m eta: 0:19:13  iter: 3499  total_loss: 0.6079  loss_cls: 0.145  loss_box_reg: 0.411  loss_rpn_cls: 0.01771  loss_rpn_loc: 0.01449    time: 0.7719  last_time: 0.7657  data_time: 0.0326  last_data_time: 0.0232   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:29:35 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:29:35 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:29:35 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:29:35 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:29:35 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:29:35 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:29:35 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:29:35 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:29:36 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0535 s/iter. Eval: 0.0002 s/iter. Total: 0.0548 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:29:42 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0015 s/iter. Inference: 0.0493 s/iter. Eval: 0.0002 s/iter. Total: 0.0511 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:29:47 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0015 s/iter. Inference: 0.0496 s/iter. Eval: 0.0002 s/iter. Total: 0.0515 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:29:49 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.478343 (0.051993 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:29:49 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049379 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:29:49 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:29:49 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:29:49 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.331\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.719\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.241\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.216\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.404\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.175\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.455\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.480\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.441\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.529\n",
            "\u001b[32m[01/11 03:29:49 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.067 | 71.928 | 24.105 | 0.000 | 21.587 | 40.369 |\n",
            "\u001b[32m[01/11 03:29:49 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.067 |\n",
            "\u001b[32m[01/11 03:29:49 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:29:49 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:29:49 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:29:49 d2.evaluation.testing]: \u001b[0mcopypaste: 33.0674,71.9284,24.1052,0.0000,21.5866,40.3686\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:29:57 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:29:57 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:29:57 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:29:57 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:29:57 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:29:57 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:29:57 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:29:57 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:29:58 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0484 s/iter. Eval: 0.0002 s/iter. Total: 0.0497 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:30:03 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0019 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:30:08 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0018 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:30:10 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.617173 (0.052572 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:30:10 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049807 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:30:10 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:30:10 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:30:10 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.333\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.728\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.252\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.212\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.407\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.174\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.465\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.492\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.448\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.545\n",
            "\u001b[32m[01/11 03:30:10 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.273 | 72.814 | 25.171 | 0.001 | 21.172 | 40.737 |\n",
            "\u001b[32m[01/11 03:30:10 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.273 |\n",
            "\u001b[32m[01/11 03:30:10 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:30:10 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:30:10 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:30:10 d2.evaluation.testing]: \u001b[0mcopypaste: 33.2732,72.8137,25.1714,0.0011,21.1717,40.7374\n",
            "\u001b[32m[01/11 03:30:10 d2.utils.events]: \u001b[0m eta: 0:18:58  iter: 3519  total_loss: 0.6562  loss_cls: 0.1613  loss_box_reg: 0.4282  loss_rpn_cls: 0.0182  loss_rpn_loc: 0.01892    time: 0.7719  last_time: 0.7775  data_time: 0.0326  last_data_time: 0.0223   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:30:19 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:30:19 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:30:19 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:30:19 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:30:19 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:30:19 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:30:19 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:30:19 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:30:20 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0020 s/iter. Inference: 0.0483 s/iter. Eval: 0.0002 s/iter. Total: 0.0506 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:30:25 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0019 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:30:30 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0021 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:30:32 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.557039 (0.052321 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:30:32 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049106 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:30:32 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:30:32 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:30:32 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.330\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.717\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.258\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.210\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.405\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.174\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.459\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.485\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.439\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.539\n",
            "\u001b[32m[01/11 03:30:32 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.034 | 71.671 | 25.757 | 0.000 | 20.961 | 40.458 |\n",
            "\u001b[32m[01/11 03:30:32 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.034 |\n",
            "\u001b[32m[01/11 03:30:32 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:30:32 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:30:32 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:30:32 d2.evaluation.testing]: \u001b[0mcopypaste: 33.0336,71.6708,25.7570,0.0000,20.9614,40.4582\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:30:40 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:30:40 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:30:40 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:30:40 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:30:40 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:30:40 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:30:40 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:30:40 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:30:41 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0041 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0532 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:30:46 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0019 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:30:51 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0020 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:30:53 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.531363 (0.052214 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:30:53 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049208 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:30:53 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:30:53 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:30:53 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.316\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.711\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.230\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.201\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.387\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.171\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.444\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.474\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.448\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.515\n",
            "\u001b[32m[01/11 03:30:53 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.641 | 71.107 | 23.033 | 0.000 | 20.095 | 38.740 |\n",
            "\u001b[32m[01/11 03:30:53 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.641 |\n",
            "\u001b[32m[01/11 03:30:53 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:30:53 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:30:53 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:30:53 d2.evaluation.testing]: \u001b[0mcopypaste: 31.6410,71.1074,23.0332,0.0000,20.0954,38.7396\n",
            "\u001b[32m[01/11 03:30:53 d2.utils.events]: \u001b[0m eta: 0:18:42  iter: 3539  total_loss: 0.5944  loss_cls: 0.1478  loss_box_reg: 0.4002  loss_rpn_cls: 0.01987  loss_rpn_loc: 0.01581    time: 0.7718  last_time: 0.7754  data_time: 0.0301  last_data_time: 0.0220   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:31:01 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:31:01 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:31:02 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:31:02 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:31:02 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:31:02 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:31:02 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:31:02 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:31:03 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0530 s/iter. Eval: 0.0003 s/iter. Total: 0.0542 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:31:08 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0018 s/iter. Inference: 0.0500 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:31:13 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0016 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:31:15 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.578821 (0.052412 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:31:15 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049682 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:31:15 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:31:15 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:31:15 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.335\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.722\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.264\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.212\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.412\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.177\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.462\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.490\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.445\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.544\n",
            "\u001b[32m[01/11 03:31:15 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.545 | 72.207 | 26.420 | 0.000 | 21.227 | 41.173 |\n",
            "\u001b[32m[01/11 03:31:15 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.545 |\n",
            "\u001b[32m[01/11 03:31:15 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:31:15 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:31:15 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:31:15 d2.evaluation.testing]: \u001b[0mcopypaste: 33.5447,72.2071,26.4195,0.0000,21.2265,41.1733\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:31:23 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:31:23 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:31:23 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:31:23 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:31:23 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:31:23 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:31:23 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:31:23 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:31:24 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0483 s/iter. Eval: 0.0003 s/iter. Total: 0.0497 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:31:29 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0017 s/iter. Inference: 0.0507 s/iter. Eval: 0.0003 s/iter. Total: 0.0527 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:31:34 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0016 s/iter. Inference: 0.0502 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:31:36 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.660260 (0.052751 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:31:36 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050132 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:31:36 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:31:36 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:31:36 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.308\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.701\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.211\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.196\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.377\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.437\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.458\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.432\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.498\n",
            "\u001b[32m[01/11 03:31:36 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.792 | 70.096 | 21.085 | 0.000 | 19.642 | 37.657 |\n",
            "\u001b[32m[01/11 03:31:36 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.792 |\n",
            "\u001b[32m[01/11 03:31:36 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:31:36 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:31:36 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:31:36 d2.evaluation.testing]: \u001b[0mcopypaste: 30.7922,70.0963,21.0848,0.0000,19.6418,37.6572\n",
            "\u001b[32m[01/11 03:31:36 d2.utils.events]: \u001b[0m eta: 0:18:27  iter: 3559  total_loss: 0.6124  loss_cls: 0.1579  loss_box_reg: 0.42  loss_rpn_cls: 0.01443  loss_rpn_loc: 0.01529    time: 0.7718  last_time: 0.7008  data_time: 0.0321  last_data_time: 0.0268   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:31:45 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:31:45 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:31:45 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:31:45 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:31:45 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:31:45 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:31:45 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:31:45 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:31:46 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0487 s/iter. Eval: 0.0003 s/iter. Total: 0.0500 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:31:51 d2.evaluation.evaluator]: \u001b[0mInference done 103/245. Dataloading: 0.0018 s/iter. Inference: 0.0520 s/iter. Eval: 0.0003 s/iter. Total: 0.0542 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:31:56 d2.evaluation.evaluator]: \u001b[0mInference done 198/245. Dataloading: 0.0021 s/iter. Inference: 0.0510 s/iter. Eval: 0.0003 s/iter. Total: 0.0535 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:31:59 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:13.038383 (0.054327 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:31:59 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.051285 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:31:59 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:31:59 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:31:59 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.315\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.709\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.205\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.200\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.387\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.445\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.472\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.441\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.516\n",
            "\u001b[32m[01/11 03:31:59 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.527 | 70.878 | 20.472 | 0.000 | 20.020 | 38.675 |\n",
            "\u001b[32m[01/11 03:31:59 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.527 |\n",
            "\u001b[32m[01/11 03:31:59 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:31:59 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:31:59 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:31:59 d2.evaluation.testing]: \u001b[0mcopypaste: 31.5271,70.8783,20.4718,0.0000,20.0202,38.6753\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:32:07 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:32:07 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:32:07 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:32:07 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:32:07 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:32:07 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:32:07 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:32:07 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:32:08 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0008 s/iter. Inference: 0.0482 s/iter. Eval: 0.0003 s/iter. Total: 0.0493 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:32:13 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0014 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:32:18 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0014 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:32:20 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.470984 (0.051962 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:32:20 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049465 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:32:20 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:32:20 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:32:20 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.65s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.325\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.718\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.231\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.205\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.397\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.453\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.478\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.439\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.527\n",
            "\u001b[32m[01/11 03:32:21 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.455 | 71.780 | 23.119 | 0.000 | 20.489 | 39.749 |\n",
            "\u001b[32m[01/11 03:32:21 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.455 |\n",
            "\u001b[32m[01/11 03:32:21 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:32:21 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:32:21 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:32:21 d2.evaluation.testing]: \u001b[0mcopypaste: 32.4552,71.7801,23.1187,0.0000,20.4893,39.7491\n",
            "\u001b[32m[01/11 03:32:21 d2.utils.events]: \u001b[0m eta: 0:18:12  iter: 3579  total_loss: 0.6316  loss_cls: 0.1442  loss_box_reg: 0.4226  loss_rpn_cls: 0.01579  loss_rpn_loc: 0.01501    time: 0.7718  last_time: 0.7694  data_time: 0.0330  last_data_time: 0.0279   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:32:29 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:32:29 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:32:29 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:32:29 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:32:29 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:32:29 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:32:29 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:32:29 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:32:30 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0500 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:32:35 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0019 s/iter. Inference: 0.0504 s/iter. Eval: 0.0003 s/iter. Total: 0.0526 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:32:40 d2.evaluation.evaluator]: \u001b[0mInference done 199/245. Dataloading: 0.0018 s/iter. Inference: 0.0511 s/iter. Eval: 0.0003 s/iter. Total: 0.0533 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:32:43 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.941123 (0.053921 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:32:43 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050862 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:32:43 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:32:43 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:32:43 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.310\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.706\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.205\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.191\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.382\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.444\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.468\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.438\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.511\n",
            "\u001b[32m[01/11 03:32:43 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.031 | 70.567 | 20.491 | 0.000 | 19.071 | 38.162 |\n",
            "\u001b[32m[01/11 03:32:43 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.031 |\n",
            "\u001b[32m[01/11 03:32:43 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:32:43 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:32:43 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:32:43 d2.evaluation.testing]: \u001b[0mcopypaste: 31.0308,70.5675,20.4909,0.0000,19.0706,38.1620\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:32:51 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:32:51 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:32:51 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:32:51 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:32:51 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:32:51 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:32:51 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:32:51 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:32:52 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0016 s/iter. Inference: 0.0505 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:32:57 d2.evaluation.evaluator]: \u001b[0mInference done 104/245. Dataloading: 0.0017 s/iter. Inference: 0.0518 s/iter. Eval: 0.0003 s/iter. Total: 0.0539 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:33:02 d2.evaluation.evaluator]: \u001b[0mInference done 199/245. Dataloading: 0.0017 s/iter. Inference: 0.0515 s/iter. Eval: 0.0003 s/iter. Total: 0.0535 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:33:04 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.868012 (0.053617 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:33:04 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050928 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:33:04 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:33:04 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:33:04 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.328\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.719\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.242\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.211\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.401\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.175\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.453\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.476\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.432\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.528\n",
            "\u001b[32m[01/11 03:33:05 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.790 | 71.850 | 24.202 | 0.000 | 21.112 | 40.053 |\n",
            "\u001b[32m[01/11 03:33:05 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.790 |\n",
            "\u001b[32m[01/11 03:33:05 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:33:05 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:33:05 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:33:05 d2.evaluation.testing]: \u001b[0mcopypaste: 32.7896,71.8500,24.2024,0.0000,21.1125,40.0529\n",
            "\u001b[32m[01/11 03:33:05 d2.utils.events]: \u001b[0m eta: 0:17:56  iter: 3599  total_loss: 0.6129  loss_cls: 0.1613  loss_box_reg: 0.4074  loss_rpn_cls: 0.01288  loss_rpn_loc: 0.01269    time: 0.7718  last_time: 0.6771  data_time: 0.0352  last_data_time: 0.0349   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:33:13 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:33:13 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:33:13 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:33:13 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:33:13 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:33:13 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:33:13 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:33:13 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:33:14 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0022 s/iter. Inference: 0.0544 s/iter. Eval: 0.0003 s/iter. Total: 0.0568 s/iter. ETA=0:00:13\n",
            "\u001b[32m[01/11 03:33:19 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0015 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:33:24 d2.evaluation.evaluator]: \u001b[0mInference done 202/245. Dataloading: 0.0015 s/iter. Inference: 0.0510 s/iter. Eval: 0.0003 s/iter. Total: 0.0529 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:33:27 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.773218 (0.053222 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:33:27 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050676 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:33:27 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:33:27 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:33:27 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.316\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.710\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.223\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.197\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.387\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.451\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.474\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.452\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.512\n",
            "\u001b[32m[01/11 03:33:27 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.568 | 71.006 | 22.289 | 0.001 | 19.685 | 38.674 |\n",
            "\u001b[32m[01/11 03:33:27 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.568 |\n",
            "\u001b[32m[01/11 03:33:27 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:33:27 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:33:27 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:33:27 d2.evaluation.testing]: \u001b[0mcopypaste: 31.5677,71.0060,22.2890,0.0013,19.6846,38.6735\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:33:35 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:33:35 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:33:35 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:33:35 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:33:35 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:33:35 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:33:35 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:33:35 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:33:36 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0017 s/iter. Inference: 0.0489 s/iter. Eval: 0.0002 s/iter. Total: 0.0509 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:33:41 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0019 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:33:46 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0017 s/iter. Inference: 0.0502 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:33:48 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.656490 (0.052735 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:33:48 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050034 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:33:48 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:33:48 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:33:48 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.332\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.724\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.261\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.209\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.408\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.177\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.462\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.487\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.434\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.545\n",
            "\u001b[32m[01/11 03:33:49 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.162 | 72.352 | 26.089 | 0.000 | 20.922 | 40.836 |\n",
            "\u001b[32m[01/11 03:33:49 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.162 |\n",
            "\u001b[32m[01/11 03:33:49 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:33:49 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:33:49 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:33:49 d2.evaluation.testing]: \u001b[0mcopypaste: 33.1625,72.3519,26.0890,0.0000,20.9218,40.8362\n",
            "\u001b[32m[01/11 03:33:49 d2.utils.events]: \u001b[0m eta: 0:17:41  iter: 3619  total_loss: 0.6029  loss_cls: 0.1545  loss_box_reg: 0.4124  loss_rpn_cls: 0.01892  loss_rpn_loc: 0.01679    time: 0.7719  last_time: 0.7677  data_time: 0.0365  last_data_time: 0.0259   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:33:57 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:33:57 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:33:57 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:33:57 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:33:57 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:33:57 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:33:57 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:33:57 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:33:58 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0046 s/iter. Inference: 0.0497 s/iter. Eval: 0.0002 s/iter. Total: 0.0545 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:34:03 d2.evaluation.evaluator]: \u001b[0mInference done 105/245. Dataloading: 0.0019 s/iter. Inference: 0.0511 s/iter. Eval: 0.0002 s/iter. Total: 0.0533 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:34:08 d2.evaluation.evaluator]: \u001b[0mInference done 202/245. Dataloading: 0.0017 s/iter. Inference: 0.0506 s/iter. Eval: 0.0003 s/iter. Total: 0.0526 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:34:10 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.766762 (0.053195 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:34:10 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050454 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:34:10 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:34:10 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:34:10 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.327\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.710\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.246\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.198\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.406\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.175\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.458\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.482\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.432\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.538\n",
            "\u001b[32m[01/11 03:34:11 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.738 | 70.960 | 24.572 | 0.000 | 19.788 | 40.591 |\n",
            "\u001b[32m[01/11 03:34:11 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.738 |\n",
            "\u001b[32m[01/11 03:34:11 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:34:11 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:34:11 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:34:11 d2.evaluation.testing]: \u001b[0mcopypaste: 32.7378,70.9601,24.5722,0.0000,19.7877,40.5913\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:34:18 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:34:18 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:34:18 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:34:18 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:34:18 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:34:18 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:34:18 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:34:18 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:34:19 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0016 s/iter. Inference: 0.0477 s/iter. Eval: 0.0003 s/iter. Total: 0.0495 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:34:24 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0018 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:34:29 d2.evaluation.evaluator]: \u001b[0mInference done 209/245. Dataloading: 0.0016 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0509 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:34:31 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.355395 (0.051481 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:34:31 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048899 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:34:31 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:34:31 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:34:31 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.22s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.323\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.715\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.240\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.205\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.394\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.170\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.454\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.476\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.441\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.523\n",
            "\u001b[32m[01/11 03:34:32 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.253 | 71.488 | 23.995 | 0.002 | 20.464 | 39.380 |\n",
            "\u001b[32m[01/11 03:34:32 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.253 |\n",
            "\u001b[32m[01/11 03:34:32 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:34:32 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:34:32 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:34:32 d2.evaluation.testing]: \u001b[0mcopypaste: 32.2529,71.4876,23.9953,0.0017,20.4641,39.3798\n",
            "\u001b[32m[01/11 03:34:32 d2.utils.events]: \u001b[0m eta: 0:17:26  iter: 3639  total_loss: 0.5998  loss_cls: 0.148  loss_box_reg: 0.3859  loss_rpn_cls: 0.02258  loss_rpn_loc: 0.02257    time: 0.7718  last_time: 0.7737  data_time: 0.0328  last_data_time: 0.0261   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:34:40 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:34:40 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:34:40 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:34:40 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:34:40 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:34:40 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:34:40 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:34:40 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:34:41 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0477 s/iter. Eval: 0.0003 s/iter. Total: 0.0490 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:34:46 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0017 s/iter. Inference: 0.0507 s/iter. Eval: 0.0003 s/iter. Total: 0.0527 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:34:51 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0020 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:34:53 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.835673 (0.053482 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:34:53 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050540 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:34:54 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:34:54 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:34:54 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.323\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.709\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.240\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.203\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.395\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.455\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.477\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.435\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.528\n",
            "\u001b[32m[01/11 03:34:54 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.268 | 70.911 | 23.955 | 0.000 | 20.347 | 39.533 |\n",
            "\u001b[32m[01/11 03:34:54 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.268 |\n",
            "\u001b[32m[01/11 03:34:54 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:34:54 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:34:54 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:34:54 d2.evaluation.testing]: \u001b[0mcopypaste: 32.2678,70.9113,23.9554,0.0000,20.3468,39.5328\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:35:01 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:35:01 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:35:01 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:35:01 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:35:01 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:35:01 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:35:01 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:35:01 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:35:03 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0482 s/iter. Eval: 0.0003 s/iter. Total: 0.0493 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:35:08 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0016 s/iter. Inference: 0.0492 s/iter. Eval: 0.0002 s/iter. Total: 0.0511 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:35:13 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0015 s/iter. Inference: 0.0490 s/iter. Eval: 0.0002 s/iter. Total: 0.0509 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:35:15 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.358473 (0.051494 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:35:15 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048929 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:35:15 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:35:15 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:35:15 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.22s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.323\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.709\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.236\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.203\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.396\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.174\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.450\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.470\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.426\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.522\n",
            "\u001b[32m[01/11 03:35:15 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.276 | 70.855 | 23.642 | 0.006 | 20.328 | 39.597 |\n",
            "\u001b[32m[01/11 03:35:15 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.276 |\n",
            "\u001b[32m[01/11 03:35:15 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:35:15 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:35:15 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:35:15 d2.evaluation.testing]: \u001b[0mcopypaste: 32.2758,70.8547,23.6417,0.0056,20.3284,39.5966\n",
            "\u001b[32m[01/11 03:35:15 d2.utils.events]: \u001b[0m eta: 0:17:10  iter: 3659  total_loss: 0.5705  loss_cls: 0.13  loss_box_reg: 0.3816  loss_rpn_cls: 0.01951  loss_rpn_loc: 0.01554    time: 0.7718  last_time: 0.7687  data_time: 0.0327  last_data_time: 0.0258   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:35:24 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:35:24 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:35:24 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:35:24 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:35:24 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:35:24 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:35:24 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:35:24 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:35:25 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0530 s/iter. Eval: 0.0003 s/iter. Total: 0.0544 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:35:30 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0015 s/iter. Inference: 0.0513 s/iter. Eval: 0.0003 s/iter. Total: 0.0532 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:35:35 d2.evaluation.evaluator]: \u001b[0mInference done 201/245. Dataloading: 0.0017 s/iter. Inference: 0.0511 s/iter. Eval: 0.0003 s/iter. Total: 0.0532 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:35:37 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.895642 (0.053732 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:35:37 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050985 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:35:37 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:35:37 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:35:37 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.67s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.323\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.713\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.247\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.207\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.395\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.171\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.455\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.475\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.427\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.528\n",
            "\u001b[32m[01/11 03:35:38 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.310 | 71.319 | 24.688 | 0.008 | 20.661 | 39.460 |\n",
            "\u001b[32m[01/11 03:35:38 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.310 |\n",
            "\u001b[32m[01/11 03:35:38 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:35:38 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:35:38 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:35:38 d2.evaluation.testing]: \u001b[0mcopypaste: 32.3103,71.3187,24.6881,0.0084,20.6612,39.4597\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:35:45 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:35:45 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:35:45 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:35:45 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:35:45 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:35:45 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:35:45 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:35:45 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:35:47 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0014 s/iter. Inference: 0.0507 s/iter. Eval: 0.0003 s/iter. Total: 0.0523 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:35:52 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0018 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:35:57 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0017 s/iter. Inference: 0.0504 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:35:59 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.665501 (0.052773 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:35:59 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050149 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:35:59 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:35:59 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:35:59 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.334\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.731\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.253\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.212\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.409\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.176\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.464\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.490\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.024\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.435\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.548\n",
            "\u001b[32m[01/11 03:35:59 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.360 | 73.076 | 25.337 | 0.029 | 21.240 | 40.913 |\n",
            "\u001b[32m[01/11 03:35:59 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.360 |\n",
            "\u001b[32m[01/11 03:35:59 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:35:59 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:35:59 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:35:59 d2.evaluation.testing]: \u001b[0mcopypaste: 33.3597,73.0764,25.3371,0.0292,21.2400,40.9125\n",
            "\u001b[32m[01/11 03:35:59 d2.utils.events]: \u001b[0m eta: 0:16:55  iter: 3679  total_loss: 0.6363  loss_cls: 0.1597  loss_box_reg: 0.4156  loss_rpn_cls: 0.02065  loss_rpn_loc: 0.02669    time: 0.7718  last_time: 0.7788  data_time: 0.0360  last_data_time: 0.0331   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:36:07 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:36:07 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:36:07 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:36:07 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:36:07 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:36:07 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:36:07 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:36:08 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:36:09 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0014 s/iter. Inference: 0.0486 s/iter. Eval: 0.0002 s/iter. Total: 0.0502 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:36:14 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0019 s/iter. Inference: 0.0495 s/iter. Eval: 0.0002 s/iter. Total: 0.0518 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:36:19 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0018 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:36:21 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.544032 (0.052267 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:36:21 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049424 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:36:21 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:36:21 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:36:21 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.310\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.708\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.212\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.197\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.380\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.439\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.460\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.423\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.506\n",
            "\u001b[32m[01/11 03:36:21 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.954 | 70.798 | 21.248 | 0.003 | 19.739 | 37.964 |\n",
            "\u001b[32m[01/11 03:36:21 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.954 |\n",
            "\u001b[32m[01/11 03:36:21 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:36:21 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:36:21 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:36:21 d2.evaluation.testing]: \u001b[0mcopypaste: 30.9544,70.7978,21.2482,0.0034,19.7392,37.9639\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:36:29 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:36:29 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:36:29 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:36:29 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:36:29 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:36:29 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:36:29 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:36:29 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:36:30 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0028 s/iter. Inference: 0.0483 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:36:35 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0019 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:36:40 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0018 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:36:42 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.433145 (0.051805 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:36:42 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048941 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:36:42 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:36:42 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:36:42 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.326\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.716\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.231\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.207\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.400\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.174\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.456\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.475\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.423\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.532\n",
            "\u001b[32m[01/11 03:36:42 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.600 | 71.611 | 23.085 | 0.004 | 20.705 | 40.050 |\n",
            "\u001b[32m[01/11 03:36:42 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.600 |\n",
            "\u001b[32m[01/11 03:36:42 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:36:42 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:36:42 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:36:42 d2.evaluation.testing]: \u001b[0mcopypaste: 32.6002,71.6108,23.0849,0.0041,20.7050,40.0496\n",
            "\u001b[32m[01/11 03:36:42 d2.utils.events]: \u001b[0m eta: 0:16:40  iter: 3699  total_loss: 0.6033  loss_cls: 0.1543  loss_box_reg: 0.4024  loss_rpn_cls: 0.02  loss_rpn_loc: 0.01441    time: 0.7718  last_time: 0.7742  data_time: 0.0351  last_data_time: 0.0346   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:36:51 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:36:51 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:36:51 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:36:51 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:36:51 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:36:51 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:36:51 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:36:51 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:36:52 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0028 s/iter. Inference: 0.0479 s/iter. Eval: 0.0002 s/iter. Total: 0.0509 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:36:57 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0025 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:37:02 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0022 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:37:04 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.521754 (0.052174 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:37:04 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049009 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:37:04 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:37:04 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:37:04 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.320\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.711\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.223\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.205\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.392\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.454\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.481\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.439\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.532\n",
            "\u001b[32m[01/11 03:37:04 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.043 | 71.076 | 22.324 | 0.000 | 20.534 | 39.224 |\n",
            "\u001b[32m[01/11 03:37:04 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.043 |\n",
            "\u001b[32m[01/11 03:37:04 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:37:04 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:37:04 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:37:04 d2.evaluation.testing]: \u001b[0mcopypaste: 32.0427,71.0757,22.3241,0.0000,20.5341,39.2244\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:37:12 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:37:12 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:37:12 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:37:12 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:37:12 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:37:12 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:37:12 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:37:12 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:37:13 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0481 s/iter. Eval: 0.0002 s/iter. Total: 0.0494 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:37:18 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0025 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:37:23 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0021 s/iter. Inference: 0.0489 s/iter. Eval: 0.0002 s/iter. Total: 0.0513 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:37:25 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.472069 (0.051967 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:37:25 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048978 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:37:25 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:37:25 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:37:25 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.322\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.722\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.228\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.212\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.391\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.169\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.455\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.478\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.434\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.529\n",
            "\u001b[32m[01/11 03:37:25 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.178 | 72.223 | 22.825 | 0.011 | 21.190 | 39.070 |\n",
            "\u001b[32m[01/11 03:37:25 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.178 |\n",
            "\u001b[32m[01/11 03:37:25 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:37:25 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:37:25 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:37:25 d2.evaluation.testing]: \u001b[0mcopypaste: 32.1778,72.2229,22.8251,0.0106,21.1905,39.0700\n",
            "\u001b[32m[01/11 03:37:25 d2.utils.events]: \u001b[0m eta: 0:16:24  iter: 3719  total_loss: 0.6329  loss_cls: 0.1469  loss_box_reg: 0.4111  loss_rpn_cls: 0.02211  loss_rpn_loc: 0.01914    time: 0.7718  last_time: 0.7634  data_time: 0.0327  last_data_time: 0.0235   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:37:33 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:37:33 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:37:33 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:37:33 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:37:33 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:37:33 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:37:33 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:37:34 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:37:35 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0007 s/iter. Inference: 0.0528 s/iter. Eval: 0.0002 s/iter. Total: 0.0538 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:37:40 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0013 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:37:45 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0017 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:37:47 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.600157 (0.052501 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:37:47 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049796 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:37:47 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:37:47 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:37:47 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.329\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.716\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.253\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.214\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.402\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.176\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.463\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.484\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.432\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.542\n",
            "\u001b[32m[01/11 03:37:47 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.890 | 71.583 | 25.337 | 0.000 | 21.390 | 40.244 |\n",
            "\u001b[32m[01/11 03:37:47 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.890 |\n",
            "\u001b[32m[01/11 03:37:47 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:37:47 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:37:47 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:37:47 d2.evaluation.testing]: \u001b[0mcopypaste: 32.8899,71.5830,25.3373,0.0000,21.3902,40.2437\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:37:55 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:37:55 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:37:55 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:37:55 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:37:55 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:37:55 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:37:55 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:37:55 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:37:56 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0509 s/iter. Eval: 0.0002 s/iter. Total: 0.0521 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:38:01 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0014 s/iter. Inference: 0.0491 s/iter. Eval: 0.0002 s/iter. Total: 0.0509 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:38:06 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0014 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:38:08 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.612595 (0.052552 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:38:08 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050092 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:38:08 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:38:08 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:38:08 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.326\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.723\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.235\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.211\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.398\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.175\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.457\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.479\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.531\n",
            "\u001b[32m[01/11 03:38:09 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.569 | 72.317 | 23.472 | 0.000 | 21.098 | 39.763 |\n",
            "\u001b[32m[01/11 03:38:09 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.569 |\n",
            "\u001b[32m[01/11 03:38:09 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:38:09 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:38:09 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:38:09 d2.evaluation.testing]: \u001b[0mcopypaste: 32.5687,72.3167,23.4717,0.0000,21.0980,39.7626\n",
            "\u001b[32m[01/11 03:38:09 d2.utils.events]: \u001b[0m eta: 0:16:09  iter: 3739  total_loss: 0.6116  loss_cls: 0.1531  loss_box_reg: 0.4157  loss_rpn_cls: 0.01936  loss_rpn_loc: 0.01404    time: 0.7717  last_time: 0.6943  data_time: 0.0315  last_data_time: 0.0221   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:38:17 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:38:17 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:38:17 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:38:17 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:38:17 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:38:17 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:38:17 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:38:17 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:38:18 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0463 s/iter. Eval: 0.0002 s/iter. Total: 0.0476 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:38:23 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0014 s/iter. Inference: 0.0487 s/iter. Eval: 0.0003 s/iter. Total: 0.0505 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:38:28 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0016 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0508 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:38:30 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.335713 (0.051399 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:38:30 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048761 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:38:30 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:38:30 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:38:30 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.720\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.205\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.390\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.174\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.453\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.476\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.439\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.524\n",
            "\u001b[32m[01/11 03:38:30 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.864 | 72.001 | 21.596 | 0.000 | 20.521 | 38.994 |\n",
            "\u001b[32m[01/11 03:38:30 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.864 |\n",
            "\u001b[32m[01/11 03:38:30 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:38:30 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:38:30 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:38:30 d2.evaluation.testing]: \u001b[0mcopypaste: 31.8638,72.0011,21.5958,0.0000,20.5206,38.9942\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:38:37 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:38:37 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:38:37 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:38:37 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:38:37 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:38:37 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:38:37 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:38:37 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:38:39 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0478 s/iter. Eval: 0.0002 s/iter. Total: 0.0490 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:38:44 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0014 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:38:49 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0014 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:38:51 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.467000 (0.051946 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:38:51 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049548 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:38:51 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:38:51 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:38:51 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.41s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.710\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.211\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.384\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.175\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.453\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.480\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.451\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.522\n",
            "\u001b[32m[01/11 03:38:51 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.709 | 71.025 | 21.676 | 0.014 | 21.060 | 38.428 |\n",
            "\u001b[32m[01/11 03:38:51 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.709 |\n",
            "\u001b[32m[01/11 03:38:51 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:38:51 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:38:51 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:38:51 d2.evaluation.testing]: \u001b[0mcopypaste: 31.7092,71.0246,21.6756,0.0136,21.0596,38.4281\n",
            "\u001b[32m[01/11 03:38:51 d2.utils.events]: \u001b[0m eta: 0:15:53  iter: 3759  total_loss: 0.6243  loss_cls: 0.1576  loss_box_reg: 0.4148  loss_rpn_cls: 0.02001  loss_rpn_loc: 0.01782    time: 0.7716  last_time: 0.6586  data_time: 0.0338  last_data_time: 0.0174   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:39:00 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:39:00 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:39:00 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:39:00 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:39:00 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:39:00 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:39:00 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:39:00 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:39:01 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0478 s/iter. Eval: 0.0002 s/iter. Total: 0.0489 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:39:06 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0015 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:39:11 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0020 s/iter. Inference: 0.0493 s/iter. Eval: 0.0002 s/iter. Total: 0.0516 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:39:13 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.602870 (0.052512 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:39:13 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049610 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:39:13 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:39:13 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:39:13 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.306\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.703\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.201\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.200\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.370\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.171\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.439\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.460\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.437\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.498\n",
            "\u001b[32m[01/11 03:39:13 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.573 | 70.291 | 20.079 | 0.006 | 20.047 | 36.971 |\n",
            "\u001b[32m[01/11 03:39:13 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.573 |\n",
            "\u001b[32m[01/11 03:39:13 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:39:13 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:39:13 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:39:13 d2.evaluation.testing]: \u001b[0mcopypaste: 30.5735,70.2911,20.0785,0.0056,20.0466,36.9706\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:39:21 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:39:21 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:39:21 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:39:21 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:39:21 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:39:21 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:39:21 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:39:21 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:39:22 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0488 s/iter. Eval: 0.0003 s/iter. Total: 0.0502 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:39:27 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0018 s/iter. Inference: 0.0488 s/iter. Eval: 0.0003 s/iter. Total: 0.0509 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:39:32 d2.evaluation.evaluator]: \u001b[0mInference done 209/245. Dataloading: 0.0016 s/iter. Inference: 0.0489 s/iter. Eval: 0.0002 s/iter. Total: 0.0508 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:39:34 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.378945 (0.051579 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:39:34 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048918 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:39:34 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:39:34 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:39:34 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.320\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.714\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.233\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.211\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.388\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.177\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.457\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.479\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.446\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.524\n",
            "\u001b[32m[01/11 03:39:34 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.016 | 71.435 | 23.305 | 0.002 | 21.091 | 38.806 |\n",
            "\u001b[32m[01/11 03:39:34 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.016 |\n",
            "\u001b[32m[01/11 03:39:35 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:39:35 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:39:35 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:39:35 d2.evaluation.testing]: \u001b[0mcopypaste: 32.0159,71.4348,23.3046,0.0023,21.0913,38.8060\n",
            "\u001b[32m[01/11 03:39:35 d2.utils.events]: \u001b[0m eta: 0:15:38  iter: 3779  total_loss: 0.6575  loss_cls: 0.1492  loss_box_reg: 0.4264  loss_rpn_cls: 0.03211  loss_rpn_loc: 0.03251    time: 0.7716  last_time: 0.7746  data_time: 0.0326  last_data_time: 0.0298   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:39:43 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:39:43 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:39:43 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:39:43 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:39:43 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:39:43 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:39:43 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:39:43 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:39:44 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0485 s/iter. Eval: 0.0003 s/iter. Total: 0.0500 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:39:49 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0018 s/iter. Inference: 0.0490 s/iter. Eval: 0.0002 s/iter. Total: 0.0512 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:39:54 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0019 s/iter. Inference: 0.0488 s/iter. Eval: 0.0002 s/iter. Total: 0.0510 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:39:56 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.336825 (0.051403 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:39:56 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048676 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:39:56 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:39:56 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:39:56 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.316\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.709\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.225\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.200\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.385\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.451\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.472\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.440\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.517\n",
            "\u001b[32m[01/11 03:39:56 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.627 | 70.911 | 22.500 | 0.002 | 20.005 | 38.543 |\n",
            "\u001b[32m[01/11 03:39:56 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.627 |\n",
            "\u001b[32m[01/11 03:39:56 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:39:56 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:39:56 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:39:56 d2.evaluation.testing]: \u001b[0mcopypaste: 31.6274,70.9108,22.5000,0.0025,20.0055,38.5428\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:40:04 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:40:04 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:40:04 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:40:04 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:40:04 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:40:04 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:40:04 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:40:04 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:40:05 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0482 s/iter. Eval: 0.0002 s/iter. Total: 0.0494 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:40:10 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0019 s/iter. Inference: 0.0484 s/iter. Eval: 0.0003 s/iter. Total: 0.0506 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:40:15 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0020 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:40:17 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.465394 (0.051939 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:40:17 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048829 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:40:17 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:40:17 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:40:17 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.329\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.721\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.226\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.203\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.405\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.176\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.463\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.483\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.426\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.544\n",
            "\u001b[32m[01/11 03:40:17 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.905 | 72.118 | 22.643 | 0.003 | 20.275 | 40.507 |\n",
            "\u001b[32m[01/11 03:40:17 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.905 |\n",
            "\u001b[32m[01/11 03:40:17 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:40:17 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:40:17 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:40:17 d2.evaluation.testing]: \u001b[0mcopypaste: 32.9051,72.1183,22.6428,0.0029,20.2750,40.5068\n",
            "\u001b[32m[01/11 03:40:17 d2.utils.events]: \u001b[0m eta: 0:15:23  iter: 3799  total_loss: 0.573  loss_cls: 0.1313  loss_box_reg: 0.3972  loss_rpn_cls: 0.0182  loss_rpn_loc: 0.01103    time: 0.7716  last_time: 0.7658  data_time: 0.0343  last_data_time: 0.0247   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:40:26 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:40:26 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:40:26 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:40:26 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:40:26 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:40:26 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:40:26 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:40:26 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:40:27 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0043 s/iter. Inference: 0.0487 s/iter. Eval: 0.0002 s/iter. Total: 0.0532 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:40:32 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0022 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:40:37 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0021 s/iter. Inference: 0.0492 s/iter. Eval: 0.0002 s/iter. Total: 0.0517 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:40:39 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.525017 (0.052188 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:40:39 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049193 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:40:39 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:40:39 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:40:39 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.303\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.696\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.187\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.201\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.368\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.435\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.456\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.429\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.497\n",
            "\u001b[32m[01/11 03:40:39 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.286 | 69.558 | 18.706 | 0.002 | 20.098 | 36.813 |\n",
            "\u001b[32m[01/11 03:40:39 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.286 |\n",
            "\u001b[32m[01/11 03:40:39 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:40:39 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:40:39 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:40:39 d2.evaluation.testing]: \u001b[0mcopypaste: 30.2861,69.5584,18.7064,0.0019,20.0982,36.8130\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:40:47 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:40:47 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:40:47 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:40:47 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:40:47 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:40:47 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:40:47 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:40:47 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:40:48 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:40:53 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0016 s/iter. Inference: 0.0507 s/iter. Eval: 0.0003 s/iter. Total: 0.0526 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:40:58 d2.evaluation.evaluator]: \u001b[0mInference done 202/245. Dataloading: 0.0020 s/iter. Inference: 0.0500 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:41:00 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.739105 (0.053080 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:41:00 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050040 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:41:00 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:41:00 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:41:00 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.332\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.717\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.243\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.210\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.410\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.176\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.467\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.493\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.437\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.553\n",
            "\u001b[32m[01/11 03:41:01 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.243 | 71.706 | 24.338 | 0.009 | 21.003 | 41.047 |\n",
            "\u001b[32m[01/11 03:41:01 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.243 |\n",
            "\u001b[32m[01/11 03:41:01 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:41:01 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:41:01 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:41:01 d2.evaluation.testing]: \u001b[0mcopypaste: 33.2432,71.7061,24.3378,0.0085,21.0034,41.0467\n",
            "\u001b[32m[01/11 03:41:01 d2.utils.events]: \u001b[0m eta: 0:15:07  iter: 3819  total_loss: 0.5854  loss_cls: 0.1476  loss_box_reg: 0.3577  loss_rpn_cls: 0.02393  loss_rpn_loc: 0.02547    time: 0.7716  last_time: 0.6549  data_time: 0.0309  last_data_time: 0.0215   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:41:09 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:41:09 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:41:09 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:41:09 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:41:09 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:41:09 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:41:09 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:41:09 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:41:10 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0013 s/iter. Inference: 0.0482 s/iter. Eval: 0.0002 s/iter. Total: 0.0497 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:41:15 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0014 s/iter. Inference: 0.0490 s/iter. Eval: 0.0002 s/iter. Total: 0.0507 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:41:20 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0017 s/iter. Inference: 0.0497 s/iter. Eval: 0.0002 s/iter. Total: 0.0518 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:41:22 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.580657 (0.052419 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:41:22 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049758 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:41:22 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:41:22 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:41:22 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.21s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.321\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.711\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.220\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.207\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.393\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.170\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.451\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.473\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.430\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.524\n",
            "\u001b[32m[01/11 03:41:22 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.064 | 71.071 | 21.991 | 0.000 | 20.689 | 39.251 |\n",
            "\u001b[32m[01/11 03:41:22 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.064 |\n",
            "\u001b[32m[01/11 03:41:22 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:41:22 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:41:22 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:41:22 d2.evaluation.testing]: \u001b[0mcopypaste: 32.0636,71.0708,21.9905,0.0000,20.6890,39.2512\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:41:30 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:41:30 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:41:30 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:41:30 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:41:30 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:41:30 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:41:30 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:41:30 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:41:31 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0034 s/iter. Inference: 0.0486 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:41:36 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0018 s/iter. Inference: 0.0496 s/iter. Eval: 0.0002 s/iter. Total: 0.0517 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:41:41 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0017 s/iter. Inference: 0.0493 s/iter. Eval: 0.0002 s/iter. Total: 0.0513 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:41:43 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.420269 (0.051751 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:41:43 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049063 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:41:43 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:41:43 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:41:43 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.20s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.326\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.716\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.242\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.202\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.400\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.452\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.474\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.425\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.528\n",
            "\u001b[32m[01/11 03:41:44 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.557 | 71.591 | 24.165 | 0.000 | 20.189 | 39.988 |\n",
            "\u001b[32m[01/11 03:41:44 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.557 |\n",
            "\u001b[32m[01/11 03:41:44 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:41:44 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:41:44 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:41:44 d2.evaluation.testing]: \u001b[0mcopypaste: 32.5567,71.5906,24.1648,0.0000,20.1892,39.9880\n",
            "\u001b[32m[01/11 03:41:44 d2.utils.events]: \u001b[0m eta: 0:14:52  iter: 3839  total_loss: 0.5694  loss_cls: 0.1275  loss_box_reg: 0.3915  loss_rpn_cls: 0.01248  loss_rpn_loc: 0.01382    time: 0.7715  last_time: 0.7643  data_time: 0.0315  last_data_time: 0.0252   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:41:52 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:41:52 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:41:52 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:41:52 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:41:52 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:41:52 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:41:52 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:41:52 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:41:53 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0487 s/iter. Eval: 0.0003 s/iter. Total: 0.0498 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:41:58 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0016 s/iter. Inference: 0.0488 s/iter. Eval: 0.0003 s/iter. Total: 0.0507 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:42:03 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0016 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:42:06 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.901560 (0.053757 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:42:06 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.051166 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:42:06 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:42:06 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:42:06 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.308\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.691\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.212\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.199\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.376\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.441\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.464\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.434\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.507\n",
            "\u001b[32m[01/11 03:42:06 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.849 | 69.101 | 21.248 | 0.000 | 19.943 | 37.610 |\n",
            "\u001b[32m[01/11 03:42:06 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.849 |\n",
            "\u001b[32m[01/11 03:42:06 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:42:06 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:42:06 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:42:06 d2.evaluation.testing]: \u001b[0mcopypaste: 30.8494,69.1009,21.2484,0.0000,19.9430,37.6097\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:42:14 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:42:14 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:42:14 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:42:14 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:42:14 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:42:14 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:42:14 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:42:14 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:42:15 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0044 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0547 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:42:20 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0022 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:42:25 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0021 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:42:27 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.633986 (0.052642 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:42:27 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049623 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:42:27 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:42:27 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:42:27 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.327\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.705\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.246\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.206\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.401\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.175\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.461\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.484\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.444\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.533\n",
            "\u001b[32m[01/11 03:42:28 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.665 | 70.545 | 24.576 | 0.002 | 20.573 | 40.128 |\n",
            "\u001b[32m[01/11 03:42:28 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.665 |\n",
            "\u001b[32m[01/11 03:42:28 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:42:28 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:42:28 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:42:28 d2.evaluation.testing]: \u001b[0mcopypaste: 32.6650,70.5453,24.5764,0.0024,20.5731,40.1278\n",
            "\u001b[32m[01/11 03:42:28 d2.utils.events]: \u001b[0m eta: 0:14:36  iter: 3859  total_loss: 0.6123  loss_cls: 0.158  loss_box_reg: 0.4206  loss_rpn_cls: 0.01375  loss_rpn_loc: 0.01424    time: 0.7716  last_time: 0.7760  data_time: 0.0364  last_data_time: 0.0301   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:42:36 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:42:36 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:42:36 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:42:36 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:42:36 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:42:36 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:42:36 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:42:36 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:42:37 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0454 s/iter. Eval: 0.0002 s/iter. Total: 0.0467 s/iter. ETA=0:00:10\n",
            "\u001b[32m[01/11 03:42:42 d2.evaluation.evaluator]: \u001b[0mInference done 111/245. Dataloading: 0.0014 s/iter. Inference: 0.0484 s/iter. Eval: 0.0003 s/iter. Total: 0.0502 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:42:47 d2.evaluation.evaluator]: \u001b[0mInference done 211/245. Dataloading: 0.0015 s/iter. Inference: 0.0485 s/iter. Eval: 0.0003 s/iter. Total: 0.0503 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:42:49 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.265748 (0.051107 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:42:49 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048594 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:42:49 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:42:49 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:42:49 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.327\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.713\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.238\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.214\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.398\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.174\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.460\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.483\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.533\n",
            "\u001b[32m[01/11 03:42:49 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.716 | 71.274 | 23.845 | 0.000 | 21.397 | 39.817 |\n",
            "\u001b[32m[01/11 03:42:49 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.716 |\n",
            "\u001b[32m[01/11 03:42:49 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:42:49 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:42:49 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:42:49 d2.evaluation.testing]: \u001b[0mcopypaste: 32.7163,71.2740,23.8447,0.0000,21.3975,39.8167\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:42:57 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:42:57 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:42:57 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:42:57 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:42:57 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:42:57 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:42:57 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:42:57 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:42:58 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0483 s/iter. Eval: 0.0003 s/iter. Total: 0.0495 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:43:03 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0017 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:43:08 d2.evaluation.evaluator]: \u001b[0mInference done 209/245. Dataloading: 0.0015 s/iter. Inference: 0.0488 s/iter. Eval: 0.0003 s/iter. Total: 0.0506 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:43:10 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.292254 (0.051218 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:43:10 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048723 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:43:10 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:43:10 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:43:10 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.329\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.708\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.243\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.218\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.402\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.174\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.456\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.485\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.448\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.533\n",
            "\u001b[32m[01/11 03:43:10 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.945 | 70.784 | 24.321 | 0.000 | 21.848 | 40.188 |\n",
            "\u001b[32m[01/11 03:43:10 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.945 |\n",
            "\u001b[32m[01/11 03:43:10 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:43:10 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:43:10 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:43:10 d2.evaluation.testing]: \u001b[0mcopypaste: 32.9454,70.7843,24.3205,0.0000,21.8476,40.1877\n",
            "\u001b[32m[01/11 03:43:10 d2.utils.events]: \u001b[0m eta: 0:14:21  iter: 3879  total_loss: 0.6471  loss_cls: 0.1654  loss_box_reg: 0.4344  loss_rpn_cls: 0.02011  loss_rpn_loc: 0.01707    time: 0.7715  last_time: 0.7065  data_time: 0.0340  last_data_time: 0.0316   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:43:18 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:43:18 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:43:18 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:43:18 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:43:18 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:43:18 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:43:18 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:43:18 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:43:19 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0008 s/iter. Inference: 0.0504 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:43:24 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0018 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:43:29 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0019 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:43:31 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.520699 (0.052170 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:43:31 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049278 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:43:31 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:43:31 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:43:31 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.345\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.730\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.271\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.222\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.425\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.182\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.474\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.497\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.432\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.564\n",
            "\u001b[32m[01/11 03:43:32 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 34.492 | 73.029 | 27.149 | 0.000 | 22.167 | 42.486 |\n",
            "\u001b[32m[01/11 03:43:32 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 34.492 |\n",
            "\u001b[32m[01/11 03:43:32 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:43:32 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:43:32 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:43:32 d2.evaluation.testing]: \u001b[0mcopypaste: 34.4921,73.0286,27.1490,0.0000,22.1675,42.4863\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:43:39 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:43:39 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:43:39 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:43:39 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:43:39 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:43:39 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:43:39 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:43:39 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:43:40 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0471 s/iter. Eval: 0.0002 s/iter. Total: 0.0485 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:43:45 d2.evaluation.evaluator]: \u001b[0mInference done 105/245. Dataloading: 0.0017 s/iter. Inference: 0.0510 s/iter. Eval: 0.0003 s/iter. Total: 0.0530 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:43:50 d2.evaluation.evaluator]: \u001b[0mInference done 201/245. Dataloading: 0.0015 s/iter. Inference: 0.0509 s/iter. Eval: 0.0003 s/iter. Total: 0.0528 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:43:53 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.925476 (0.053856 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:43:53 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.051342 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:43:53 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:43:53 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:43:53 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.310\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.704\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.201\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.379\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.171\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.438\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.464\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.506\n",
            "\u001b[32m[01/11 03:43:53 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.977 | 70.360 | 21.617 | 0.000 | 20.110 | 37.893 |\n",
            "\u001b[32m[01/11 03:43:53 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.977 |\n",
            "\u001b[32m[01/11 03:43:53 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:43:53 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:43:53 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:43:53 d2.evaluation.testing]: \u001b[0mcopypaste: 30.9766,70.3599,21.6172,0.0000,20.1095,37.8930\n",
            "\u001b[32m[01/11 03:43:53 d2.utils.events]: \u001b[0m eta: 0:14:05  iter: 3899  total_loss: 0.6156  loss_cls: 0.161  loss_box_reg: 0.4032  loss_rpn_cls: 0.01719  loss_rpn_loc: 0.01942    time: 0.7715  last_time: 0.6970  data_time: 0.0333  last_data_time: 0.0222   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:44:02 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:44:02 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:44:02 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:44:02 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:44:02 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:44:02 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:44:02 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:44:02 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:44:03 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0486 s/iter. Eval: 0.0003 s/iter. Total: 0.0498 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:44:08 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0014 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:44:13 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0015 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:44:15 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.594771 (0.052478 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:44:15 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049843 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:44:15 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:44:15 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:44:15 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.307\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.702\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.199\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.194\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.376\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.170\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.462\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.439\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.501\n",
            "\u001b[32m[01/11 03:44:15 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.717 | 70.177 | 19.881 | 0.000 | 19.437 | 37.627 |\n",
            "\u001b[32m[01/11 03:44:15 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.717 |\n",
            "\u001b[32m[01/11 03:44:15 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:44:15 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:44:15 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:44:15 d2.evaluation.testing]: \u001b[0mcopypaste: 30.7168,70.1768,19.8813,0.0000,19.4367,37.6273\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:44:23 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:44:23 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:44:23 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:44:23 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:44:23 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:44:23 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:44:23 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:44:23 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:44:24 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:44:29 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0015 s/iter. Inference: 0.0499 s/iter. Eval: 0.0002 s/iter. Total: 0.0517 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:44:34 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0016 s/iter. Inference: 0.0495 s/iter. Eval: 0.0002 s/iter. Total: 0.0514 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:44:36 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.557237 (0.052322 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:44:36 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049744 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:44:36 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:44:36 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:44:36 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.334\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.722\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.242\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.213\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.410\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.175\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.459\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.484\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.439\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.537\n",
            "\u001b[32m[01/11 03:44:37 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.403 | 72.183 | 24.181 | 0.004 | 21.317 | 40.981 |\n",
            "\u001b[32m[01/11 03:44:37 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.403 |\n",
            "\u001b[32m[01/11 03:44:37 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:44:37 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:44:37 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:44:37 d2.evaluation.testing]: \u001b[0mcopypaste: 33.4030,72.1826,24.1806,0.0036,21.3174,40.9815\n",
            "\u001b[32m[01/11 03:44:37 d2.utils.events]: \u001b[0m eta: 0:13:50  iter: 3919  total_loss: 0.6386  loss_cls: 0.1555  loss_box_reg: 0.4271  loss_rpn_cls: 0.01967  loss_rpn_loc: 0.02089    time: 0.7715  last_time: 0.7713  data_time: 0.0332  last_data_time: 0.0314   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:44:45 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:44:45 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:44:45 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:44:45 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:44:45 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:44:45 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:44:45 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:44:45 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:44:46 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0503 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:44:51 d2.evaluation.evaluator]: \u001b[0mInference done 111/245. Dataloading: 0.0018 s/iter. Inference: 0.0483 s/iter. Eval: 0.0003 s/iter. Total: 0.0504 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:44:56 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0020 s/iter. Inference: 0.0488 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:44:58 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.415637 (0.051732 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:44:58 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048709 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:44:58 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:44:58 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:44:58 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.46s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.330\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.723\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.235\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.214\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.405\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.175\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.456\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.484\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.438\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.537\n",
            "\u001b[32m[01/11 03:44:59 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.996 | 72.270 | 23.545 | 0.004 | 21.408 | 40.458 |\n",
            "\u001b[32m[01/11 03:44:59 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.996 |\n",
            "\u001b[32m[01/11 03:44:59 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:44:59 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:44:59 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:44:59 d2.evaluation.testing]: \u001b[0mcopypaste: 32.9964,72.2698,23.5452,0.0040,21.4079,40.4581\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:45:06 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:45:06 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:45:06 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:45:06 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:45:06 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:45:06 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:45:06 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:45:06 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:45:07 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0483 s/iter. Eval: 0.0002 s/iter. Total: 0.0496 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:45:13 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0019 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:45:18 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0017 s/iter. Inference: 0.0500 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:45:20 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.713909 (0.052975 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:45:20 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050234 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:45:20 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:45:20 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:45:20 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.330\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.717\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.239\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.209\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.406\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.459\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.480\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.425\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.539\n",
            "\u001b[32m[01/11 03:45:20 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.964 | 71.714 | 23.936 | 0.000 | 20.883 | 40.575 |\n",
            "\u001b[32m[01/11 03:45:20 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.964 |\n",
            "\u001b[32m[01/11 03:45:20 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:45:20 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:45:20 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:45:20 d2.evaluation.testing]: \u001b[0mcopypaste: 32.9638,71.7137,23.9358,0.0000,20.8833,40.5751\n",
            "\u001b[32m[01/11 03:45:20 d2.utils.events]: \u001b[0m eta: 0:13:35  iter: 3939  total_loss: 0.5766  loss_cls: 0.1348  loss_box_reg: 0.3916  loss_rpn_cls: 0.01767  loss_rpn_loc: 0.01425    time: 0.7714  last_time: 0.7867  data_time: 0.0357  last_data_time: 0.0391   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:45:29 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:45:29 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:45:29 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:45:29 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:45:29 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:45:29 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:45:29 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:45:29 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:45:30 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0031 s/iter. Inference: 0.0494 s/iter. Eval: 0.0002 s/iter. Total: 0.0527 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:45:35 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0023 s/iter. Inference: 0.0497 s/iter. Eval: 0.0002 s/iter. Total: 0.0523 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:45:40 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0019 s/iter. Inference: 0.0496 s/iter. Eval: 0.0002 s/iter. Total: 0.0518 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:45:42 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.530587 (0.052211 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:45:42 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049386 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:45:42 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:45:42 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:45:42 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.324\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.713\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.230\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.205\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.398\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.174\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.458\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.480\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.532\n",
            "\u001b[32m[01/11 03:45:42 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.427 | 71.321 | 22.980 | 0.000 | 20.461 | 39.803 |\n",
            "\u001b[32m[01/11 03:45:42 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.427 |\n",
            "\u001b[32m[01/11 03:45:42 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:45:42 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:45:42 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:45:42 d2.evaluation.testing]: \u001b[0mcopypaste: 32.4270,71.3214,22.9797,0.0000,20.4606,39.8032\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:45:50 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:45:50 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:45:50 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:45:50 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:45:50 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:45:50 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:45:50 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:45:50 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:45:51 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0485 s/iter. Eval: 0.0003 s/iter. Total: 0.0500 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:45:56 d2.evaluation.evaluator]: \u001b[0mInference done 105/245. Dataloading: 0.0023 s/iter. Inference: 0.0507 s/iter. Eval: 0.0003 s/iter. Total: 0.0534 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:46:01 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0021 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0525 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:46:04 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.807955 (0.053366 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:46:04 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050201 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:46:04 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:46:04 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:46:04 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.321\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.721\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.224\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.206\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.394\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.169\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.454\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.477\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.438\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.526\n",
            "\u001b[32m[01/11 03:46:04 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.144 | 72.058 | 22.410 | 0.004 | 20.630 | 39.355 |\n",
            "\u001b[32m[01/11 03:46:04 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.144 |\n",
            "\u001b[32m[01/11 03:46:04 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:46:04 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:46:04 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:46:04 d2.evaluation.testing]: \u001b[0mcopypaste: 32.1436,72.0583,22.4102,0.0041,20.6303,39.3548\n",
            "\u001b[32m[01/11 03:46:04 d2.utils.events]: \u001b[0m eta: 0:13:19  iter: 3959  total_loss: 0.6161  loss_cls: 0.1447  loss_box_reg: 0.4112  loss_rpn_cls: 0.0168  loss_rpn_loc: 0.01651    time: 0.7715  last_time: 0.7665  data_time: 0.0308  last_data_time: 0.0226   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:46:12 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:46:12 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:46:12 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:46:12 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:46:12 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:46:12 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:46:12 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:46:12 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:46:13 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0485 s/iter. Eval: 0.0002 s/iter. Total: 0.0497 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:46:18 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0015 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:46:23 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0018 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:46:26 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.485545 (0.052023 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:46:26 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049122 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:46:26 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:46:26 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:46:26 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.332\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.723\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.261\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.212\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.406\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.174\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.459\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.479\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.424\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.539\n",
            "\u001b[32m[01/11 03:46:26 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.179 | 72.345 | 26.054 | 0.000 | 21.227 | 40.599 |\n",
            "\u001b[32m[01/11 03:46:26 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.179 |\n",
            "\u001b[32m[01/11 03:46:26 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:46:26 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:46:26 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:46:26 d2.evaluation.testing]: \u001b[0mcopypaste: 33.1787,72.3449,26.0541,0.0000,21.2267,40.5990\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:46:34 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:46:34 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:46:34 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:46:34 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:46:34 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:46:34 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:46:34 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:46:34 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:46:35 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0507 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:46:40 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0021 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:46:45 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0020 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:46:47 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.572413 (0.052385 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:46:47 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049430 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:46:47 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:46:47 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:46:47 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.331\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.716\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.243\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.215\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.403\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.458\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.479\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.433\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.533\n",
            "\u001b[32m[01/11 03:46:47 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.060 | 71.626 | 24.349 | 0.000 | 21.515 | 40.307 |\n",
            "\u001b[32m[01/11 03:46:47 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.060 |\n",
            "\u001b[32m[01/11 03:46:47 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:46:47 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:46:47 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:46:47 d2.evaluation.testing]: \u001b[0mcopypaste: 33.0598,71.6263,24.3494,0.0000,21.5146,40.3066\n",
            "\u001b[32m[01/11 03:46:47 d2.utils.events]: \u001b[0m eta: 0:13:04  iter: 3979  total_loss: 0.6069  loss_cls: 0.145  loss_box_reg: 0.4152  loss_rpn_cls: 0.01529  loss_rpn_loc: 0.01489    time: 0.7715  last_time: 0.7917  data_time: 0.0326  last_data_time: 0.0352   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:46:56 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:46:56 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:46:56 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:46:56 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:46:56 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:46:56 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:46:56 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:46:56 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:46:57 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0484 s/iter. Eval: 0.0002 s/iter. Total: 0.0499 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:47:02 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0021 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:47:07 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0019 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:47:09 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.613130 (0.052555 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:47:09 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049685 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:47:09 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:47:09 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:47:09 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.311\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.716\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.205\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.205\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.377\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.167\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.472\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.453\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.508\n",
            "\u001b[32m[01/11 03:47:10 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.138 | 71.642 | 20.534 | 0.002 | 20.503 | 37.729 |\n",
            "\u001b[32m[01/11 03:47:10 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.138 |\n",
            "\u001b[32m[01/11 03:47:10 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:47:10 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:47:10 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:47:10 d2.evaluation.testing]: \u001b[0mcopypaste: 31.1382,71.6417,20.5341,0.0024,20.5027,37.7285\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:47:17 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:47:17 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:47:17 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:47:17 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:47:17 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:47:17 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:47:17 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:47:17 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:47:18 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0017 s/iter. Inference: 0.0487 s/iter. Eval: 0.0003 s/iter. Total: 0.0506 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:47:23 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0018 s/iter. Inference: 0.0508 s/iter. Eval: 0.0003 s/iter. Total: 0.0529 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:47:28 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0017 s/iter. Inference: 0.0502 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:47:31 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.707775 (0.052949 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:47:31 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050253 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:47:31 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:47:31 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:47:31 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.309\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.716\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.210\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.206\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.373\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.166\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.440\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.468\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.450\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.503\n",
            "\u001b[32m[01/11 03:47:31 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.925 | 71.557 | 21.013 | 0.003 | 20.633 | 37.347 |\n",
            "\u001b[32m[01/11 03:47:31 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.925 |\n",
            "\u001b[32m[01/11 03:47:31 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:47:31 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:47:31 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:47:31 d2.evaluation.testing]: \u001b[0mcopypaste: 30.9253,71.5572,21.0129,0.0026,20.6334,37.3465\n",
            "\u001b[32m[01/11 03:47:31 d2.utils.events]: \u001b[0m eta: 0:12:49  iter: 3999  total_loss: 0.6215  loss_cls: 0.1509  loss_box_reg: 0.4065  loss_rpn_cls: 0.02042  loss_rpn_loc: 0.02    time: 0.7715  last_time: 0.7739  data_time: 0.0311  last_data_time: 0.0294   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:47:39 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:47:39 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:47:39 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:47:39 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:47:39 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:47:39 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:47:39 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:47:39 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:47:40 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0482 s/iter. Eval: 0.0003 s/iter. Total: 0.0495 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:47:45 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0017 s/iter. Inference: 0.0506 s/iter. Eval: 0.0003 s/iter. Total: 0.0526 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:47:50 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0018 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:47:53 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.626460 (0.052610 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:47:53 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049787 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:47:53 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:47:53 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:47:53 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.714\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.219\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.215\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.384\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.170\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.452\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.475\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.518\n",
            "\u001b[32m[01/11 03:47:53 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.903 | 71.359 | 21.929 | 0.003 | 21.467 | 38.450 |\n",
            "\u001b[32m[01/11 03:47:53 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.903 |\n",
            "\u001b[32m[01/11 03:47:53 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:47:53 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:47:53 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:47:53 d2.evaluation.testing]: \u001b[0mcopypaste: 31.9031,71.3586,21.9290,0.0032,21.4667,38.4499\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:48:00 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:48:00 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:48:00 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:48:00 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:48:00 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:48:00 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:48:00 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:48:00 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:48:01 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0519 s/iter. Eval: 0.0002 s/iter. Total: 0.0533 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:48:06 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0017 s/iter. Inference: 0.0490 s/iter. Eval: 0.0002 s/iter. Total: 0.0511 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:48:11 d2.evaluation.evaluator]: \u001b[0mInference done 200/245. Dataloading: 0.0018 s/iter. Inference: 0.0511 s/iter. Eval: 0.0002 s/iter. Total: 0.0533 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:48:14 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.826489 (0.053444 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:48:14 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050681 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:48:14 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:48:14 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:48:14 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.308\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.711\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.186\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.209\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.372\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.167\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.461\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.497\n",
            "\u001b[32m[01/11 03:48:14 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.763 | 71.072 | 18.599 | 0.001 | 20.888 | 37.170 |\n",
            "\u001b[32m[01/11 03:48:14 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.763 |\n",
            "\u001b[32m[01/11 03:48:14 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:48:14 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:48:14 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:48:14 d2.evaluation.testing]: \u001b[0mcopypaste: 30.7634,71.0724,18.5995,0.0012,20.8882,37.1699\n",
            "\u001b[32m[01/11 03:48:14 d2.utils.events]: \u001b[0m eta: 0:12:33  iter: 4019  total_loss: 0.5831  loss_cls: 0.1506  loss_box_reg: 0.3935  loss_rpn_cls: 0.02074  loss_rpn_loc: 0.02125    time: 0.7714  last_time: 0.7057  data_time: 0.0347  last_data_time: 0.0313   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:48:23 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:48:23 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:48:23 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:48:23 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:48:23 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:48:23 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:48:23 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:48:23 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:48:24 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0527 s/iter. Eval: 0.0002 s/iter. Total: 0.0540 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:48:29 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0017 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:48:34 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0020 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:48:36 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.749955 (0.053125 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:48:36 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050155 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:48:36 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:48:36 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:48:36 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.334\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.728\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.248\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.216\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.408\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.176\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.466\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.494\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.455\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.544\n",
            "\u001b[32m[01/11 03:48:36 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.423 | 72.830 | 24.828 | 0.008 | 21.609 | 40.798 |\n",
            "\u001b[32m[01/11 03:48:36 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.423 |\n",
            "\u001b[32m[01/11 03:48:36 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:48:36 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:48:36 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:48:36 d2.evaluation.testing]: \u001b[0mcopypaste: 33.4226,72.8304,24.8276,0.0076,21.6090,40.7977\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:48:44 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:48:44 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:48:44 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:48:44 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:48:44 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:48:44 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:48:44 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:48:44 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:48:45 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0484 s/iter. Eval: 0.0002 s/iter. Total: 0.0495 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:48:50 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0018 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:48:55 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0019 s/iter. Inference: 0.0491 s/iter. Eval: 0.0002 s/iter. Total: 0.0513 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:48:57 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.489266 (0.052039 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:48:57 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049100 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:48:57 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:48:57 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:48:57 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.21s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.320\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.717\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.226\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.209\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.389\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.174\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.452\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.474\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.522\n",
            "\u001b[32m[01/11 03:48:58 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.969 | 71.678 | 22.633 | 0.000 | 20.859 | 38.861 |\n",
            "\u001b[32m[01/11 03:48:58 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.969 |\n",
            "\u001b[32m[01/11 03:48:58 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:48:58 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:48:58 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:48:58 d2.evaluation.testing]: \u001b[0mcopypaste: 31.9694,71.6783,22.6332,0.0000,20.8591,38.8606\n",
            "\u001b[32m[01/11 03:48:58 d2.utils.events]: \u001b[0m eta: 0:12:18  iter: 4039  total_loss: 0.6476  loss_cls: 0.1494  loss_box_reg: 0.4173  loss_rpn_cls: 0.01555  loss_rpn_loc: 0.01606    time: 0.7715  last_time: 0.7701  data_time: 0.0324  last_data_time: 0.0269   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:49:06 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:49:06 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:49:06 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:49:06 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:49:06 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:49:06 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:49:06 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:49:06 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:49:07 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0542 s/iter. Eval: 0.0002 s/iter. Total: 0.0553 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:49:12 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0017 s/iter. Inference: 0.0502 s/iter. Eval: 0.0003 s/iter. Total: 0.0523 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:49:17 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0017 s/iter. Inference: 0.0499 s/iter. Eval: 0.0002 s/iter. Total: 0.0519 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:49:19 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.574116 (0.052392 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:49:19 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049687 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:49:19 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:49:19 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:49:19 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.21s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.321\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.713\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.220\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.206\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.391\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.177\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.452\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.470\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.423\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.524\n",
            "\u001b[32m[01/11 03:49:19 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.052 | 71.289 | 22.041 | 0.000 | 20.633 | 39.111 |\n",
            "\u001b[32m[01/11 03:49:19 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.052 |\n",
            "\u001b[32m[01/11 03:49:19 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:49:19 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:49:19 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:49:19 d2.evaluation.testing]: \u001b[0mcopypaste: 32.0518,71.2890,22.0407,0.0000,20.6325,39.1114\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:49:27 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:49:27 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:49:27 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:49:27 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:49:27 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:49:27 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:49:27 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:49:27 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:49:28 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0017 s/iter. Inference: 0.0490 s/iter. Eval: 0.0002 s/iter. Total: 0.0509 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:49:33 d2.evaluation.evaluator]: \u001b[0mInference done 112/245. Dataloading: 0.0016 s/iter. Inference: 0.0479 s/iter. Eval: 0.0003 s/iter. Total: 0.0498 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:49:38 d2.evaluation.evaluator]: \u001b[0mInference done 209/245. Dataloading: 0.0016 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0509 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:49:40 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.352348 (0.051468 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:49:40 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048821 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:49:40 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:49:40 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:49:40 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.316\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.706\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.205\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.207\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.386\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.176\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.447\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.469\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.426\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.520\n",
            "\u001b[32m[01/11 03:49:40 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.560 | 70.576 | 20.480 | 0.000 | 20.661 | 38.637 |\n",
            "\u001b[32m[01/11 03:49:40 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.560 |\n",
            "\u001b[32m[01/11 03:49:40 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:49:40 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:49:40 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:49:40 d2.evaluation.testing]: \u001b[0mcopypaste: 31.5600,70.5755,20.4798,0.0000,20.6606,38.6371\n",
            "\u001b[32m[01/11 03:49:40 d2.utils.events]: \u001b[0m eta: 0:12:03  iter: 4059  total_loss: 0.6282  loss_cls: 0.1481  loss_box_reg: 0.4232  loss_rpn_cls: 0.02158  loss_rpn_loc: 0.01935    time: 0.7714  last_time: 0.7698  data_time: 0.0326  last_data_time: 0.0230   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:49:49 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:49:49 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:49:49 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:49:49 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:49:49 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:49:49 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:49:49 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:49:49 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:49:50 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0041 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0546 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:49:55 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0016 s/iter. Inference: 0.0504 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:50:00 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0015 s/iter. Inference: 0.0498 s/iter. Eval: 0.0002 s/iter. Total: 0.0517 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:50:02 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.481716 (0.052007 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:50:02 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049605 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:50:02 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:50:02 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:50:02 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.21s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.321\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.711\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.209\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.393\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.456\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.474\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.426\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.528\n",
            "\u001b[32m[01/11 03:50:02 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.143 | 71.083 | 21.664 | 0.000 | 20.853 | 39.310 |\n",
            "\u001b[32m[01/11 03:50:02 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.143 |\n",
            "\u001b[32m[01/11 03:50:02 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:50:02 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:50:02 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:50:02 d2.evaluation.testing]: \u001b[0mcopypaste: 32.1431,71.0830,21.6642,0.0000,20.8528,39.3103\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:50:10 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:50:10 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:50:10 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:50:10 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:50:10 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:50:10 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:50:10 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:50:10 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:50:11 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0541 s/iter. Eval: 0.0003 s/iter. Total: 0.0553 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:50:16 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0019 s/iter. Inference: 0.0504 s/iter. Eval: 0.0003 s/iter. Total: 0.0526 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:50:21 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0018 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:50:23 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.512463 (0.052135 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:50:23 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049272 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:50:23 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:50:23 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:50:23 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.316\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.706\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.212\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.205\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.389\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.451\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.473\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.423\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.528\n",
            "\u001b[32m[01/11 03:50:23 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.640 | 70.604 | 21.155 | 0.003 | 20.480 | 38.934 |\n",
            "\u001b[32m[01/11 03:50:23 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.640 |\n",
            "\u001b[32m[01/11 03:50:23 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:50:23 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:50:23 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:50:23 d2.evaluation.testing]: \u001b[0mcopypaste: 31.6405,70.6042,21.1551,0.0026,20.4796,38.9342\n",
            "\u001b[32m[01/11 03:50:23 d2.utils.events]: \u001b[0m eta: 0:11:47  iter: 4079  total_loss: 0.6621  loss_cls: 0.1704  loss_box_reg: 0.4232  loss_rpn_cls: 0.01828  loss_rpn_loc: 0.01728    time: 0.7714  last_time: 0.7916  data_time: 0.0318  last_data_time: 0.0299   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:50:32 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:50:32 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:50:32 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:50:32 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:50:32 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:50:32 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:50:32 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:50:32 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:50:33 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0507 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:50:38 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0018 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:50:43 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0017 s/iter. Inference: 0.0502 s/iter. Eval: 0.0003 s/iter. Total: 0.0523 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:50:45 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.727019 (0.053029 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:50:45 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050326 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:50:45 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:50:45 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:50:45 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.323\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.708\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.231\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.210\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.394\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.174\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.458\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.480\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.432\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.535\n",
            "\u001b[32m[01/11 03:50:45 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.252 | 70.820 | 23.119 | 0.002 | 20.991 | 39.425 |\n",
            "\u001b[32m[01/11 03:50:45 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.252 |\n",
            "\u001b[32m[01/11 03:50:45 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:50:45 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:50:45 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:50:45 d2.evaluation.testing]: \u001b[0mcopypaste: 32.2516,70.8199,23.1191,0.0020,20.9911,39.4251\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:50:53 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:50:53 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:50:53 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:50:53 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:50:53 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:50:53 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:50:53 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:50:53 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:50:54 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0527 s/iter. Eval: 0.0003 s/iter. Total: 0.0541 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:50:59 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0017 s/iter. Inference: 0.0508 s/iter. Eval: 0.0003 s/iter. Total: 0.0528 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:51:04 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0017 s/iter. Inference: 0.0504 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:51:07 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.795473 (0.053314 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:51:07 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050575 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:51:07 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:51:07 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:51:07 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.332\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.724\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.252\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.210\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.409\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.174\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.464\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.482\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.418\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.548\n",
            "\u001b[32m[01/11 03:51:07 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.199 | 72.370 | 25.183 | 0.000 | 21.006 | 40.927 |\n",
            "\u001b[32m[01/11 03:51:07 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.199 |\n",
            "\u001b[32m[01/11 03:51:07 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:51:07 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:51:07 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:51:07 d2.evaluation.testing]: \u001b[0mcopypaste: 33.1988,72.3699,25.1827,0.0000,21.0058,40.9268\n",
            "\u001b[32m[01/11 03:51:07 d2.utils.events]: \u001b[0m eta: 0:11:32  iter: 4099  total_loss: 0.5869  loss_cls: 0.1482  loss_box_reg: 0.389  loss_rpn_cls: 0.01991  loss_rpn_loc: 0.02238    time: 0.7714  last_time: 0.7675  data_time: 0.0324  last_data_time: 0.0279   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:51:15 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:51:15 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:51:15 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:51:15 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:51:15 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:51:15 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:51:15 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:51:15 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:51:17 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0508 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:51:22 d2.evaluation.evaluator]: \u001b[0mInference done 100/245. Dataloading: 0.0014 s/iter. Inference: 0.0547 s/iter. Eval: 0.0003 s/iter. Total: 0.0564 s/iter. ETA=0:00:08\n",
            "\u001b[32m[01/11 03:51:27 d2.evaluation.evaluator]: \u001b[0mInference done 197/245. Dataloading: 0.0017 s/iter. Inference: 0.0521 s/iter. Eval: 0.0003 s/iter. Total: 0.0542 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:51:29 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.987869 (0.054116 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:51:29 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.051411 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:51:29 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:51:29 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:51:29 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.313\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.705\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.218\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.198\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.385\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.167\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.446\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.474\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.442\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.518\n",
            "\u001b[32m[01/11 03:51:30 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.345 | 70.518 | 21.769 | 0.007 | 19.848 | 38.526 |\n",
            "\u001b[32m[01/11 03:51:30 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.345 |\n",
            "\u001b[32m[01/11 03:51:30 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:51:30 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:51:30 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:51:30 d2.evaluation.testing]: \u001b[0mcopypaste: 31.3447,70.5175,21.7691,0.0074,19.8483,38.5260\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:51:37 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:51:37 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:51:37 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:51:37 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:51:37 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:51:37 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:51:37 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:51:37 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:51:38 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0036 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0531 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:51:43 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0019 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:51:48 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0018 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:51:50 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.471109 (0.051963 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:51:50 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049080 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:51:50 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:51:50 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:51:50 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.335\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.716\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.255\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.212\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.413\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.174\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.467\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.494\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.438\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.554\n",
            "\u001b[32m[01/11 03:51:51 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.471 | 71.596 | 25.467 | 0.002 | 21.189 | 41.287 |\n",
            "\u001b[32m[01/11 03:51:51 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.471 |\n",
            "\u001b[32m[01/11 03:51:51 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:51:51 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:51:51 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:51:51 d2.evaluation.testing]: \u001b[0mcopypaste: 33.4712,71.5961,25.4672,0.0023,21.1892,41.2865\n",
            "\u001b[32m[01/11 03:51:51 d2.utils.events]: \u001b[0m eta: 0:11:16  iter: 4119  total_loss: 0.6099  loss_cls: 0.1493  loss_box_reg: 0.413  loss_rpn_cls: 0.0183  loss_rpn_loc: 0.01403    time: 0.7714  last_time: 0.7587  data_time: 0.0345  last_data_time: 0.0220   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:51:59 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:51:59 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:51:59 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:51:59 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:51:59 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:51:59 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:51:59 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:51:59 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:52:00 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0473 s/iter. Eval: 0.0002 s/iter. Total: 0.0486 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:52:05 d2.evaluation.evaluator]: \u001b[0mInference done 111/245. Dataloading: 0.0015 s/iter. Inference: 0.0481 s/iter. Eval: 0.0003 s/iter. Total: 0.0500 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:52:10 d2.evaluation.evaluator]: \u001b[0mInference done 210/245. Dataloading: 0.0015 s/iter. Inference: 0.0485 s/iter. Eval: 0.0003 s/iter. Total: 0.0503 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:52:12 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.249878 (0.051041 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:52:12 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048501 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:52:12 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:52:12 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:52:12 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.326\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.716\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.248\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.203\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.401\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.175\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.456\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.483\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.438\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.536\n",
            "\u001b[32m[01/11 03:52:12 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.611 | 71.634 | 24.809 | 0.000 | 20.305 | 40.081 |\n",
            "\u001b[32m[01/11 03:52:12 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.611 |\n",
            "\u001b[32m[01/11 03:52:12 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:52:12 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:52:12 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:52:12 d2.evaluation.testing]: \u001b[0mcopypaste: 32.6108,71.6340,24.8094,0.0000,20.3051,40.0806\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:52:20 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:52:20 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:52:20 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:52:20 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:52:20 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:52:20 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:52:20 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:52:20 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:52:21 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0537 s/iter. Eval: 0.0003 s/iter. Total: 0.0549 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:52:26 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0014 s/iter. Inference: 0.0506 s/iter. Eval: 0.0003 s/iter. Total: 0.0523 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:52:31 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0015 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:52:33 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.555080 (0.052313 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:52:33 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049739 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:52:33 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:52:33 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:52:33 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.332\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.719\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.254\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.211\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.407\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.176\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.464\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.489\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.435\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.546\n",
            "\u001b[32m[01/11 03:52:34 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.164 | 71.938 | 25.373 | 0.004 | 21.071 | 40.746 |\n",
            "\u001b[32m[01/11 03:52:34 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.164 |\n",
            "\u001b[32m[01/11 03:52:34 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:52:34 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:52:34 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:52:34 d2.evaluation.testing]: \u001b[0mcopypaste: 33.1645,71.9379,25.3732,0.0043,21.0713,40.7456\n",
            "\u001b[32m[01/11 03:52:34 d2.utils.events]: \u001b[0m eta: 0:11:01  iter: 4139  total_loss: 0.5789  loss_cls: 0.1354  loss_box_reg: 0.382  loss_rpn_cls: 0.01588  loss_rpn_loc: 0.01618    time: 0.7714  last_time: 0.7682  data_time: 0.0360  last_data_time: 0.0271   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:52:42 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:52:42 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:52:42 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:52:42 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:52:42 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:52:42 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:52:42 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:52:42 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:52:43 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0051 s/iter. Inference: 0.0482 s/iter. Eval: 0.0002 s/iter. Total: 0.0535 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:52:48 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0018 s/iter. Inference: 0.0492 s/iter. Eval: 0.0002 s/iter. Total: 0.0513 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:52:53 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0017 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:52:55 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.424430 (0.051768 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:52:55 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048907 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:52:55 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:52:55 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:52:55 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.325\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.716\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.224\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.199\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.403\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.454\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.480\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.434\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.533\n",
            "\u001b[32m[01/11 03:52:55 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.526 | 71.574 | 22.375 | 0.003 | 19.940 | 40.313 |\n",
            "\u001b[32m[01/11 03:52:55 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.526 |\n",
            "\u001b[32m[01/11 03:52:55 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:52:55 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:52:55 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:52:55 d2.evaluation.testing]: \u001b[0mcopypaste: 32.5257,71.5741,22.3747,0.0034,19.9401,40.3131\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:53:03 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:53:03 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:53:03 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:53:03 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:53:03 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:53:03 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:53:03 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:53:03 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:53:04 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0007 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0502 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:53:09 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0022 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:53:14 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0019 s/iter. Inference: 0.0489 s/iter. Eval: 0.0002 s/iter. Total: 0.0511 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:53:16 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.433860 (0.051808 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:53:16 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048847 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:53:16 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:53:16 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:53:16 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.313\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.718\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.209\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.199\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.383\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.169\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.442\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.468\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.442\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.509\n",
            "\u001b[32m[01/11 03:53:16 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.339 | 71.825 | 20.907 | 0.001 | 19.901 | 38.292 |\n",
            "\u001b[32m[01/11 03:53:16 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.339 |\n",
            "\u001b[32m[01/11 03:53:16 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:53:16 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:53:16 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:53:16 d2.evaluation.testing]: \u001b[0mcopypaste: 31.3393,71.8252,20.9073,0.0013,19.9012,38.2924\n",
            "\u001b[32m[01/11 03:53:16 d2.utils.events]: \u001b[0m eta: 0:10:45  iter: 4159  total_loss: 0.5685  loss_cls: 0.1434  loss_box_reg: 0.3797  loss_rpn_cls: 0.01732  loss_rpn_loc: 0.019    time: 0.7714  last_time: 0.6936  data_time: 0.0323  last_data_time: 0.0228   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:53:25 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:53:25 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:53:25 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:53:25 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:53:25 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:53:25 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:53:25 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:53:25 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:53:26 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0023 s/iter. Inference: 0.0479 s/iter. Eval: 0.0002 s/iter. Total: 0.0504 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:53:31 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0023 s/iter. Inference: 0.0494 s/iter. Eval: 0.0002 s/iter. Total: 0.0520 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:53:36 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0022 s/iter. Inference: 0.0492 s/iter. Eval: 0.0002 s/iter. Total: 0.0517 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:53:38 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.635604 (0.052648 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:53:38 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049450 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:53:38 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:53:38 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:53:38 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.21s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.339\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.727\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.265\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.214\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.417\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.176\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.467\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.489\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.429\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.552\n",
            "\u001b[32m[01/11 03:53:38 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.942 | 72.725 | 26.508 | 0.000 | 21.419 | 41.742 |\n",
            "\u001b[32m[01/11 03:53:38 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.942 |\n",
            "\u001b[32m[01/11 03:53:38 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:53:38 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:53:38 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:53:38 d2.evaluation.testing]: \u001b[0mcopypaste: 33.9424,72.7251,26.5076,0.0000,21.4194,41.7423\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:53:46 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:53:46 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:53:46 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:53:46 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:53:46 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:53:46 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:53:46 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:53:46 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:53:47 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0489 s/iter. Eval: 0.0002 s/iter. Total: 0.0502 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:53:52 d2.evaluation.evaluator]: \u001b[0mInference done 111/245. Dataloading: 0.0016 s/iter. Inference: 0.0485 s/iter. Eval: 0.0002 s/iter. Total: 0.0504 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:53:57 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0017 s/iter. Inference: 0.0490 s/iter. Eval: 0.0002 s/iter. Total: 0.0510 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:53:59 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.357440 (0.051489 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:53:59 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048787 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:53:59 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:53:59 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:53:59 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.316\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.711\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.223\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.202\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.386\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.444\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.470\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.441\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.512\n",
            "\u001b[32m[01/11 03:53:59 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.628 | 71.077 | 22.343 | 0.005 | 20.178 | 38.630 |\n",
            "\u001b[32m[01/11 03:53:59 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.628 |\n",
            "\u001b[32m[01/11 03:53:59 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:53:59 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:53:59 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:53:59 d2.evaluation.testing]: \u001b[0mcopypaste: 31.6278,71.0766,22.3430,0.0046,20.1783,38.6301\n",
            "\u001b[32m[01/11 03:53:59 d2.utils.events]: \u001b[0m eta: 0:10:30  iter: 4179  total_loss: 0.5796  loss_cls: 0.14  loss_box_reg: 0.3989  loss_rpn_cls: 0.01548  loss_rpn_loc: 0.01785    time: 0.7713  last_time: 0.6917  data_time: 0.0321  last_data_time: 0.0219   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:54:08 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:54:08 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:54:08 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:54:08 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:54:08 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:54:08 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:54:08 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:54:08 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:54:09 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0016 s/iter. Inference: 0.0486 s/iter. Eval: 0.0002 s/iter. Total: 0.0505 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:54:14 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0021 s/iter. Inference: 0.0493 s/iter. Eval: 0.0002 s/iter. Total: 0.0518 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:54:19 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0018 s/iter. Inference: 0.0501 s/iter. Eval: 0.0002 s/iter. Total: 0.0522 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:54:21 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.614664 (0.052561 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:54:21 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049825 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:54:21 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:54:21 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:54:21 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.45s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.21s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.320\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.714\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.225\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.204\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.391\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.448\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.468\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.428\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.518\n",
            "\u001b[32m[01/11 03:54:22 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.961 | 71.393 | 22.460 | 0.000 | 20.434 | 39.097 |\n",
            "\u001b[32m[01/11 03:54:22 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.961 |\n",
            "\u001b[32m[01/11 03:54:22 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:54:22 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:54:22 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:54:22 d2.evaluation.testing]: \u001b[0mcopypaste: 31.9607,71.3930,22.4604,0.0000,20.4335,39.0967\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:54:29 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:54:29 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:54:29 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:54:29 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:54:29 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:54:29 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:54:29 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:54:29 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:54:30 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0512 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:54:35 d2.evaluation.evaluator]: \u001b[0mInference done 105/245. Dataloading: 0.0017 s/iter. Inference: 0.0516 s/iter. Eval: 0.0003 s/iter. Total: 0.0536 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:54:40 d2.evaluation.evaluator]: \u001b[0mInference done 201/245. Dataloading: 0.0016 s/iter. Inference: 0.0510 s/iter. Eval: 0.0003 s/iter. Total: 0.0529 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:54:43 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.751529 (0.053131 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:54:43 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050495 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:54:43 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:54:43 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:54:43 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.22s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.332\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.723\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.242\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.218\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.406\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.177\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.461\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.486\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.441\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.540\n",
            "\u001b[32m[01/11 03:54:43 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.231 | 72.291 | 24.204 | 0.000 | 21.830 | 40.560 |\n",
            "\u001b[32m[01/11 03:54:43 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.231 |\n",
            "\u001b[32m[01/11 03:54:43 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:54:43 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:54:43 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:54:43 d2.evaluation.testing]: \u001b[0mcopypaste: 33.2307,72.2906,24.2038,0.0000,21.8300,40.5598\n",
            "\u001b[32m[01/11 03:54:43 d2.utils.events]: \u001b[0m eta: 0:10:14  iter: 4199  total_loss: 0.6022  loss_cls: 0.1425  loss_box_reg: 0.3998  loss_rpn_cls: 0.01722  loss_rpn_loc: 0.01749    time: 0.7713  last_time: 0.7772  data_time: 0.0320  last_data_time: 0.0312   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:54:51 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:54:51 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:54:51 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:54:51 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:54:51 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:54:51 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:54:51 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:54:51 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:54:52 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0508 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:54:57 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0014 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:55:02 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0016 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:55:04 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.554079 (0.052309 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:55:04 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049594 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:55:05 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:55:05 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:55:05 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.323\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.712\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.204\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.396\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.174\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.453\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.480\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.434\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.533\n",
            "\u001b[32m[01/11 03:55:05 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.309 | 71.169 | 21.733 | 0.002 | 20.379 | 39.620 |\n",
            "\u001b[32m[01/11 03:55:05 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.309 |\n",
            "\u001b[32m[01/11 03:55:05 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:55:05 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:55:05 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:55:05 d2.evaluation.testing]: \u001b[0mcopypaste: 32.3091,71.1689,21.7328,0.0022,20.3791,39.6197\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:55:13 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:55:13 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:55:13 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:55:13 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:55:13 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:55:13 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:55:13 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:55:13 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:55:14 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0034 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0537 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:55:19 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0019 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:55:24 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0021 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:55:26 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.561190 (0.052338 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:55:26 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049339 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:55:26 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:55:26 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:55:26 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.325\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.715\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.233\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.213\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.396\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.175\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.455\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.481\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.438\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.532\n",
            "\u001b[32m[01/11 03:55:26 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.511 | 71.484 | 23.266 | 0.003 | 21.266 | 39.566 |\n",
            "\u001b[32m[01/11 03:55:26 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.511 |\n",
            "\u001b[32m[01/11 03:55:26 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:55:26 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:55:26 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:55:26 d2.evaluation.testing]: \u001b[0mcopypaste: 32.5112,71.4844,23.2656,0.0033,21.2656,39.5662\n",
            "\u001b[32m[01/11 03:55:26 d2.utils.events]: \u001b[0m eta: 0:09:59  iter: 4219  total_loss: 0.5737  loss_cls: 0.1552  loss_box_reg: 0.4  loss_rpn_cls: 0.0168  loss_rpn_loc: 0.01384    time: 0.7713  last_time: 0.7663  data_time: 0.0331  last_data_time: 0.0254   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:55:35 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:55:35 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:55:35 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:55:35 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:55:35 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:55:35 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:55:35 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:55:35 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:55:36 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0044 s/iter. Inference: 0.0486 s/iter. Eval: 0.0002 s/iter. Total: 0.0532 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:55:41 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0025 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:55:46 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0025 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:55:48 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.533242 (0.052222 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:55:48 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048852 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:55:48 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:55:48 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:55:48 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.334\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.723\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.251\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.212\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.410\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.176\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.463\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.483\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.425\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.543\n",
            "\u001b[32m[01/11 03:55:48 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.434 | 72.298 | 25.115 | 0.002 | 21.187 | 40.984 |\n",
            "\u001b[32m[01/11 03:55:48 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.434 |\n",
            "\u001b[32m[01/11 03:55:48 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:55:48 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:55:48 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:55:48 d2.evaluation.testing]: \u001b[0mcopypaste: 33.4338,72.2980,25.1155,0.0019,21.1871,40.9837\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:55:56 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:55:56 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:55:56 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:55:56 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:55:56 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:55:56 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:55:56 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:55:56 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:55:57 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0538 s/iter. Eval: 0.0003 s/iter. Total: 0.0550 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:56:02 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0015 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:56:07 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0017 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:56:09 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.599353 (0.052497 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:56:09 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049784 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:56:09 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:56:09 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:56:09 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.29s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.313\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.705\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.214\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.202\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.382\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.441\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.467\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.438\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.509\n",
            "\u001b[32m[01/11 03:56:10 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.258 | 70.512 | 21.448 | 0.003 | 20.196 | 38.152 |\n",
            "\u001b[32m[01/11 03:56:10 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.258 |\n",
            "\u001b[32m[01/11 03:56:10 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:56:10 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:56:10 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:56:10 d2.evaluation.testing]: \u001b[0mcopypaste: 31.2577,70.5125,21.4481,0.0033,20.1956,38.1523\n",
            "\u001b[32m[01/11 03:56:10 d2.utils.events]: \u001b[0m eta: 0:09:44  iter: 4239  total_loss: 0.6252  loss_cls: 0.1601  loss_box_reg: 0.4044  loss_rpn_cls: 0.01759  loss_rpn_loc: 0.02042    time: 0.7713  last_time: 0.7789  data_time: 0.0327  last_data_time: 0.0239   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:56:18 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:56:18 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:56:18 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:56:18 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:56:18 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:56:18 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:56:18 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:56:18 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:56:19 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0034 s/iter. Inference: 0.0488 s/iter. Eval: 0.0002 s/iter. Total: 0.0524 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:56:24 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0018 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:56:29 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0017 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:56:31 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.525942 (0.052191 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:56:31 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049418 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:56:31 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:56:31 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:56:31 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.323\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.719\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.226\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.206\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.399\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.171\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.453\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.480\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.429\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.536\n",
            "\u001b[32m[01/11 03:56:31 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.325 | 71.868 | 22.598 | 0.000 | 20.555 | 39.855 |\n",
            "\u001b[32m[01/11 03:56:31 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.325 |\n",
            "\u001b[32m[01/11 03:56:31 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:56:31 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:56:31 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:56:31 d2.evaluation.testing]: \u001b[0mcopypaste: 32.3247,71.8684,22.5982,0.0000,20.5548,39.8549\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:56:39 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:56:39 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:56:39 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:56:39 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:56:39 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:56:39 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:56:39 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:56:39 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:56:40 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0033 s/iter. Inference: 0.0490 s/iter. Eval: 0.0002 s/iter. Total: 0.0525 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:56:45 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0022 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:56:50 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0021 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:56:52 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.551569 (0.052298 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:56:52 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049396 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:56:52 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:56:52 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:56:52 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.313\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.709\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.210\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.203\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.381\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.167\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.448\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.468\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.513\n",
            "\u001b[32m[01/11 03:56:52 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.349 | 70.930 | 21.007 | 0.007 | 20.255 | 38.064 |\n",
            "\u001b[32m[01/11 03:56:52 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.349 |\n",
            "\u001b[32m[01/11 03:56:52 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:56:52 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:56:52 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:56:52 d2.evaluation.testing]: \u001b[0mcopypaste: 31.3489,70.9298,21.0074,0.0074,20.2550,38.0644\n",
            "\u001b[32m[01/11 03:56:52 d2.utils.events]: \u001b[0m eta: 0:09:28  iter: 4259  total_loss: 0.6135  loss_cls: 0.1601  loss_box_reg: 0.4005  loss_rpn_cls: 0.02259  loss_rpn_loc: 0.02334    time: 0.7712  last_time: 0.7696  data_time: 0.0341  last_data_time: 0.0246   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:57:01 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:57:01 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:57:01 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:57:01 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:57:01 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:57:01 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:57:01 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:57:01 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:57:02 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0480 s/iter. Eval: 0.0002 s/iter. Total: 0.0492 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:57:07 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0021 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:57:12 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0017 s/iter. Inference: 0.0490 s/iter. Eval: 0.0002 s/iter. Total: 0.0510 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:57:14 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.442870 (0.051845 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:57:14 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048987 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:57:14 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:57:14 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:57:14 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.708\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.223\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.203\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.393\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.170\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.453\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.475\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.425\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.531\n",
            "\u001b[32m[01/11 03:57:14 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.858 | 70.777 | 22.307 | 0.003 | 20.293 | 39.280 |\n",
            "\u001b[32m[01/11 03:57:14 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.858 |\n",
            "\u001b[32m[01/11 03:57:14 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:57:14 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:57:14 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:57:14 d2.evaluation.testing]: \u001b[0mcopypaste: 31.8576,70.7767,22.3073,0.0026,20.2930,39.2796\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:57:22 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:57:22 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:57:22 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:57:22 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:57:22 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:57:22 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:57:22 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:57:22 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:57:23 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0021 s/iter. Inference: 0.0609 s/iter. Eval: 0.0003 s/iter. Total: 0.0633 s/iter. ETA=0:00:14\n",
            "\u001b[32m[01/11 03:57:28 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0017 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:57:33 d2.evaluation.evaluator]: \u001b[0mInference done 209/245. Dataloading: 0.0016 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:57:35 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.442509 (0.051844 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:57:35 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049211 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:57:35 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:57:35 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:57:35 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.65s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.331\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.720\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.243\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.217\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.405\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.174\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.464\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.490\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.545\n",
            "\u001b[32m[01/11 03:57:36 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.063 | 72.025 | 24.324 | 0.000 | 21.713 | 40.492 |\n",
            "\u001b[32m[01/11 03:57:36 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.063 |\n",
            "\u001b[32m[01/11 03:57:36 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:57:36 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:57:36 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:57:36 d2.evaluation.testing]: \u001b[0mcopypaste: 33.0631,72.0247,24.3237,0.0000,21.7132,40.4918\n",
            "\u001b[32m[01/11 03:57:36 d2.utils.events]: \u001b[0m eta: 0:09:13  iter: 4279  total_loss: 0.6076  loss_cls: 0.1493  loss_box_reg: 0.4049  loss_rpn_cls: 0.01435  loss_rpn_loc: 0.01484    time: 0.7712  last_time: 0.7787  data_time: 0.0342  last_data_time: 0.0350   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:57:44 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:57:44 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:57:44 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:57:44 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:57:44 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:57:44 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:57:44 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:57:44 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:57:45 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0494 s/iter. Eval: 0.0002 s/iter. Total: 0.0506 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:57:50 d2.evaluation.evaluator]: \u001b[0mInference done 111/245. Dataloading: 0.0015 s/iter. Inference: 0.0487 s/iter. Eval: 0.0003 s/iter. Total: 0.0505 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:57:55 d2.evaluation.evaluator]: \u001b[0mInference done 209/245. Dataloading: 0.0017 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0509 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:57:57 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.418591 (0.051744 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:57:57 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049132 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:57:57 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:57:57 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:57:57 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.321\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.718\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.209\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.393\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.448\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.473\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.433\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.522\n",
            "\u001b[32m[01/11 03:57:58 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.091 | 71.780 | 21.660 | 0.009 | 20.890 | 39.267 |\n",
            "\u001b[32m[01/11 03:57:58 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.091 |\n",
            "\u001b[32m[01/11 03:57:58 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:57:58 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:57:58 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:57:58 d2.evaluation.testing]: \u001b[0mcopypaste: 32.0913,71.7797,21.6599,0.0088,20.8898,39.2672\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:58:05 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:58:05 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:58:05 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:58:05 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:58:05 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:58:05 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:58:05 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:58:05 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:58:06 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:58:12 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0019 s/iter. Inference: 0.0507 s/iter. Eval: 0.0003 s/iter. Total: 0.0530 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:58:17 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0019 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0523 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:58:19 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.669653 (0.052790 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:58:19 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049865 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:58:19 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:58:19 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:58:19 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.340\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.722\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.258\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.216\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.418\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.178\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.463\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.490\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.434\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.551\n",
            "\u001b[32m[01/11 03:58:19 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.973 | 72.181 | 25.792 | 0.000 | 21.628 | 41.756 |\n",
            "\u001b[32m[01/11 03:58:19 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.973 |\n",
            "\u001b[32m[01/11 03:58:19 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:58:19 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:58:19 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:58:19 d2.evaluation.testing]: \u001b[0mcopypaste: 33.9728,72.1805,25.7922,0.0000,21.6281,41.7558\n",
            "\u001b[32m[01/11 03:58:19 d2.utils.events]: \u001b[0m eta: 0:08:58  iter: 4299  total_loss: 0.5844  loss_cls: 0.1435  loss_box_reg: 0.407  loss_rpn_cls: 0.01397  loss_rpn_loc: 0.0141    time: 0.7712  last_time: 0.7894  data_time: 0.0329  last_data_time: 0.0374   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:58:28 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:58:28 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:58:28 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:58:28 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:58:28 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:58:28 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:58:28 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:58:28 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:58:29 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0508 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:58:34 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0017 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:58:39 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0019 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:58:41 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.564600 (0.052353 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:58:41 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049323 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:58:41 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:58:41 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:58:41 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.325\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.710\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.242\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.204\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.400\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.176\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.456\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.479\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.434\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.531\n",
            "\u001b[32m[01/11 03:58:41 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.544 | 70.999 | 24.213 | 0.001 | 20.405 | 39.956 |\n",
            "\u001b[32m[01/11 03:58:41 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.544 |\n",
            "\u001b[32m[01/11 03:58:41 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:58:41 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:58:41 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:58:41 d2.evaluation.testing]: \u001b[0mcopypaste: 32.5443,70.9993,24.2131,0.0014,20.4048,39.9559\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:58:49 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:58:49 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:58:49 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:58:49 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:58:49 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:58:49 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:58:49 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:58:49 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:58:50 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0047 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0539 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 03:58:55 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0017 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0509 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:59:00 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0018 s/iter. Inference: 0.0494 s/iter. Eval: 0.0002 s/iter. Total: 0.0515 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:59:02 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.531013 (0.052213 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:59:02 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049273 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:59:02 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:59:02 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:59:02 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.22s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.324\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.709\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.232\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.206\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.398\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.175\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.450\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.469\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.421\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.523\n",
            "\u001b[32m[01/11 03:59:03 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.421 | 70.950 | 23.227 | 0.000 | 20.614 | 39.755 |\n",
            "\u001b[32m[01/11 03:59:03 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.421 |\n",
            "\u001b[32m[01/11 03:59:03 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:59:03 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:59:03 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:59:03 d2.evaluation.testing]: \u001b[0mcopypaste: 32.4211,70.9500,23.2274,0.0000,20.6139,39.7551\n",
            "\u001b[32m[01/11 03:59:03 d2.utils.events]: \u001b[0m eta: 0:08:42  iter: 4319  total_loss: 0.6064  loss_cls: 0.1459  loss_box_reg: 0.4128  loss_rpn_cls: 0.0139  loss_rpn_loc: 0.01655    time: 0.7712  last_time: 0.7803  data_time: 0.0312  last_data_time: 0.0407   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:59:11 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:59:11 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:59:11 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:59:11 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:59:11 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:59:11 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:59:11 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:59:11 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:59:12 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:59:17 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0017 s/iter. Inference: 0.0488 s/iter. Eval: 0.0003 s/iter. Total: 0.0509 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 03:59:22 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0017 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 03:59:24 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.376669 (0.051569 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:59:24 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048837 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:59:24 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:59:24 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:59:24 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.303\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.705\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.203\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.190\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.371\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.166\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.434\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.454\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.425\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.494\n",
            "\u001b[32m[01/11 03:59:24 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.341 | 70.492 | 20.330 | 0.004 | 19.012 | 37.075 |\n",
            "\u001b[32m[01/11 03:59:24 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.341 |\n",
            "\u001b[32m[01/11 03:59:24 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:59:24 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:59:24 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:59:24 d2.evaluation.testing]: \u001b[0mcopypaste: 30.3412,70.4921,20.3298,0.0044,19.0117,37.0752\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:59:32 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:59:32 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:59:32 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:59:32 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:59:32 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:59:32 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:59:32 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:59:32 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:59:33 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0506 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 03:59:38 d2.evaluation.evaluator]: \u001b[0mInference done 105/245. Dataloading: 0.0018 s/iter. Inference: 0.0511 s/iter. Eval: 0.0003 s/iter. Total: 0.0532 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 03:59:43 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0016 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 03:59:45 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.571175 (0.052380 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:59:45 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049598 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 03:59:45 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 03:59:45 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 03:59:45 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.323\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.713\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.230\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.215\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.391\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.175\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.456\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.477\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.033\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.447\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.520\n",
            "\u001b[32m[01/11 03:59:46 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.276 | 71.331 | 23.037 | 0.015 | 21.468 | 39.124 |\n",
            "\u001b[32m[01/11 03:59:46 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.276 |\n",
            "\u001b[32m[01/11 03:59:46 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 03:59:46 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 03:59:46 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 03:59:46 d2.evaluation.testing]: \u001b[0mcopypaste: 32.2761,71.3310,23.0368,0.0148,21.4683,39.1245\n",
            "\u001b[32m[01/11 03:59:46 d2.utils.events]: \u001b[0m eta: 0:08:27  iter: 4339  total_loss: 0.6684  loss_cls: 0.1501  loss_box_reg: 0.4177  loss_rpn_cls: 0.01875  loss_rpn_loc: 0.0198    time: 0.7713  last_time: 0.7767  data_time: 0.0317  last_data_time: 0.0305   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 03:59:54 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 03:59:54 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 03:59:54 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 03:59:54 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 03:59:54 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 03:59:54 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 03:59:54 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 03:59:54 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 03:59:55 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0066 s/iter. Inference: 0.0483 s/iter. Eval: 0.0003 s/iter. Total: 0.0553 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:00:00 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0016 s/iter. Inference: 0.0506 s/iter. Eval: 0.0003 s/iter. Total: 0.0525 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 04:00:05 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0017 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:00:07 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.629835 (0.052624 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:00:07 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049704 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:00:07 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:00:07 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:00:07 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.314\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.711\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.211\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.203\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.383\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.442\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.468\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.511\n",
            "\u001b[32m[01/11 04:00:08 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.370 | 71.055 | 21.146 | 0.003 | 20.315 | 38.316 |\n",
            "\u001b[32m[01/11 04:00:08 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.370 |\n",
            "\u001b[32m[01/11 04:00:08 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:00:08 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:00:08 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:00:08 d2.evaluation.testing]: \u001b[0mcopypaste: 31.3698,71.0554,21.1462,0.0033,20.3148,38.3162\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:00:15 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:00:15 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:00:15 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:00:15 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:00:15 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:00:15 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:00:15 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:00:15 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:00:16 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0505 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 04:00:21 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0017 s/iter. Inference: 0.0488 s/iter. Eval: 0.0002 s/iter. Total: 0.0508 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 04:00:26 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0017 s/iter. Inference: 0.0493 s/iter. Eval: 0.0002 s/iter. Total: 0.0513 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 04:00:29 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.572937 (0.052387 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:00:29 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049492 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:00:29 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:00:29 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:00:29 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.21s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.334\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.715\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.252\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.214\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.409\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.177\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.457\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.474\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.415\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.535\n",
            "\u001b[32m[01/11 04:00:29 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.401 | 71.531 | 25.174 | 0.002 | 21.385 | 40.939 |\n",
            "\u001b[32m[01/11 04:00:29 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.401 |\n",
            "\u001b[32m[01/11 04:00:29 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:00:29 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:00:29 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:00:29 d2.evaluation.testing]: \u001b[0mcopypaste: 33.4010,71.5314,25.1737,0.0024,21.3854,40.9394\n",
            "\u001b[32m[01/11 04:00:29 d2.utils.events]: \u001b[0m eta: 0:08:11  iter: 4359  total_loss: 0.6012  loss_cls: 0.1471  loss_box_reg: 0.4025  loss_rpn_cls: 0.01733  loss_rpn_loc: 0.01646    time: 0.7712  last_time: 0.6921  data_time: 0.0327  last_data_time: 0.0218   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:00:37 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:00:37 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:00:37 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:00:37 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:00:37 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:00:37 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:00:37 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:00:37 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:00:38 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0507 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:00:43 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0017 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 04:00:48 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0017 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 04:00:50 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.413111 (0.051721 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:00:50 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049036 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:00:50 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:00:50 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:00:50 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.322\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.708\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.238\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.207\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.393\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.175\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.449\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.470\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.430\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.519\n",
            "\u001b[32m[01/11 04:00:51 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.206 | 70.833 | 23.818 | 0.004 | 20.722 | 39.285 |\n",
            "\u001b[32m[01/11 04:00:51 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.206 |\n",
            "\u001b[32m[01/11 04:00:51 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:00:51 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:00:51 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:00:51 d2.evaluation.testing]: \u001b[0mcopypaste: 32.2061,70.8334,23.8181,0.0038,20.7224,39.2852\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:00:58 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:00:58 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:00:58 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:00:58 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:00:58 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:00:58 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:00:58 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:00:58 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:00:59 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0478 s/iter. Eval: 0.0003 s/iter. Total: 0.0491 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 04:01:04 d2.evaluation.evaluator]: \u001b[0mInference done 100/245. Dataloading: 0.0018 s/iter. Inference: 0.0539 s/iter. Eval: 0.0003 s/iter. Total: 0.0560 s/iter. ETA=0:00:08\n",
            "\u001b[32m[01/11 04:01:09 d2.evaluation.evaluator]: \u001b[0mInference done 193/245. Dataloading: 0.0016 s/iter. Inference: 0.0530 s/iter. Eval: 0.0003 s/iter. Total: 0.0549 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:01:12 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:13.123603 (0.054682 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:01:12 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.052105 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:01:12 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:01:12 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:01:12 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.336\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.723\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.249\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.211\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.415\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.179\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.460\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.487\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.435\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.543\n",
            "\u001b[32m[01/11 04:01:13 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.623 | 72.279 | 24.937 | 0.004 | 21.124 | 41.458 |\n",
            "\u001b[32m[01/11 04:01:13 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.623 |\n",
            "\u001b[32m[01/11 04:01:13 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:01:13 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:01:13 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:01:13 d2.evaluation.testing]: \u001b[0mcopypaste: 33.6229,72.2786,24.9373,0.0036,21.1238,41.4577\n",
            "\u001b[32m[01/11 04:01:13 d2.utils.events]: \u001b[0m eta: 0:07:56  iter: 4379  total_loss: 0.5764  loss_cls: 0.1417  loss_box_reg: 0.391  loss_rpn_cls: 0.01764  loss_rpn_loc: 0.01373    time: 0.7712  last_time: 0.7749  data_time: 0.0360  last_data_time: 0.0335   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:01:21 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:01:21 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:01:21 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:01:21 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:01:21 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:01:21 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:01:21 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:01:21 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:01:22 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0529 s/iter. Eval: 0.0003 s/iter. Total: 0.0542 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:01:27 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0014 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 04:01:32 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0015 s/iter. Inference: 0.0507 s/iter. Eval: 0.0003 s/iter. Total: 0.0525 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:01:34 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.763053 (0.053179 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:01:34 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050628 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:01:34 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:01:34 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:01:34 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.334\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.712\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.262\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.210\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.411\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.178\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.458\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.483\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.432\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.540\n",
            "\u001b[32m[01/11 04:01:35 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.407 | 71.190 | 26.210 | 0.002 | 21.016 | 41.104 |\n",
            "\u001b[32m[01/11 04:01:35 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.407 |\n",
            "\u001b[32m[01/11 04:01:35 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:01:35 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:01:35 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:01:35 d2.evaluation.testing]: \u001b[0mcopypaste: 33.4075,71.1905,26.2097,0.0018,21.0159,41.1043\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:01:42 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:01:42 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:01:42 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:01:42 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:01:42 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:01:42 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:01:42 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:01:42 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:01:43 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0502 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:01:49 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0017 s/iter. Inference: 0.0507 s/iter. Eval: 0.0003 s/iter. Total: 0.0528 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 04:01:54 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0016 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:01:56 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.621024 (0.052588 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:01:56 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049785 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:01:56 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:01:56 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:01:56 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.20s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.323\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.718\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.211\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.212\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.393\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.449\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.468\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.424\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.520\n",
            "\u001b[32m[01/11 04:01:56 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.279 | 71.816 | 21.133 | 0.000 | 21.206 | 39.269 |\n",
            "\u001b[32m[01/11 04:01:56 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.279 |\n",
            "\u001b[32m[01/11 04:01:56 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:01:56 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:01:56 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:01:56 d2.evaluation.testing]: \u001b[0mcopypaste: 32.2785,71.8163,21.1328,0.0000,21.2064,39.2686\n",
            "\u001b[32m[01/11 04:01:56 d2.utils.events]: \u001b[0m eta: 0:07:41  iter: 4399  total_loss: 0.6097  loss_cls: 0.1558  loss_box_reg: 0.3773  loss_rpn_cls: 0.01615  loss_rpn_loc: 0.01346    time: 0.7713  last_time: 0.7763  data_time: 0.0357  last_data_time: 0.0269   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:02:04 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:02:04 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:02:04 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:02:04 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:02:04 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:02:04 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:02:04 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:02:04 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:02:05 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0031 s/iter. Inference: 0.0499 s/iter. Eval: 0.0003 s/iter. Total: 0.0534 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:02:10 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0025 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 04:02:15 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0022 s/iter. Inference: 0.0490 s/iter. Eval: 0.0002 s/iter. Total: 0.0516 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:02:18 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.545956 (0.052275 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:02:18 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049205 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:02:18 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:02:18 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:02:18 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.21s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.313\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.713\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.222\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.198\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.381\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.170\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.444\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.465\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.438\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.506\n",
            "\u001b[32m[01/11 04:02:18 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.331 | 71.308 | 22.244 | 0.000 | 19.766 | 38.145 |\n",
            "\u001b[32m[01/11 04:02:18 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.331 |\n",
            "\u001b[32m[01/11 04:02:18 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:02:18 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:02:18 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:02:18 d2.evaluation.testing]: \u001b[0mcopypaste: 31.3311,71.3082,22.2442,0.0000,19.7659,38.1446\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:02:25 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:02:25 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:02:25 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:02:25 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:02:25 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:02:25 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:02:25 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:02:25 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:02:27 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0477 s/iter. Eval: 0.0002 s/iter. Total: 0.0491 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 04:02:32 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0018 s/iter. Inference: 0.0486 s/iter. Eval: 0.0003 s/iter. Total: 0.0508 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 04:02:37 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0018 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 04:02:39 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.406058 (0.051692 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:02:39 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048746 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:02:39 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:02:39 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:02:39 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.310\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.707\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.219\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.189\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.383\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.171\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.467\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.442\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.506\n",
            "\u001b[32m[01/11 04:02:39 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.036 | 70.704 | 21.875 | 0.000 | 18.902 | 38.267 |\n",
            "\u001b[32m[01/11 04:02:39 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.036 |\n",
            "\u001b[32m[01/11 04:02:39 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:02:39 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:02:39 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:02:39 d2.evaluation.testing]: \u001b[0mcopypaste: 31.0364,70.7041,21.8754,0.0000,18.9019,38.2673\n",
            "\u001b[32m[01/11 04:02:39 d2.utils.events]: \u001b[0m eta: 0:07:25  iter: 4419  total_loss: 0.5452  loss_cls: 0.1302  loss_box_reg: 0.3985  loss_rpn_cls: 0.01361  loss_rpn_loc: 0.01467    time: 0.7712  last_time: 0.7684  data_time: 0.0343  last_data_time: 0.0235   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:02:47 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:02:47 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:02:47 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:02:47 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:02:47 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:02:47 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:02:47 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:02:47 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:02:48 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0479 s/iter. Eval: 0.0003 s/iter. Total: 0.0491 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 04:02:53 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0019 s/iter. Inference: 0.0487 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 04:02:58 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0020 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:03:00 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.496443 (0.052069 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:03:00 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049133 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:03:01 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:03:01 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:03:01 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.329\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.711\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.254\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.200\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.408\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.174\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.456\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.481\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.423\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.542\n",
            "\u001b[32m[01/11 04:03:01 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.863 | 71.092 | 25.426 | 0.002 | 19.987 | 40.830 |\n",
            "\u001b[32m[01/11 04:03:01 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.863 |\n",
            "\u001b[32m[01/11 04:03:01 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:03:01 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:03:01 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:03:01 d2.evaluation.testing]: \u001b[0mcopypaste: 32.8630,71.0917,25.4259,0.0021,19.9865,40.8301\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:03:09 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:03:09 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:03:09 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:03:09 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:03:09 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:03:09 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:03:09 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:03:09 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:03:10 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0521 s/iter. Eval: 0.0002 s/iter. Total: 0.0533 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:03:15 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0016 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0523 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 04:03:20 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0019 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0523 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:03:22 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.643220 (0.052680 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:03:22 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049719 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:03:22 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:03:22 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:03:22 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.22s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.312\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.707\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.203\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.201\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.386\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.438\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.457\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.410\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.510\n",
            "\u001b[32m[01/11 04:03:22 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.172 | 70.662 | 20.263 | 0.002 | 20.100 | 38.571 |\n",
            "\u001b[32m[01/11 04:03:22 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.172 |\n",
            "\u001b[32m[01/11 04:03:22 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:03:22 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:03:22 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:03:22 d2.evaluation.testing]: \u001b[0mcopypaste: 31.1721,70.6624,20.2629,0.0018,20.0996,38.5713\n",
            "\u001b[32m[01/11 04:03:22 d2.utils.events]: \u001b[0m eta: 0:07:10  iter: 4439  total_loss: 0.58  loss_cls: 0.1392  loss_box_reg: 0.3832  loss_rpn_cls: 0.01628  loss_rpn_loc: 0.0199    time: 0.7712  last_time: 0.7677  data_time: 0.0306  last_data_time: 0.0231   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:03:31 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:03:31 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:03:31 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:03:31 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:03:31 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:03:31 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:03:31 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:03:31 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:03:32 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0495 s/iter. Eval: 0.0002 s/iter. Total: 0.0509 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 04:03:37 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0016 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 04:03:42 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0016 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:03:44 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.497555 (0.052073 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:03:44 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049483 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:03:44 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:03:44 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:03:44 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.326\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.715\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.229\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.208\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.400\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.179\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.453\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.475\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.427\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.528\n",
            "\u001b[32m[01/11 04:03:44 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.597 | 71.514 | 22.904 | 0.006 | 20.847 | 40.037 |\n",
            "\u001b[32m[01/11 04:03:44 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.597 |\n",
            "\u001b[32m[01/11 04:03:44 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:03:44 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:03:44 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:03:44 d2.evaluation.testing]: \u001b[0mcopypaste: 32.5968,71.5141,22.9036,0.0059,20.8474,40.0369\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:03:52 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:03:52 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:03:52 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:03:52 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:03:52 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:03:52 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:03:52 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:03:52 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:03:53 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0500 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:03:58 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0021 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 04:04:03 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0020 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:04:05 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.529887 (0.052208 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:04:05 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049215 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:04:05 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:04:05 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:04:05 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.331\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.720\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.244\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.203\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.409\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.178\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.460\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.487\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.439\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.542\n",
            "\u001b[32m[01/11 04:04:05 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.052 | 72.004 | 24.395 | 0.003 | 20.273 | 40.906 |\n",
            "\u001b[32m[01/11 04:04:05 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.052 |\n",
            "\u001b[32m[01/11 04:04:05 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:04:05 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:04:05 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:04:05 d2.evaluation.testing]: \u001b[0mcopypaste: 33.0525,72.0044,24.3946,0.0029,20.2727,40.9057\n",
            "\u001b[32m[01/11 04:04:05 d2.utils.events]: \u001b[0m eta: 0:06:54  iter: 4459  total_loss: 0.6134  loss_cls: 0.1511  loss_box_reg: 0.4094  loss_rpn_cls: 0.0209  loss_rpn_loc: 0.01675    time: 0.7712  last_time: 0.7724  data_time: 0.0306  last_data_time: 0.0284   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:04:14 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:04:14 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:04:14 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:04:14 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:04:14 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:04:14 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:04:14 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:04:14 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:04:15 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0500 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:04:20 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0020 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 04:04:25 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0019 s/iter. Inference: 0.0492 s/iter. Eval: 0.0002 s/iter. Total: 0.0514 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:04:27 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.434350 (0.051810 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:04:27 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049022 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:04:27 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:04:27 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:04:27 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.313\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.710\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.211\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.206\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.383\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.174\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.472\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.444\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.514\n",
            "\u001b[32m[01/11 04:04:28 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.343 | 70.969 | 21.099 | 0.001 | 20.575 | 38.307 |\n",
            "\u001b[32m[01/11 04:04:28 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.343 |\n",
            "\u001b[32m[01/11 04:04:28 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:04:28 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:04:28 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:04:28 d2.evaluation.testing]: \u001b[0mcopypaste: 31.3430,70.9689,21.0989,0.0015,20.5751,38.3065\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:04:35 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:04:35 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:04:35 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:04:35 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:04:35 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:04:35 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:04:35 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:04:35 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:04:36 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0064 s/iter. Inference: 0.0479 s/iter. Eval: 0.0002 s/iter. Total: 0.0546 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:04:42 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0017 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 04:04:47 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0017 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 04:04:49 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.444527 (0.051852 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:04:49 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049126 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:04:49 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:04:49 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:04:49 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.22s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.333\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.725\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.255\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.211\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.410\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.179\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.463\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.483\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.420\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.547\n",
            "\u001b[32m[01/11 04:04:49 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.295 | 72.524 | 25.533 | 0.002 | 21.079 | 41.005 |\n",
            "\u001b[32m[01/11 04:04:49 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.295 |\n",
            "\u001b[32m[01/11 04:04:49 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:04:49 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:04:49 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:04:49 d2.evaluation.testing]: \u001b[0mcopypaste: 33.2953,72.5244,25.5335,0.0021,21.0786,41.0047\n",
            "\u001b[32m[01/11 04:04:49 d2.utils.events]: \u001b[0m eta: 0:06:39  iter: 4479  total_loss: 0.6403  loss_cls: 0.1522  loss_box_reg: 0.4382  loss_rpn_cls: 0.01541  loss_rpn_loc: 0.01996    time: 0.7712  last_time: 0.7687  data_time: 0.0321  last_data_time: 0.0278   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:04:57 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:04:57 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:04:57 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:04:57 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:04:57 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:04:57 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:04:57 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:04:57 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:04:58 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0016 s/iter. Inference: 0.0483 s/iter. Eval: 0.0002 s/iter. Total: 0.0502 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 04:05:03 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0017 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0509 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 04:05:08 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0019 s/iter. Inference: 0.0492 s/iter. Eval: 0.0002 s/iter. Total: 0.0514 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 04:05:11 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.489913 (0.052041 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:05:11 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049025 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:05:11 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:05:11 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:05:11 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.709\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.229\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.213\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.384\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.450\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.473\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.521\n",
            "\u001b[32m[01/11 04:05:11 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.659 | 70.916 | 22.858 | 0.007 | 21.262 | 38.442 |\n",
            "\u001b[32m[01/11 04:05:11 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.659 |\n",
            "\u001b[32m[01/11 04:05:11 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:05:11 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:05:11 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:05:11 d2.evaluation.testing]: \u001b[0mcopypaste: 31.6587,70.9162,22.8578,0.0074,21.2622,38.4418\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:05:19 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:05:19 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:05:19 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:05:19 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:05:19 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:05:19 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:05:19 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:05:19 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:05:20 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0008 s/iter. Inference: 0.0516 s/iter. Eval: 0.0003 s/iter. Total: 0.0527 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:05:25 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0022 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 04:05:30 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0021 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:05:32 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.659079 (0.052746 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:05:32 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049569 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:05:32 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:05:32 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:05:32 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.314\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.709\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.222\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.207\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.382\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.170\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.449\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.475\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.520\n",
            "\u001b[32m[01/11 04:05:32 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.431 | 70.936 | 22.240 | 0.003 | 20.681 | 38.228 |\n",
            "\u001b[32m[01/11 04:05:32 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.431 |\n",
            "\u001b[32m[01/11 04:05:32 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:05:32 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:05:32 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:05:32 d2.evaluation.testing]: \u001b[0mcopypaste: 31.4309,70.9365,22.2403,0.0028,20.6806,38.2284\n",
            "\u001b[32m[01/11 04:05:32 d2.utils.events]: \u001b[0m eta: 0:06:24  iter: 4499  total_loss: 0.6306  loss_cls: 0.1591  loss_box_reg: 0.3967  loss_rpn_cls: 0.01914  loss_rpn_loc: 0.02064    time: 0.7713  last_time: 0.7658  data_time: 0.0321  last_data_time: 0.0226   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:05:41 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:05:41 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:05:41 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:05:41 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:05:41 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:05:41 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:05:41 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:05:41 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:05:42 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0476 s/iter. Eval: 0.0003 s/iter. Total: 0.0491 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 04:05:47 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0015 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 04:05:52 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0016 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:05:54 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.644650 (0.052686 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:05:54 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050042 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:05:54 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:05:54 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:05:54 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.323\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.716\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.229\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.207\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.398\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.458\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.476\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.420\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.534\n",
            "\u001b[32m[01/11 04:05:54 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.348 | 71.627 | 22.866 | 0.013 | 20.715 | 39.822 |\n",
            "\u001b[32m[01/11 04:05:54 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.348 |\n",
            "\u001b[32m[01/11 04:05:54 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:05:54 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:05:54 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:05:54 d2.evaluation.testing]: \u001b[0mcopypaste: 32.3479,71.6271,22.8658,0.0128,20.7148,39.8221\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:06:02 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:06:02 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:06:02 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:06:02 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:06:02 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:06:02 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:06:02 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:06:02 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:06:03 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0023 s/iter. Inference: 0.0523 s/iter. Eval: 0.0002 s/iter. Total: 0.0549 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:06:08 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0015 s/iter. Inference: 0.0496 s/iter. Eval: 0.0002 s/iter. Total: 0.0514 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 04:06:13 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0016 s/iter. Inference: 0.0499 s/iter. Eval: 0.0002 s/iter. Total: 0.0518 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:06:15 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.560468 (0.052335 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:06:15 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049804 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:06:15 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:06:15 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:06:15 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.22s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.324\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.714\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.211\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.396\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.455\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.478\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.529\n",
            "\u001b[32m[01/11 04:06:16 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.417 | 71.428 | 21.689 | 0.004 | 21.087 | 39.578 |\n",
            "\u001b[32m[01/11 04:06:16 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.417 |\n",
            "\u001b[32m[01/11 04:06:16 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:06:16 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:06:16 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:06:16 d2.evaluation.testing]: \u001b[0mcopypaste: 32.4174,71.4278,21.6893,0.0045,21.0871,39.5784\n",
            "\u001b[32m[01/11 04:06:16 d2.utils.events]: \u001b[0m eta: 0:06:08  iter: 4519  total_loss: 0.5921  loss_cls: 0.1546  loss_box_reg: 0.4074  loss_rpn_cls: 0.01496  loss_rpn_loc: 0.01373    time: 0.7713  last_time: 0.7730  data_time: 0.0336  last_data_time: 0.0251   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:06:24 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:06:24 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:06:24 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:06:24 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:06:24 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:06:24 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:06:24 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:06:24 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:06:25 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0008 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0502 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 04:06:30 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0017 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 04:06:35 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0021 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:06:37 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.627442 (0.052614 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:06:37 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049578 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:06:37 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:06:37 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:06:37 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.318\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.711\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.221\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.204\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.387\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.449\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.473\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.438\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.519\n",
            "\u001b[32m[01/11 04:06:38 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.754 | 71.054 | 22.075 | 0.004 | 20.382 | 38.679 |\n",
            "\u001b[32m[01/11 04:06:38 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.754 |\n",
            "\u001b[32m[01/11 04:06:38 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:06:38 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:06:38 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:06:38 d2.evaluation.testing]: \u001b[0mcopypaste: 31.7538,71.0539,22.0745,0.0036,20.3823,38.6791\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:06:45 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:06:45 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:06:45 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:06:45 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:06:45 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:06:45 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:06:45 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:06:45 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:06:46 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0020 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:06:51 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0018 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 04:06:56 d2.evaluation.evaluator]: \u001b[0mInference done 209/245. Dataloading: 0.0016 s/iter. Inference: 0.0489 s/iter. Eval: 0.0002 s/iter. Total: 0.0507 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 04:06:58 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.397796 (0.051657 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:06:58 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048970 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:06:58 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:06:58 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:06:58 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.324\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.711\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.239\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.207\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.394\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.451\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.472\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.519\n",
            "\u001b[32m[01/11 04:06:59 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.363 | 71.104 | 23.935 | 0.007 | 20.651 | 39.438 |\n",
            "\u001b[32m[01/11 04:06:59 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.363 |\n",
            "\u001b[32m[01/11 04:06:59 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:06:59 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:06:59 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:06:59 d2.evaluation.testing]: \u001b[0mcopypaste: 32.3627,71.1041,23.9348,0.0068,20.6510,39.4379\n",
            "\u001b[32m[01/11 04:06:59 d2.utils.events]: \u001b[0m eta: 0:05:53  iter: 4539  total_loss: 0.5611  loss_cls: 0.1423  loss_box_reg: 0.3867  loss_rpn_cls: 0.0157  loss_rpn_loc: 0.01448    time: 0.7712  last_time: 0.7631  data_time: 0.0317  last_data_time: 0.0222   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:07:07 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:07:07 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:07:07 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:07:07 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:07:07 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:07:07 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:07:07 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:07:07 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:07:08 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0008 s/iter. Inference: 0.0522 s/iter. Eval: 0.0003 s/iter. Total: 0.0533 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:07:13 d2.evaluation.evaluator]: \u001b[0mInference done 111/245. Dataloading: 0.0015 s/iter. Inference: 0.0486 s/iter. Eval: 0.0002 s/iter. Total: 0.0505 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 04:07:18 d2.evaluation.evaluator]: \u001b[0mInference done 209/245. Dataloading: 0.0016 s/iter. Inference: 0.0490 s/iter. Eval: 0.0002 s/iter. Total: 0.0508 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 04:07:20 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.378486 (0.051577 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:07:20 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049032 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:07:20 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:07:20 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:07:20 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.22s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.336\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.728\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.255\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.213\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.412\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.177\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.468\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.485\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.427\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.546\n",
            "\u001b[32m[01/11 04:07:20 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.575 | 72.817 | 25.508 | 0.014 | 21.277 | 41.196 |\n",
            "\u001b[32m[01/11 04:07:20 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.575 |\n",
            "\u001b[32m[01/11 04:07:20 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:07:20 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:07:20 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:07:20 d2.evaluation.testing]: \u001b[0mcopypaste: 33.5754,72.8170,25.5075,0.0139,21.2769,41.1965\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:07:28 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:07:28 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:07:28 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:07:28 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:07:28 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:07:28 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:07:28 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:07:28 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:07:29 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0026 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0527 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:07:34 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0019 s/iter. Inference: 0.0500 s/iter. Eval: 0.0003 s/iter. Total: 0.0523 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 04:07:39 d2.evaluation.evaluator]: \u001b[0mInference done 194/245. Dataloading: 0.0019 s/iter. Inference: 0.0526 s/iter. Eval: 0.0003 s/iter. Total: 0.0548 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:07:42 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:13.105315 (0.054605 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:07:42 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.051684 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:07:42 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:07:42 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:07:42 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.320\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.707\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.204\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.392\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.171\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.453\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.471\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.432\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.518\n",
            "\u001b[32m[01/11 04:07:42 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.000 | 70.669 | 21.475 | 0.004 | 20.389 | 39.204 |\n",
            "\u001b[32m[01/11 04:07:42 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.000 |\n",
            "\u001b[32m[01/11 04:07:42 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:07:42 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:07:42 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:07:42 d2.evaluation.testing]: \u001b[0mcopypaste: 32.0003,70.6690,21.4748,0.0043,20.3885,39.2044\n",
            "\u001b[32m[01/11 04:07:42 d2.utils.events]: \u001b[0m eta: 0:05:38  iter: 4559  total_loss: 0.6356  loss_cls: 0.1486  loss_box_reg: 0.4174  loss_rpn_cls: 0.01715  loss_rpn_loc: 0.02025    time: 0.7712  last_time: 0.6929  data_time: 0.0338  last_data_time: 0.0213   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:07:50 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:07:50 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:07:50 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:07:50 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:07:50 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:07:50 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:07:50 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:07:50 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:07:51 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0480 s/iter. Eval: 0.0003 s/iter. Total: 0.0492 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 04:07:56 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0018 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0517 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 04:08:01 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0017 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 04:08:04 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.385225 (0.051605 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:08:04 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048864 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:08:04 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:08:04 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:08:04 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.324\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.713\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.253\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.203\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.400\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.174\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.457\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.478\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.430\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.532\n",
            "\u001b[32m[01/11 04:08:04 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.378 | 71.271 | 25.321 | 0.005 | 20.334 | 40.009 |\n",
            "\u001b[32m[01/11 04:08:04 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.378 |\n",
            "\u001b[32m[01/11 04:08:04 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:08:04 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:08:04 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:08:04 d2.evaluation.testing]: \u001b[0mcopypaste: 32.3780,71.2712,25.3213,0.0054,20.3342,40.0092\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:08:12 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:08:12 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:08:12 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:08:12 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:08:12 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:08:12 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:08:12 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:08:12 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:08:13 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0062 s/iter. Inference: 0.0487 s/iter. Eval: 0.0002 s/iter. Total: 0.0552 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:08:18 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0021 s/iter. Inference: 0.0497 s/iter. Eval: 0.0002 s/iter. Total: 0.0521 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 04:08:23 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0019 s/iter. Inference: 0.0494 s/iter. Eval: 0.0002 s/iter. Total: 0.0516 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:08:25 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.614978 (0.052562 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:08:25 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049697 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:08:25 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:08:25 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:08:25 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.21s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.312\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.705\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.197\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.212\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.377\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.167\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.444\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.459\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.420\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.507\n",
            "\u001b[32m[01/11 04:08:25 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.232 | 70.542 | 19.702 | 0.002 | 21.158 | 37.727 |\n",
            "\u001b[32m[01/11 04:08:25 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.232 |\n",
            "\u001b[32m[01/11 04:08:25 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:08:25 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:08:25 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:08:25 d2.evaluation.testing]: \u001b[0mcopypaste: 31.2316,70.5419,19.7016,0.0024,21.1581,37.7270\n",
            "\u001b[32m[01/11 04:08:25 d2.utils.events]: \u001b[0m eta: 0:05:22  iter: 4579  total_loss: 0.5464  loss_cls: 0.1254  loss_box_reg: 0.3795  loss_rpn_cls: 0.01482  loss_rpn_loc: 0.01036    time: 0.7712  last_time: 0.7658  data_time: 0.0341  last_data_time: 0.0237   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:08:33 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:08:33 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:08:33 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:08:33 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:08:33 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:08:33 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:08:33 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:08:33 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:08:35 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0501 s/iter. Eval: 0.0002 s/iter. Total: 0.0513 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:08:40 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0018 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 04:08:45 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0020 s/iter. Inference: 0.0492 s/iter. Eval: 0.0002 s/iter. Total: 0.0515 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:08:47 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.549053 (0.052288 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:08:47 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049187 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:08:47 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:08:47 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:08:47 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.316\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.715\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.213\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.203\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.386\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.445\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.466\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.430\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.512\n",
            "\u001b[32m[01/11 04:08:47 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.641 | 71.501 | 21.262 | 0.006 | 20.334 | 38.650 |\n",
            "\u001b[32m[01/11 04:08:47 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.641 |\n",
            "\u001b[32m[01/11 04:08:47 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:08:47 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:08:47 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:08:47 d2.evaluation.testing]: \u001b[0mcopypaste: 31.6412,71.5007,21.2621,0.0058,20.3335,38.6499\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:08:55 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:08:55 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:08:55 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:08:55 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:08:55 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:08:55 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:08:55 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:08:55 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:08:56 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0015 s/iter. Inference: 0.0493 s/iter. Eval: 0.0002 s/iter. Total: 0.0510 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 04:09:01 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0018 s/iter. Inference: 0.0487 s/iter. Eval: 0.0003 s/iter. Total: 0.0508 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 04:09:06 d2.evaluation.evaluator]: \u001b[0mInference done 210/245. Dataloading: 0.0016 s/iter. Inference: 0.0487 s/iter. Eval: 0.0002 s/iter. Total: 0.0507 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 04:09:08 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.304145 (0.051267 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:09:08 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048662 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:09:08 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:09:08 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:09:08 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.325\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.724\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.240\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.215\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.394\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.170\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.455\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.475\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.524\n",
            "\u001b[32m[01/11 04:09:08 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.470 | 72.405 | 24.032 | 0.000 | 21.501 | 39.425 |\n",
            "\u001b[32m[01/11 04:09:08 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.470 |\n",
            "\u001b[32m[01/11 04:09:08 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:09:08 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:09:08 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:09:08 d2.evaluation.testing]: \u001b[0mcopypaste: 32.4696,72.4055,24.0320,0.0000,21.5009,39.4251\n",
            "\u001b[32m[01/11 04:09:08 d2.utils.events]: \u001b[0m eta: 0:05:07  iter: 4599  total_loss: 0.5956  loss_cls: 0.1485  loss_box_reg: 0.405  loss_rpn_cls: 0.01641  loss_rpn_loc: 0.01433    time: 0.7712  last_time: 0.7596  data_time: 0.0325  last_data_time: 0.0151   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:09:16 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:09:16 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:09:16 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:09:16 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:09:16 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:09:16 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:09:16 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:09:16 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:09:18 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0514 s/iter. Eval: 0.0002 s/iter. Total: 0.0526 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:09:23 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0015 s/iter. Inference: 0.0493 s/iter. Eval: 0.0002 s/iter. Total: 0.0512 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 04:09:28 d2.evaluation.evaluator]: \u001b[0mInference done 209/245. Dataloading: 0.0015 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0508 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 04:09:30 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.331234 (0.051380 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:09:30 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048839 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:09:30 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:09:30 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:09:30 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.21s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.321\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.711\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.236\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.211\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.389\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.170\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.446\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.467\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.437\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.510\n",
            "\u001b[32m[01/11 04:09:30 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.074 | 71.134 | 23.574 | 0.000 | 21.148 | 38.864 |\n",
            "\u001b[32m[01/11 04:09:30 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.074 |\n",
            "\u001b[32m[01/11 04:09:30 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:09:30 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:09:30 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:09:30 d2.evaluation.testing]: \u001b[0mcopypaste: 32.0739,71.1345,23.5744,0.0000,21.1481,38.8644\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:09:38 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:09:38 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:09:38 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:09:38 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:09:38 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:09:38 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:09:38 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:09:38 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:09:39 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0502 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 04:09:44 d2.evaluation.evaluator]: \u001b[0mInference done 105/245. Dataloading: 0.0014 s/iter. Inference: 0.0515 s/iter. Eval: 0.0003 s/iter. Total: 0.0532 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 04:09:49 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0016 s/iter. Inference: 0.0502 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:09:51 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.652185 (0.052717 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:09:51 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050120 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:09:51 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:09:51 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:09:51 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.314\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.711\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.218\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.208\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.382\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.169\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.441\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.475\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.019\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.459\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.509\n",
            "\u001b[32m[01/11 04:09:51 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.400 | 71.124 | 21.832 | 0.010 | 20.751 | 38.163 |\n",
            "\u001b[32m[01/11 04:09:51 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.400 |\n",
            "\u001b[32m[01/11 04:09:51 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:09:51 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:09:51 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:09:51 d2.evaluation.testing]: \u001b[0mcopypaste: 31.4004,71.1237,21.8324,0.0099,20.7506,38.1633\n",
            "\u001b[32m[01/11 04:09:51 d2.utils.events]: \u001b[0m eta: 0:04:51  iter: 4619  total_loss: 0.6497  loss_cls: 0.1577  loss_box_reg: 0.4111  loss_rpn_cls: 0.01826  loss_rpn_loc: 0.01994    time: 0.7712  last_time: 0.7674  data_time: 0.0337  last_data_time: 0.0181   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:10:00 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:10:00 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:10:00 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:10:00 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:10:00 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:10:00 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:10:00 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:10:00 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:10:01 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0492 s/iter. Eval: 0.0002 s/iter. Total: 0.0503 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 04:10:06 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0015 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0511 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 04:10:11 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0019 s/iter. Inference: 0.0488 s/iter. Eval: 0.0002 s/iter. Total: 0.0511 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 04:10:13 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.390819 (0.051628 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:10:13 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048724 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:10:13 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:10:13 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:10:13 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.21s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.327\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.714\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.253\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.215\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.397\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.176\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.454\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.478\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.440\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.527\n",
            "\u001b[32m[01/11 04:10:13 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.666 | 71.400 | 25.317 | 0.000 | 21.461 | 39.739 |\n",
            "\u001b[32m[01/11 04:10:13 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.666 |\n",
            "\u001b[32m[01/11 04:10:13 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:10:13 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:10:13 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:10:13 d2.evaluation.testing]: \u001b[0mcopypaste: 32.6656,71.4001,25.3166,0.0000,21.4609,39.7392\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:10:21 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:10:21 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:10:21 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:10:21 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:10:21 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:10:21 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:10:21 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:10:21 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:10:22 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0015 s/iter. Inference: 0.0537 s/iter. Eval: 0.0003 s/iter. Total: 0.0554 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:10:27 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0017 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 04:10:32 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0017 s/iter. Inference: 0.0500 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:10:34 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.675265 (0.052814 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:10:34 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050007 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:10:34 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:10:34 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:10:34 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.320\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.713\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.210\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.390\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.451\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.474\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.441\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.518\n",
            "\u001b[32m[01/11 04:10:34 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.029 | 71.256 | 21.723 | 0.001 | 20.998 | 38.976 |\n",
            "\u001b[32m[01/11 04:10:34 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.029 |\n",
            "\u001b[32m[01/11 04:10:34 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:10:34 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:10:34 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:10:34 d2.evaluation.testing]: \u001b[0mcopypaste: 32.0286,71.2564,21.7234,0.0012,20.9984,38.9763\n",
            "\u001b[32m[01/11 04:10:34 d2.utils.events]: \u001b[0m eta: 0:04:36  iter: 4639  total_loss: 0.6067  loss_cls: 0.159  loss_box_reg: 0.3977  loss_rpn_cls: 0.01421  loss_rpn_loc: 0.01449    time: 0.7712  last_time: 0.7683  data_time: 0.0322  last_data_time: 0.0238   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:10:43 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:10:43 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:10:43 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:10:43 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:10:43 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:10:43 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:10:43 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:10:43 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:10:44 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0506 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 04:10:49 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0016 s/iter. Inference: 0.0486 s/iter. Eval: 0.0003 s/iter. Total: 0.0506 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 04:10:54 d2.evaluation.evaluator]: \u001b[0mInference done 209/245. Dataloading: 0.0016 s/iter. Inference: 0.0488 s/iter. Eval: 0.0003 s/iter. Total: 0.0507 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 04:10:56 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.340289 (0.051418 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:10:56 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048876 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:10:56 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:10:56 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:10:56 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.709\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.207\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.388\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.451\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.471\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.019\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.432\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.519\n",
            "\u001b[32m[01/11 04:10:56 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.854 | 70.904 | 21.494 | 0.006 | 20.724 | 38.824 |\n",
            "\u001b[32m[01/11 04:10:56 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.854 |\n",
            "\u001b[32m[01/11 04:10:56 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:10:56 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:10:56 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:10:56 d2.evaluation.testing]: \u001b[0mcopypaste: 31.8540,70.9037,21.4937,0.0060,20.7243,38.8236\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:11:04 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:11:04 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:11:04 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:11:04 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:11:04 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:11:04 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:11:04 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:11:04 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:11:05 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0034 s/iter. Inference: 0.0487 s/iter. Eval: 0.0002 s/iter. Total: 0.0523 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:11:10 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0018 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 04:11:15 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0020 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:11:17 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.553839 (0.052308 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:11:17 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049343 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:11:17 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:11:17 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:11:17 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.324\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.711\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.243\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.210\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.396\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.176\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.458\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.480\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.429\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.537\n",
            "\u001b[32m[01/11 04:11:18 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.392 | 71.140 | 24.299 | 0.002 | 21.015 | 39.556 |\n",
            "\u001b[32m[01/11 04:11:18 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.392 |\n",
            "\u001b[32m[01/11 04:11:18 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:11:18 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:11:18 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:11:18 d2.evaluation.testing]: \u001b[0mcopypaste: 32.3921,71.1397,24.2985,0.0020,21.0146,39.5564\n",
            "\u001b[32m[01/11 04:11:18 d2.utils.events]: \u001b[0m eta: 0:04:21  iter: 4659  total_loss: 0.5917  loss_cls: 0.1396  loss_box_reg: 0.3993  loss_rpn_cls: 0.01312  loss_rpn_loc: 0.01463    time: 0.7712  last_time: 0.7662  data_time: 0.0311  last_data_time: 0.0214   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:11:26 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:11:26 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:11:26 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:11:26 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:11:26 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:11:26 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:11:26 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:11:26 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:11:27 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0477 s/iter. Eval: 0.0002 s/iter. Total: 0.0489 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 04:11:32 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0016 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 04:11:37 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0015 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 04:11:39 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.412527 (0.051719 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:11:39 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049233 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:11:39 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:11:39 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:11:39 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.315\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.712\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.200\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.208\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.383\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.451\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.470\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.430\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.519\n",
            "\u001b[32m[01/11 04:11:39 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.539 | 71.151 | 20.015 | 0.000 | 20.804 | 38.292 |\n",
            "\u001b[32m[01/11 04:11:39 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.539 |\n",
            "\u001b[32m[01/11 04:11:39 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:11:39 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:11:39 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:11:39 d2.evaluation.testing]: \u001b[0mcopypaste: 31.5388,71.1508,20.0152,0.0000,20.8039,38.2920\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:11:47 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:11:47 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:11:47 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:11:47 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:11:47 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:11:47 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:11:47 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:11:47 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:11:48 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0481 s/iter. Eval: 0.0003 s/iter. Total: 0.0494 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 04:11:53 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0016 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 04:11:58 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0015 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:12:00 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.454456 (0.051894 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:12:00 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049336 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:12:00 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:12:00 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:12:00 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.20s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.318\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.714\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.221\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.204\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.386\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.448\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.463\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.416\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.515\n",
            "\u001b[32m[01/11 04:12:01 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.760 | 71.393 | 22.098 | 0.000 | 20.388 | 38.622 |\n",
            "\u001b[32m[01/11 04:12:01 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.760 |\n",
            "\u001b[32m[01/11 04:12:01 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:12:01 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:12:01 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:12:01 d2.evaluation.testing]: \u001b[0mcopypaste: 31.7595,71.3928,22.0983,0.0000,20.3876,38.6222\n",
            "\u001b[32m[01/11 04:12:01 d2.utils.events]: \u001b[0m eta: 0:04:05  iter: 4679  total_loss: 0.5894  loss_cls: 0.1324  loss_box_reg: 0.3839  loss_rpn_cls: 0.01826  loss_rpn_loc: 0.0188    time: 0.7712  last_time: 0.7003  data_time: 0.0368  last_data_time: 0.0306   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:12:09 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:12:09 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:12:09 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:12:09 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:12:09 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:12:09 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:12:09 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:12:09 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:12:10 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:12:15 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0015 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0509 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 04:12:20 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0016 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 04:12:22 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.430608 (0.051794 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:12:22 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049197 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:12:22 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:12:22 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:12:22 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.341\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.720\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.282\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.220\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.418\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.177\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.469\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.492\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.423\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.560\n",
            "\u001b[32m[01/11 04:12:22 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 34.080 | 71.993 | 28.197 | 0.000 | 22.039 | 41.849 |\n",
            "\u001b[32m[01/11 04:12:22 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 34.080 |\n",
            "\u001b[32m[01/11 04:12:22 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:12:22 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:12:22 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:12:22 d2.evaluation.testing]: \u001b[0mcopypaste: 34.0798,71.9930,28.1967,0.0000,22.0390,41.8488\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:12:30 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:12:30 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:12:30 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:12:30 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:12:30 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:12:30 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:12:30 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:12:30 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:12:31 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0531 s/iter. Eval: 0.0003 s/iter. Total: 0.0543 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:12:36 d2.evaluation.evaluator]: \u001b[0mInference done 104/245. Dataloading: 0.0022 s/iter. Inference: 0.0513 s/iter. Eval: 0.0003 s/iter. Total: 0.0538 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 04:12:41 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0019 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0520 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:12:43 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.632371 (0.052635 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:12:43 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049744 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:12:43 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:12:43 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:12:43 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.321\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.717\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.221\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.209\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.390\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.453\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.481\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.452\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.524\n",
            "\u001b[32m[01/11 04:12:44 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.116 | 71.689 | 22.066 | 0.003 | 20.887 | 39.046 |\n",
            "\u001b[32m[01/11 04:12:44 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.116 |\n",
            "\u001b[32m[01/11 04:12:44 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:12:44 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:12:44 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:12:44 d2.evaluation.testing]: \u001b[0mcopypaste: 32.1161,71.6895,22.0659,0.0025,20.8875,39.0459\n",
            "\u001b[32m[01/11 04:12:44 d2.utils.events]: \u001b[0m eta: 0:03:50  iter: 4699  total_loss: 0.5919  loss_cls: 0.1293  loss_box_reg: 0.391  loss_rpn_cls: 0.02193  loss_rpn_loc: 0.02199    time: 0.7712  last_time: 0.7619  data_time: 0.0304  last_data_time: 0.0226   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:12:52 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:12:52 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:12:52 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:12:52 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:12:52 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:12:52 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:12:52 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:12:52 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:12:53 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0008 s/iter. Inference: 0.0485 s/iter. Eval: 0.0002 s/iter. Total: 0.0496 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 04:12:58 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0016 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0508 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 04:13:03 d2.evaluation.evaluator]: \u001b[0mInference done 209/245. Dataloading: 0.0015 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0508 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 04:13:05 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.358934 (0.051496 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:13:05 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048987 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:13:05 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:13:05 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:13:05 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.306\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.703\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.199\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.196\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.376\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.165\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.435\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.460\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.431\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.502\n",
            "\u001b[32m[01/11 04:13:06 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.648 | 70.281 | 19.886 | 0.000 | 19.617 | 37.632 |\n",
            "\u001b[32m[01/11 04:13:06 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.648 |\n",
            "\u001b[32m[01/11 04:13:06 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:13:06 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:13:06 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:13:06 d2.evaluation.testing]: \u001b[0mcopypaste: 30.6483,70.2806,19.8858,0.0000,19.6167,37.6324\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:13:13 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:13:13 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:13:13 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:13:13 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:13:13 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:13:13 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:13:13 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:13:13 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:13:14 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0536 s/iter. Eval: 0.0003 s/iter. Total: 0.0550 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:13:19 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0018 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 04:13:24 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0018 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0514 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 04:13:26 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.452485 (0.051885 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:13:26 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049105 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:13:26 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:13:26 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:13:26 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.324\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.722\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.239\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.214\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.395\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.170\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.458\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.483\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.448\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.530\n",
            "\u001b[32m[01/11 04:13:27 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.445 | 72.216 | 23.893 | 0.000 | 21.386 | 39.481 |\n",
            "\u001b[32m[01/11 04:13:27 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.445 |\n",
            "\u001b[32m[01/11 04:13:27 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:13:27 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:13:27 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:13:27 d2.evaluation.testing]: \u001b[0mcopypaste: 32.4448,72.2158,23.8927,0.0000,21.3859,39.4808\n",
            "\u001b[32m[01/11 04:13:27 d2.utils.events]: \u001b[0m eta: 0:03:35  iter: 4719  total_loss: 0.6171  loss_cls: 0.1477  loss_box_reg: 0.4259  loss_rpn_cls: 0.01415  loss_rpn_loc: 0.02022    time: 0.7712  last_time: 0.7685  data_time: 0.0320  last_data_time: 0.0231   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:13:35 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:13:35 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:13:35 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:13:35 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:13:35 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:13:35 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:13:35 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:13:35 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:13:36 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0013 s/iter. Inference: 0.0514 s/iter. Eval: 0.0003 s/iter. Total: 0.0530 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:13:41 d2.evaluation.evaluator]: \u001b[0mInference done 111/245. Dataloading: 0.0016 s/iter. Inference: 0.0486 s/iter. Eval: 0.0003 s/iter. Total: 0.0505 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 04:13:46 d2.evaluation.evaluator]: \u001b[0mInference done 199/245. Dataloading: 0.0017 s/iter. Inference: 0.0515 s/iter. Eval: 0.0003 s/iter. Total: 0.0535 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:13:49 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.859492 (0.053581 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:13:49 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050894 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:13:49 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:13:49 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:13:49 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.29s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.323\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.720\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.223\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.216\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.392\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.457\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.489\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.019\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.464\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.529\n",
            "\u001b[32m[01/11 04:13:49 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.326 | 72.024 | 22.252 | 0.007 | 21.559 | 39.213 |\n",
            "\u001b[32m[01/11 04:13:49 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.326 |\n",
            "\u001b[32m[01/11 04:13:49 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:13:49 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:13:49 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:13:49 d2.evaluation.testing]: \u001b[0mcopypaste: 32.3264,72.0242,22.2520,0.0069,21.5592,39.2134\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:13:57 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:13:57 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:13:57 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:13:57 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:13:57 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:13:57 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:13:57 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:13:57 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:13:58 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0530 s/iter. Eval: 0.0003 s/iter. Total: 0.0543 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:14:03 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0017 s/iter. Inference: 0.0506 s/iter. Eval: 0.0003 s/iter. Total: 0.0526 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 04:14:08 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0017 s/iter. Inference: 0.0497 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:14:10 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.534639 (0.052228 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:14:10 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049529 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:14:10 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:14:10 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:14:10 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.314\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.706\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.202\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.207\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.382\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.169\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.449\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.472\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.441\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.515\n",
            "\u001b[32m[01/11 04:14:11 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.384 | 70.573 | 20.231 | 0.001 | 20.709 | 38.174 |\n",
            "\u001b[32m[01/11 04:14:11 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.384 |\n",
            "\u001b[32m[01/11 04:14:11 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:14:11 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:14:11 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:14:11 d2.evaluation.testing]: \u001b[0mcopypaste: 31.3836,70.5732,20.2314,0.0014,20.7093,38.1741\n",
            "\u001b[32m[01/11 04:14:11 d2.utils.events]: \u001b[0m eta: 0:03:19  iter: 4739  total_loss: 0.6257  loss_cls: 0.1509  loss_box_reg: 0.4107  loss_rpn_cls: 0.02458  loss_rpn_loc: 0.01762    time: 0.7712  last_time: 0.7654  data_time: 0.0339  last_data_time: 0.0223   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:14:19 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:14:19 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:14:19 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:14:19 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:14:19 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:14:19 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:14:19 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:14:19 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:14:20 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0480 s/iter. Eval: 0.0003 s/iter. Total: 0.0494 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 04:14:25 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0019 s/iter. Inference: 0.0484 s/iter. Eval: 0.0003 s/iter. Total: 0.0506 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 04:14:30 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0021 s/iter. Inference: 0.0486 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 04:14:32 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.381390 (0.051589 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:14:32 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048537 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:14:32 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:14:32 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:14:32 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.326\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.713\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.238\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.205\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.400\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.456\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.484\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.019\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.452\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.528\n",
            "\u001b[32m[01/11 04:14:32 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.643 | 71.262 | 23.810 | 0.006 | 20.484 | 39.985 |\n",
            "\u001b[32m[01/11 04:14:32 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.643 |\n",
            "\u001b[32m[01/11 04:14:32 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:14:32 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:14:32 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:14:32 d2.evaluation.testing]: \u001b[0mcopypaste: 32.6428,71.2621,23.8104,0.0056,20.4842,39.9855\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:14:40 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:14:40 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:14:40 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:14:40 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:14:40 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:14:40 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:14:40 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:14:40 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:14:41 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0529 s/iter. Eval: 0.0003 s/iter. Total: 0.0541 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:14:46 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0014 s/iter. Inference: 0.0512 s/iter. Eval: 0.0003 s/iter. Total: 0.0530 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 04:14:51 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0016 s/iter. Inference: 0.0504 s/iter. Eval: 0.0003 s/iter. Total: 0.0523 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:14:53 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.670935 (0.052796 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:14:53 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050081 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:14:53 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:14:53 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:14:53 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.325\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.713\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.229\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.203\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.399\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.174\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.457\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.480\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.438\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.531\n",
            "\u001b[32m[01/11 04:14:53 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.462 | 71.271 | 22.939 | 0.001 | 20.327 | 39.900 |\n",
            "\u001b[32m[01/11 04:14:53 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.462 |\n",
            "\u001b[32m[01/11 04:14:53 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:14:53 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:14:53 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:14:53 d2.evaluation.testing]: \u001b[0mcopypaste: 32.4622,71.2706,22.9392,0.0011,20.3270,39.8999\n",
            "\u001b[32m[01/11 04:14:53 d2.utils.events]: \u001b[0m eta: 0:03:04  iter: 4759  total_loss: 0.6443  loss_cls: 0.1437  loss_box_reg: 0.4093  loss_rpn_cls: 0.027  loss_rpn_loc: 0.02235    time: 0.7712  last_time: 0.7038  data_time: 0.0331  last_data_time: 0.0249   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:15:02 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:15:02 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:15:02 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:15:02 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:15:02 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:15:02 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:15:02 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:15:02 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:15:03 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0053 s/iter. Inference: 0.0483 s/iter. Eval: 0.0002 s/iter. Total: 0.0538 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:15:08 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0021 s/iter. Inference: 0.0487 s/iter. Eval: 0.0002 s/iter. Total: 0.0510 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 04:15:13 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0020 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:15:15 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.571142 (0.052380 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:15:15 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049345 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:15:15 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:15:15 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:15:15 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.20s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.307\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.700\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.207\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.196\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.374\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.452\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.421\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.494\n",
            "\u001b[32m[01/11 04:15:16 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.698 | 69.971 | 20.714 | 0.000 | 19.551 | 37.398 |\n",
            "\u001b[32m[01/11 04:15:16 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.698 |\n",
            "\u001b[32m[01/11 04:15:16 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:15:16 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:15:16 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:15:16 d2.evaluation.testing]: \u001b[0mcopypaste: 30.6980,69.9713,20.7142,0.0000,19.5507,37.3981\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:15:23 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:15:23 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:15:23 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:15:23 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:15:23 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:15:23 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:15:23 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:15:23 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:15:24 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0016 s/iter. Inference: 0.0493 s/iter. Eval: 0.0002 s/iter. Total: 0.0511 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 04:15:29 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0014 s/iter. Inference: 0.0508 s/iter. Eval: 0.0002 s/iter. Total: 0.0526 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 04:15:34 d2.evaluation.evaluator]: \u001b[0mInference done 202/245. Dataloading: 0.0015 s/iter. Inference: 0.0506 s/iter. Eval: 0.0002 s/iter. Total: 0.0524 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:15:37 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.746962 (0.053112 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:15:37 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050569 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:15:37 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:15:37 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:15:37 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.322\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.710\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.239\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.206\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.392\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.174\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.459\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.477\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.434\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.529\n",
            "\u001b[32m[01/11 04:15:37 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.177 | 71.020 | 23.880 | 0.000 | 20.594 | 39.248 |\n",
            "\u001b[32m[01/11 04:15:37 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.177 |\n",
            "\u001b[32m[01/11 04:15:37 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:15:37 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:15:37 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:15:37 d2.evaluation.testing]: \u001b[0mcopypaste: 32.1771,71.0195,23.8801,0.0000,20.5943,39.2485\n",
            "\u001b[32m[01/11 04:15:37 d2.utils.events]: \u001b[0m eta: 0:02:48  iter: 4779  total_loss: 0.6126  loss_cls: 0.1456  loss_box_reg: 0.4067  loss_rpn_cls: 0.01603  loss_rpn_loc: 0.01278    time: 0.7712  last_time: 0.7681  data_time: 0.0318  last_data_time: 0.0262   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:15:45 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:15:45 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:15:45 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:15:45 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:15:45 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:15:45 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:15:45 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:15:45 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:15:46 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:15:51 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0020 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 04:15:56 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0020 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:15:58 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.514043 (0.052142 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:15:58 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049156 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:15:58 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:15:58 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:15:58 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.312\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.707\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.219\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.210\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.379\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.170\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.447\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.472\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.452\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.508\n",
            "\u001b[32m[01/11 04:15:59 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.235 | 70.713 | 21.890 | 0.002 | 21.032 | 37.889 |\n",
            "\u001b[32m[01/11 04:15:59 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.235 |\n",
            "\u001b[32m[01/11 04:15:59 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:15:59 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:15:59 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:15:59 d2.evaluation.testing]: \u001b[0mcopypaste: 31.2350,70.7127,21.8897,0.0020,21.0316,37.8885\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:16:06 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:16:06 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:16:06 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:16:06 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:16:06 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:16:06 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:16:06 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:16:06 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:16:07 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0487 s/iter. Eval: 0.0003 s/iter. Total: 0.0499 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 04:16:12 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0022 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 04:16:17 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0021 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:16:20 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.559051 (0.052329 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:16:20 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049356 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:16:20 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:16:20 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:16:20 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.328\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.722\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.247\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.213\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.401\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.464\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.487\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.459\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.530\n",
            "\u001b[32m[01/11 04:16:20 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.773 | 72.197 | 24.675 | 0.002 | 21.263 | 40.084 |\n",
            "\u001b[32m[01/11 04:16:20 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.773 |\n",
            "\u001b[32m[01/11 04:16:20 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:16:20 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:16:20 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:16:20 d2.evaluation.testing]: \u001b[0mcopypaste: 32.7730,72.1969,24.6751,0.0015,21.2633,40.0845\n",
            "\u001b[32m[01/11 04:16:20 d2.utils.events]: \u001b[0m eta: 0:02:33  iter: 4799  total_loss: 0.6218  loss_cls: 0.1617  loss_box_reg: 0.4212  loss_rpn_cls: 0.02004  loss_rpn_loc: 0.02148    time: 0.7711  last_time: 0.7672  data_time: 0.0310  last_data_time: 0.0225   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:16:28 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:16:28 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:16:28 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:16:28 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:16:28 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:16:28 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:16:28 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:16:28 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:16:29 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0014 s/iter. Inference: 0.0485 s/iter. Eval: 0.0002 s/iter. Total: 0.0502 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 04:16:34 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0016 s/iter. Inference: 0.0503 s/iter. Eval: 0.0003 s/iter. Total: 0.0523 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 04:16:40 d2.evaluation.evaluator]: \u001b[0mInference done 202/245. Dataloading: 0.0019 s/iter. Inference: 0.0524 s/iter. Eval: 0.0003 s/iter. Total: 0.0546 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:16:42 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:13.135432 (0.054731 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:16:42 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.052016 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:16:42 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:16:42 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:16:42 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.340\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.722\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.287\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.213\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.418\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.180\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.471\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.492\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.441\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.550\n",
            "\u001b[32m[01/11 04:16:42 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.989 | 72.169 | 28.651 | 0.000 | 21.342 | 41.772 |\n",
            "\u001b[32m[01/11 04:16:42 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.989 |\n",
            "\u001b[32m[01/11 04:16:42 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:16:42 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:16:42 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:16:42 d2.evaluation.testing]: \u001b[0mcopypaste: 33.9885,72.1685,28.6513,0.0000,21.3420,41.7718\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:16:50 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:16:50 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:16:50 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:16:50 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:16:50 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:16:50 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:16:50 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:16:50 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:16:51 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0008 s/iter. Inference: 0.0491 s/iter. Eval: 0.0002 s/iter. Total: 0.0502 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 04:16:56 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0017 s/iter. Inference: 0.0487 s/iter. Eval: 0.0002 s/iter. Total: 0.0507 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 04:17:01 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0018 s/iter. Inference: 0.0490 s/iter. Eval: 0.0002 s/iter. Total: 0.0511 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 04:17:03 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.413492 (0.051723 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:17:03 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048943 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:17:03 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:17:03 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:17:03 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.21s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.309\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.707\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.206\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.210\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.373\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.446\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.465\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.437\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.506\n",
            "\u001b[32m[01/11 04:17:03 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.934 | 70.711 | 20.644 | 0.000 | 20.977 | 37.251 |\n",
            "\u001b[32m[01/11 04:17:03 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.934 |\n",
            "\u001b[32m[01/11 04:17:03 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:17:03 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:17:03 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:17:03 d2.evaluation.testing]: \u001b[0mcopypaste: 30.9339,70.7113,20.6438,0.0000,20.9775,37.2513\n",
            "\u001b[32m[01/11 04:17:03 d2.utils.events]: \u001b[0m eta: 0:02:18  iter: 4819  total_loss: 0.608  loss_cls: 0.1395  loss_box_reg: 0.3836  loss_rpn_cls: 0.01932  loss_rpn_loc: 0.01593    time: 0.7711  last_time: 0.7697  data_time: 0.0319  last_data_time: 0.0240   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:17:12 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:17:12 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:17:12 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:17:12 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:17:12 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:17:12 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:17:12 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:17:12 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:17:13 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0488 s/iter. Eval: 0.0002 s/iter. Total: 0.0500 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 04:17:18 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0016 s/iter. Inference: 0.0505 s/iter. Eval: 0.0003 s/iter. Total: 0.0525 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 04:17:23 d2.evaluation.evaluator]: \u001b[0mInference done 202/245. Dataloading: 0.0016 s/iter. Inference: 0.0506 s/iter. Eval: 0.0002 s/iter. Total: 0.0526 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:17:25 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.703210 (0.052930 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:17:25 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050202 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:17:25 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:17:25 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:17:25 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.19s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.318\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.703\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.222\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.211\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.385\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.454\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.471\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.428\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.521\n",
            "\u001b[32m[01/11 04:17:25 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.800 | 70.315 | 22.226 | 0.000 | 21.113 | 38.492 |\n",
            "\u001b[32m[01/11 04:17:25 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.800 |\n",
            "\u001b[32m[01/11 04:17:25 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:17:25 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:17:25 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:17:25 d2.evaluation.testing]: \u001b[0mcopypaste: 31.7995,70.3150,22.2265,0.0000,21.1126,38.4923\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:17:33 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:17:33 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:17:33 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:17:33 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:17:33 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:17:33 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:17:33 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:17:33 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:17:34 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0032 s/iter. Inference: 0.0501 s/iter. Eval: 0.0002 s/iter. Total: 0.0536 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:17:39 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0017 s/iter. Inference: 0.0506 s/iter. Eval: 0.0003 s/iter. Total: 0.0527 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 04:17:44 d2.evaluation.evaluator]: \u001b[0mInference done 203/245. Dataloading: 0.0018 s/iter. Inference: 0.0501 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:17:47 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.697388 (0.052906 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:17:47 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050231 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:17:47 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:17:47 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:17:47 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.20s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.311\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.699\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.205\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.205\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.381\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.464\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.420\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.515\n",
            "\u001b[32m[01/11 04:17:47 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.067 | 69.882 | 20.457 | 0.003 | 20.546 | 38.076 |\n",
            "\u001b[32m[01/11 04:17:47 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.067 |\n",
            "\u001b[32m[01/11 04:17:47 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:17:47 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:17:47 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:17:47 d2.evaluation.testing]: \u001b[0mcopypaste: 31.0674,69.8823,20.4567,0.0026,20.5461,38.0755\n",
            "\u001b[32m[01/11 04:17:47 d2.utils.events]: \u001b[0m eta: 0:02:02  iter: 4839  total_loss: 0.5618  loss_cls: 0.1395  loss_box_reg: 0.3697  loss_rpn_cls: 0.01596  loss_rpn_loc: 0.02062    time: 0.7711  last_time: 0.7758  data_time: 0.0334  last_data_time: 0.0260   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:17:55 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:17:55 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:17:55 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:17:55 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:17:55 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:17:55 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:17:55 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:17:55 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:17:56 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0507 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 04:18:01 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0017 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 04:18:06 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0018 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0510 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 04:18:08 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.403162 (0.051680 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:18:08 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048952 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:18:08 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:18:08 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:18:08 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.21s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.320\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.708\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.213\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.210\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.391\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.170\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.451\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.469\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.422\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.524\n",
            "\u001b[32m[01/11 04:18:09 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.980 | 70.848 | 21.270 | 0.000 | 21.035 | 39.117 |\n",
            "\u001b[32m[01/11 04:18:09 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.980 |\n",
            "\u001b[32m[01/11 04:18:09 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:18:09 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:18:09 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:18:09 d2.evaluation.testing]: \u001b[0mcopypaste: 31.9801,70.8480,21.2704,0.0000,21.0348,39.1165\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:18:16 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:18:16 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:18:16 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:18:16 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:18:16 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:18:16 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:18:16 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:18:16 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:18:17 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0016 s/iter. Inference: 0.0497 s/iter. Eval: 0.0002 s/iter. Total: 0.0515 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:18:22 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0024 s/iter. Inference: 0.0487 s/iter. Eval: 0.0002 s/iter. Total: 0.0515 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 04:18:27 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0019 s/iter. Inference: 0.0493 s/iter. Eval: 0.0002 s/iter. Total: 0.0515 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 04:18:29 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.555029 (0.052313 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:18:29 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049438 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:18:29 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:18:29 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:18:29 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.337\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.729\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.259\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.215\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.416\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.178\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.466\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.484\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.422\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.548\n",
            "\u001b[32m[01/11 04:18:30 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.728 | 72.939 | 25.937 | 0.000 | 21.505 | 41.557 |\n",
            "\u001b[32m[01/11 04:18:30 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.728 |\n",
            "\u001b[32m[01/11 04:18:30 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:18:30 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:18:30 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:18:30 d2.evaluation.testing]: \u001b[0mcopypaste: 33.7278,72.9386,25.9369,0.0000,21.5049,41.5573\n",
            "\u001b[32m[01/11 04:18:30 d2.utils.events]: \u001b[0m eta: 0:01:47  iter: 4859  total_loss: 0.5635  loss_cls: 0.1398  loss_box_reg: 0.39  loss_rpn_cls: 0.02021  loss_rpn_loc: 0.01365    time: 0.7710  last_time: 0.6966  data_time: 0.0328  last_data_time: 0.0233   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:18:38 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:18:38 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:18:38 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:18:38 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:18:38 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:18:38 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:18:38 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:18:38 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:18:39 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0498 s/iter. Eval: 0.0002 s/iter. Total: 0.0512 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 04:18:44 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0016 s/iter. Inference: 0.0497 s/iter. Eval: 0.0002 s/iter. Total: 0.0517 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 04:18:49 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0017 s/iter. Inference: 0.0498 s/iter. Eval: 0.0002 s/iter. Total: 0.0519 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:18:51 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.580613 (0.052419 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:18:51 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049724 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:18:51 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:18:51 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:18:51 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.20s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.338\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.723\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.269\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.219\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.415\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.177\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.466\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.484\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.416\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.551\n",
            "\u001b[32m[01/11 04:18:52 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.763 | 72.258 | 26.884 | 0.000 | 21.859 | 41.456 |\n",
            "\u001b[32m[01/11 04:18:52 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.763 |\n",
            "\u001b[32m[01/11 04:18:52 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:18:52 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:18:52 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:18:52 d2.evaluation.testing]: \u001b[0mcopypaste: 33.7629,72.2580,26.8839,0.0000,21.8595,41.4557\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:18:59 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:18:59 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:18:59 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:18:59 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:18:59 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:18:59 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:18:59 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:18:59 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:19:00 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0015 s/iter. Inference: 0.0490 s/iter. Eval: 0.0003 s/iter. Total: 0.0507 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 04:19:05 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0018 s/iter. Inference: 0.0487 s/iter. Eval: 0.0003 s/iter. Total: 0.0508 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 04:19:10 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0019 s/iter. Inference: 0.0487 s/iter. Eval: 0.0003 s/iter. Total: 0.0509 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 04:19:13 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.366618 (0.051528 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:19:13 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048703 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:19:13 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:19:13 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:19:13 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.311\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.706\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.197\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.197\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.381\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.462\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.431\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.506\n",
            "\u001b[32m[01/11 04:19:13 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.072 | 70.617 | 19.741 | 0.001 | 19.672 | 38.059 |\n",
            "\u001b[32m[01/11 04:19:13 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.072 |\n",
            "\u001b[32m[01/11 04:19:13 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:19:13 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:19:13 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:19:13 d2.evaluation.testing]: \u001b[0mcopypaste: 31.0725,70.6167,19.7409,0.0009,19.6719,38.0589\n",
            "\u001b[32m[01/11 04:19:13 d2.utils.events]: \u001b[0m eta: 0:01:32  iter: 4879  total_loss: 0.606  loss_cls: 0.1628  loss_box_reg: 0.4133  loss_rpn_cls: 0.01407  loss_rpn_loc: 0.01367    time: 0.7710  last_time: 0.8001  data_time: 0.0350  last_data_time: 0.0432   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:19:21 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:19:21 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:19:21 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:19:21 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:19:21 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:19:21 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:19:21 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:19:21 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:19:22 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0503 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 04:19:27 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0016 s/iter. Inference: 0.0494 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 04:19:32 d2.evaluation.evaluator]: \u001b[0mInference done 208/245. Dataloading: 0.0017 s/iter. Inference: 0.0490 s/iter. Eval: 0.0002 s/iter. Total: 0.0510 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 04:19:34 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.481512 (0.052006 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:19:34 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049331 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:19:34 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:19:34 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:19:34 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.703\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.225\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.198\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.390\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.169\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.449\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.465\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.426\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.514\n",
            "\u001b[32m[01/11 04:19:35 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.663 | 70.335 | 22.475 | 0.000 | 19.836 | 38.964 |\n",
            "\u001b[32m[01/11 04:19:35 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.663 |\n",
            "\u001b[32m[01/11 04:19:35 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:19:35 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:19:35 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:19:35 d2.evaluation.testing]: \u001b[0mcopypaste: 31.6627,70.3353,22.4751,0.0000,19.8365,38.9643\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:19:42 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:19:42 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:19:42 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:19:42 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:19:42 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:19:42 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:19:42 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:19:42 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:19:44 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0502 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:19:49 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0022 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 04:19:54 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0020 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:19:56 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.526674 (0.052194 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:19:56 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049125 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:19:56 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:19:56 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:19:56 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.43s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.327\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.710\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.248\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.199\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.406\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.178\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.456\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.477\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.421\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.536\n",
            "\u001b[32m[01/11 04:19:57 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.677 | 70.960 | 24.838 | 0.005 | 19.942 | 40.587 |\n",
            "\u001b[32m[01/11 04:19:57 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.677 |\n",
            "\u001b[32m[01/11 04:19:57 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:19:57 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:19:57 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:19:57 d2.evaluation.testing]: \u001b[0mcopypaste: 32.6775,70.9604,24.8377,0.0051,19.9422,40.5875\n",
            "\u001b[32m[01/11 04:19:57 d2.utils.events]: \u001b[0m eta: 0:01:16  iter: 4899  total_loss: 0.6022  loss_cls: 0.1555  loss_box_reg: 0.4089  loss_rpn_cls: 0.0144  loss_rpn_loc: 0.01544    time: 0.7711  last_time: 0.7651  data_time: 0.0319  last_data_time: 0.0266   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:20:05 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:20:05 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:20:05 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:20:05 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:20:05 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:20:05 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:20:05 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:20:05 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:20:06 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0012 s/iter. Inference: 0.0508 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:20:11 d2.evaluation.evaluator]: \u001b[0mInference done 111/245. Dataloading: 0.0014 s/iter. Inference: 0.0485 s/iter. Eval: 0.0003 s/iter. Total: 0.0502 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 04:20:16 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0016 s/iter. Inference: 0.0493 s/iter. Eval: 0.0003 s/iter. Total: 0.0512 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 04:20:18 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.476192 (0.051984 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:20:18 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049188 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:20:18 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:20:18 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:20:18 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.333\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.719\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.251\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.214\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.409\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.178\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.467\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.489\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.431\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.550\n",
            "\u001b[32m[01/11 04:20:18 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.323 | 71.941 | 25.100 | 0.012 | 21.420 | 40.911 |\n",
            "\u001b[32m[01/11 04:20:18 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.323 |\n",
            "\u001b[32m[01/11 04:20:18 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:20:18 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:20:18 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:20:18 d2.evaluation.testing]: \u001b[0mcopypaste: 33.3230,71.9406,25.1002,0.0118,21.4201,40.9113\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:20:26 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:20:26 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:20:26 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:20:26 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:20:26 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:20:26 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:20:26 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:20:26 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:20:27 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0480 s/iter. Eval: 0.0003 s/iter. Total: 0.0493 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 04:20:32 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0020 s/iter. Inference: 0.0482 s/iter. Eval: 0.0003 s/iter. Total: 0.0505 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 04:20:37 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0019 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0513 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:20:39 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.494479 (0.052060 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:20:39 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049225 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:20:39 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:20:39 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:20:39 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.326\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.718\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.230\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.207\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.402\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.176\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.459\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.488\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.541\n",
            "\u001b[32m[01/11 04:20:39 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 32.638 | 71.770 | 23.026 | 0.009 | 20.740 | 40.235 |\n",
            "\u001b[32m[01/11 04:20:39 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 32.638 |\n",
            "\u001b[32m[01/11 04:20:39 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:20:39 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:20:39 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:20:39 d2.evaluation.testing]: \u001b[0mcopypaste: 32.6383,71.7698,23.0261,0.0087,20.7400,40.2353\n",
            "\u001b[32m[01/11 04:20:39 d2.utils.events]: \u001b[0m eta: 0:01:01  iter: 4919  total_loss: 0.6305  loss_cls: 0.1531  loss_box_reg: 0.4189  loss_rpn_cls: 0.01943  loss_rpn_loc: 0.02603    time: 0.7710  last_time: 0.7744  data_time: 0.0359  last_data_time: 0.0295   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:20:48 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:20:48 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:20:48 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:20:48 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:20:48 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:20:48 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:20:48 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:20:48 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:20:49 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0030 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.0525 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:20:54 d2.evaluation.evaluator]: \u001b[0mInference done 106/245. Dataloading: 0.0020 s/iter. Inference: 0.0506 s/iter. Eval: 0.0003 s/iter. Total: 0.0529 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 04:20:59 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0019 s/iter. Inference: 0.0498 s/iter. Eval: 0.0003 s/iter. Total: 0.0521 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:21:01 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.603686 (0.052515 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:21:01 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049622 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:21:01 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:21:01 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:21:01 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.318\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.707\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.213\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.202\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.392\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.171\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.455\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.476\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.436\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.526\n",
            "\u001b[32m[01/11 04:21:01 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.770 | 70.728 | 21.262 | 0.007 | 20.218 | 39.151 |\n",
            "\u001b[32m[01/11 04:21:01 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.770 |\n",
            "\u001b[32m[01/11 04:21:01 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:21:01 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:21:01 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:21:01 d2.evaluation.testing]: \u001b[0mcopypaste: 31.7702,70.7281,21.2621,0.0072,20.2182,39.1510\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:21:09 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:21:09 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:21:09 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:21:09 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:21:09 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:21:09 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:21:09 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:21:09 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:21:10 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0033 s/iter. Inference: 0.0489 s/iter. Eval: 0.0003 s/iter. Total: 0.0524 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:21:15 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0017 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 04:21:20 d2.evaluation.evaluator]: \u001b[0mInference done 207/245. Dataloading: 0.0017 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0515 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 04:21:22 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.551849 (0.052299 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:21:22 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049449 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:21:22 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:21:22 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:21:22 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.20s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.335\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.722\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.261\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.215\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.412\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.174\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.462\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.478\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.416\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.541\n",
            "\u001b[32m[01/11 04:21:23 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.485 | 72.218 | 26.099 | 0.000 | 21.510 | 41.186 |\n",
            "\u001b[32m[01/11 04:21:23 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.485 |\n",
            "\u001b[32m[01/11 04:21:23 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:21:23 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:21:23 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:21:23 d2.evaluation.testing]: \u001b[0mcopypaste: 33.4848,72.2177,26.0985,0.0000,21.5103,41.1860\n",
            "\u001b[32m[01/11 04:21:23 d2.utils.events]: \u001b[0m eta: 0:00:46  iter: 4939  total_loss: 0.5817  loss_cls: 0.1301  loss_box_reg: 0.3641  loss_rpn_cls: 0.02089  loss_rpn_loc: 0.01993    time: 0.7710  last_time: 0.7677  data_time: 0.0328  last_data_time: 0.0230   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:21:31 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:21:31 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:21:31 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:21:31 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:21:31 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:21:31 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:21:31 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:21:31 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:21:32 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0024 s/iter. Inference: 0.0491 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:21:37 d2.evaluation.evaluator]: \u001b[0mInference done 107/245. Dataloading: 0.0023 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.0522 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 04:21:42 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0020 s/iter. Inference: 0.0495 s/iter. Eval: 0.0003 s/iter. Total: 0.0519 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:21:44 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.558836 (0.052328 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:21:44 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049268 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:21:44 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:21:44 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:21:44 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.302\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.699\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.191\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.196\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.369\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.161\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.434\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.455\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.425\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.497\n",
            "\u001b[32m[01/11 04:21:44 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.204 | 69.907 | 19.127 | 0.000 | 19.554 | 36.911 |\n",
            "\u001b[32m[01/11 04:21:44 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.204 |\n",
            "\u001b[32m[01/11 04:21:44 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:21:44 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:21:44 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:21:44 d2.evaluation.testing]: \u001b[0mcopypaste: 30.2037,69.9074,19.1274,0.0000,19.5537,36.9113\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:21:52 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:21:52 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:21:52 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:21:52 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:21:52 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:21:52 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:21:52 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:21:52 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:21:53 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0040 s/iter. Inference: 0.0500 s/iter. Eval: 0.0003 s/iter. Total: 0.0543 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:21:58 d2.evaluation.evaluator]: \u001b[0mInference done 104/245. Dataloading: 0.0017 s/iter. Inference: 0.0519 s/iter. Eval: 0.0003 s/iter. Total: 0.0539 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 04:22:03 d2.evaluation.evaluator]: \u001b[0mInference done 201/245. Dataloading: 0.0017 s/iter. Inference: 0.0507 s/iter. Eval: 0.0003 s/iter. Total: 0.0528 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:22:06 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.780389 (0.053252 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:22:06 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.050564 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:22:06 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:22:06 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:22:06 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.318\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.705\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.223\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.203\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.392\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.170\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.456\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.477\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.434\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.528\n",
            "\u001b[32m[01/11 04:22:06 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.794 | 70.451 | 22.252 | 0.004 | 20.260 | 39.156 |\n",
            "\u001b[32m[01/11 04:22:06 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.794 |\n",
            "\u001b[32m[01/11 04:22:06 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:22:06 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:22:06 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:22:06 d2.evaluation.testing]: \u001b[0mcopypaste: 31.7941,70.4511,22.2516,0.0037,20.2602,39.1555\n",
            "\u001b[32m[01/11 04:22:06 d2.utils.events]: \u001b[0m eta: 0:00:30  iter: 4959  total_loss: 0.6018  loss_cls: 0.1509  loss_box_reg: 0.4074  loss_rpn_cls: 0.01578  loss_rpn_loc: 0.01636    time: 0.7710  last_time: 0.7739  data_time: 0.0328  last_data_time: 0.0280   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:22:14 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:22:14 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:22:14 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:22:14 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:22:14 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:22:14 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:22:14 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:22:14 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:22:15 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0010 s/iter. Inference: 0.0494 s/iter. Eval: 0.0002 s/iter. Total: 0.0506 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 04:22:21 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0014 s/iter. Inference: 0.0500 s/iter. Eval: 0.0003 s/iter. Total: 0.0518 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 04:22:26 d2.evaluation.evaluator]: \u001b[0mInference done 204/245. Dataloading: 0.0019 s/iter. Inference: 0.0499 s/iter. Eval: 0.0002 s/iter. Total: 0.0521 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:22:28 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.640839 (0.052670 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:22:28 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049759 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:22:28 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:22:28 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:22:28 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.21s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.314\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.705\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.213\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.204\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.385\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.170\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.450\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.468\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.423\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.520\n",
            "\u001b[32m[01/11 04:22:28 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.361 | 70.543 | 21.322 | 0.000 | 20.357 | 38.454 |\n",
            "\u001b[32m[01/11 04:22:28 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.361 |\n",
            "\u001b[32m[01/11 04:22:28 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:22:28 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:22:28 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:22:28 d2.evaluation.testing]: \u001b[0mcopypaste: 31.3606,70.5432,21.3220,0.0000,20.3566,38.4536\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:22:36 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:22:36 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:22:36 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:22:36 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:22:36 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:22:36 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:22:36 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:22:36 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:22:37 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0011 s/iter. Inference: 0.0490 s/iter. Eval: 0.0002 s/iter. Total: 0.0504 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 04:22:42 d2.evaluation.evaluator]: \u001b[0mInference done 109/245. Dataloading: 0.0018 s/iter. Inference: 0.0491 s/iter. Eval: 0.0002 s/iter. Total: 0.0513 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 04:22:47 d2.evaluation.evaluator]: \u001b[0mInference done 206/245. Dataloading: 0.0022 s/iter. Inference: 0.0491 s/iter. Eval: 0.0002 s/iter. Total: 0.0516 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:22:49 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.578373 (0.052410 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:22:49 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049196 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:22:49 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:22:49 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:22:49 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.311\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.701\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.206\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.202\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.382\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.449\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.467\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.425\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.517\n",
            "\u001b[32m[01/11 04:22:49 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 31.108 | 70.057 | 20.586 | 0.002 | 20.156 | 38.224 |\n",
            "\u001b[32m[01/11 04:22:49 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 31.108 |\n",
            "\u001b[32m[01/11 04:22:49 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:22:49 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:22:49 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:22:49 d2.evaluation.testing]: \u001b[0mcopypaste: 31.1084,70.0573,20.5855,0.0023,20.1559,38.2243\n",
            "\u001b[32m[01/11 04:22:49 d2.utils.events]: \u001b[0m eta: 0:00:15  iter: 4979  total_loss: 0.5926  loss_cls: 0.1363  loss_box_reg: 0.3768  loss_rpn_cls: 0.02589  loss_rpn_loc: 0.03065    time: 0.7710  last_time: 0.7758  data_time: 0.0326  last_data_time: 0.0335   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:22:58 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:22:58 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:22:58 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:22:58 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:22:58 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:22:58 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:22:58 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:22:58 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:22:59 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0009 s/iter. Inference: 0.0494 s/iter. Eval: 0.0002 s/iter. Total: 0.0505 s/iter. ETA=0:00:11\n",
            "\u001b[32m[01/11 04:23:04 d2.evaluation.evaluator]: \u001b[0mInference done 110/245. Dataloading: 0.0019 s/iter. Inference: 0.0484 s/iter. Eval: 0.0002 s/iter. Total: 0.0506 s/iter. ETA=0:00:06\n",
            "\u001b[32m[01/11 04:23:09 d2.evaluation.evaluator]: \u001b[0mInference done 209/245. Dataloading: 0.0019 s/iter. Inference: 0.0486 s/iter. Eval: 0.0002 s/iter. Total: 0.0507 s/iter. ETA=0:00:01\n",
            "\u001b[32m[01/11 04:23:11 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.314929 (0.051312 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:23:11 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.048435 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:23:11 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:23:11 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:23:11 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.21s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.337\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.727\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.260\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.214\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.417\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.175\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.473\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.490\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.417\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.560\n",
            "\u001b[32m[01/11 04:23:11 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 33.725 | 72.673 | 26.043 | 0.000 | 21.417 | 41.655 |\n",
            "\u001b[32m[01/11 04:23:11 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 33.725 |\n",
            "\u001b[32m[01/11 04:23:11 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:23:11 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:23:11 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:23:11 d2.evaluation.testing]: \u001b[0mcopypaste: 33.7252,72.6729,26.0431,0.0000,21.4167,41.6554\n",
            "Saving checkpoint to ./output/model_0004999.pth\n",
            "Saving checkpoint to ./output/model_final.pth\n",
            "\u001b[32m[01/11 04:23:20 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 4999  total_loss: 0.6046  loss_cls: 0.1545  loss_box_reg: 0.3921  loss_rpn_cls: 0.01801  loss_rpn_loc: 0.01374    time: 0.7710  last_time: 0.7813  data_time: 0.0306  last_data_time: 0.0377   lr: 0.00025  max_mem: 9595M\n",
            "\u001b[32m[01/11 04:23:21 d2.engine.hooks]: \u001b[0mOverall training speed: 4998 iterations in 1:04:13 (0.7711 s / it)\n",
            "\u001b[32m[01/11 04:23:21 d2.engine.hooks]: \u001b[0mTotal training time: 3:01:36 (1:57:23 on hooks)\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/11 04:23:21 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[01/11 04:23:21 d2.data.datasets.coco]: \u001b[0mLoaded 245 images in COCO format from /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1/test/_annotations.coco.json\n",
            "\u001b[32m[01/11 04:23:21 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[01/11 04:23:21 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/11 04:23:21 d2.data.common]: \u001b[0mSerializing 245 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/11 04:23:21 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
            "\u001b[32m[01/11 04:23:21 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
            "\u001b[32m[01/11 04:23:21 d2.evaluation.evaluator]: \u001b[0mStart inference on 245 batches\n",
            "\u001b[32m[01/11 04:23:22 d2.evaluation.evaluator]: \u001b[0mInference done 11/245. Dataloading: 0.0033 s/iter. Inference: 0.0480 s/iter. Eval: 0.0003 s/iter. Total: 0.0516 s/iter. ETA=0:00:12\n",
            "\u001b[32m[01/11 04:23:27 d2.evaluation.evaluator]: \u001b[0mInference done 108/245. Dataloading: 0.0018 s/iter. Inference: 0.0495 s/iter. Eval: 0.0002 s/iter. Total: 0.0516 s/iter. ETA=0:00:07\n",
            "\u001b[32m[01/11 04:23:32 d2.evaluation.evaluator]: \u001b[0mInference done 205/245. Dataloading: 0.0016 s/iter. Inference: 0.0497 s/iter. Eval: 0.0002 s/iter. Total: 0.0517 s/iter. ETA=0:00:02\n",
            "\u001b[32m[01/11 04:23:34 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.549879 (0.052291 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:23:34 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:11 (0.049666 s / iter per device, on 1 devices)\n",
            "\u001b[32m[01/11 04:23:34 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[01/11 04:23:34 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/weed_val/coco_instances_results.json\n",
            "\u001b[32m[01/11 04:23:34 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.302\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.701\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.194\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.190\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.374\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.166\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.437\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.455\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.415\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.504\n",
            "\u001b[32m[01/11 04:23:35 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 30.224 | 70.099 | 19.401 | 0.000 | 19.037 | 37.415 |\n",
            "\u001b[32m[01/11 04:23:35 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category    | AP   | category       | AP     |\n",
            "|:------------|:-----|:---------------|:-------|\n",
            "| grass-weeds | nan  | 0 ridderzuring | 30.224 |\n",
            "\u001b[32m[01/11 04:23:35 d2.engine.defaults]: \u001b[0mEvaluation results for weed_val in csv format:\n",
            "\u001b[32m[01/11 04:23:35 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
            "\u001b[32m[01/11 04:23:35 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
            "\u001b[32m[01/11 04:23:35 d2.evaluation.testing]: \u001b[0mcopypaste: 30.2238,70.0994,19.4009,0.0000,19.0372,37.4155\n"
          ]
        }
      ],
      "source": [
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.DATASETS.TRAIN = (\"weed_train\",)\n",
        "cfg.DATASETS.TEST = (\"weed_val\",)\n",
        "cfg.DATALOADER.NUM_WORKERS = 4\n",
        "\n",
        "# Load pre-trained weights\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
        "\n",
        "# Set number of classes (2 classes: grass-weeds and ridderzuring)\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 2\n",
        "\n",
        "# Training parameters\n",
        "cfg.SOLVER.IMS_PER_BATCH = 8\n",
        "cfg.SOLVER.BASE_LR = 0.00025\n",
        "cfg.SOLVER.MAX_ITER = 5000\n",
        "cfg.SOLVER.STEPS = []  # do not decay learning rate\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.05\n",
        "cfg.TEST.DETECTIONS_PER_IMAGE = 100\n",
        "\n",
        "# Evaluation parameters\n",
        "cfg.TEST.EVAL_PERIOD = 10  # Evaluate every 50 iterations\n",
        "cfg.VIS_PERIOD = 10  # Visualize predictions every 50 iterations\n",
        "\n",
        "# Initialize the output directory\n",
        "cfg.OUTPUT_DIR = \"./output\"\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = MyTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training complete\n"
          ]
        }
      ],
      "source": [
        "print(\"Training complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'cfg' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[43mcfg\u001b[49m\u001b[38;5;241m.\u001b[39mOUTPUT_DIR)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cfg' is not defined"
          ]
        }
      ],
      "source": [
        "print (cfg.OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: graphs/bbox_AP_over_iterations.png\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: graphs/bbox_AP50_over_iterations.png\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: graphs/bbox_AP75_over_iterations.png\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAArMAAAHWCAYAAABkNgFvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/OQEPoAAAACXBIWXMAAA9hAAAPYQGoP6dpAACVTElEQVR4nOzdd3xT9foH8E92utK9W2gpe5WNZQgow4I4rwsnThQnThwgLq56rxv15wLHdYuggkrZe1NZZba0ULr3TNLk/P44OadJF91p2s/79bqva5OTnG/yTemT5zzf56sQBEEAEREREZELUjp7AEREREREzcVgloiIiIhcFoNZIiIiInJZDGaJiIiIyGUxmCUiIiIil8VgloiIiIhcFoNZIiIiInJZDGaJiIiIyGUxmCUiIiIil8Vglohc0pkzZ6BQKLBs2bI2fQy5Ns45UefHYJaImmz79u148cUXUVhY2Ozn+PDDDxlguJiNGzdCoVDg559/lm9rjc9Ca/j222/xzjvvOHUMROQcDGaJqMm2b9+ORYsWMZilVvkstIb6gtnu3bujoqICt956a/sPiojaBYNZIiKSlZWVOXsIAIDy8vJWeR6FQgG9Xg+VStUqz0dEHQ+DWSJqkhdffBFPPvkkACA6OhoKhQIKhQJnzpwBAFRVVeHll19GTEwMdDodoqKi8Oyzz8JoNMrPERUVhSNHjmDTpk3y4ydOnAgAyM/PxxNPPIFBgwbB09MTBoMB8fHx+Oeff9rsNa1fvx7jx4+Hh4cHfHx8cOWVVyIpKcnhmJKSEjz66KOIioqCTqdDUFAQpkyZgv3798vHnDx5Etdeey1CQkKg1+sRERGBG2+8EUVFRRccw08//YThw4fDzc0NAQEBuOWWW5Ceni7f/5///AcKhQKpqam1Hjt//nxotVoUFBTIt+3atQuXXXYZvL294e7ujgkTJmDbtm0Oj3vxxRehUChw9OhRzJo1C76+vhg3blyj37cLfRYA4JtvvpFfl5+fH2688UacPXvW4XkmTpyIgQMHYt++fbj44ovh7u6OZ599FgCwcuVKzJgxA2FhYdDpdIiJicHLL78Mi8Xi8PhVq1YhNTVVHkNUVBSA+mtmGzPn0vtz6tQp3HHHHfDx8YG3tzdmz55dK9hOSEjAuHHj4OPjA09PT/Tp00d+DUTUttTOHgARuZZrrrkGJ06cwHfffYe3334bAQEBAIDAwEAAwN13340vv/wS//rXv/D4449j165dWLx4MZKSkvDrr78CAN555x089NBD8PT0xHPPPQcACA4OBgAkJydjxYoVuO666xAdHY2srCz83//9HyZMmICjR48iLCysVV/P2rVrER8fjx49euDFF19ERUUF3n//fYwdOxb79++Xg6I5c+bg559/xoMPPoj+/fsjLy8PW7duRVJSEoYNGwaTyYRp06bBaDTioYceQkhICNLT0/HHH3+gsLAQ3t7e9Y5h2bJlmD17NkaOHInFixcjKysL7777LrZt24YDBw7Ax8cH119/PZ566in8+OOPcgAp+fHHHzF16lT4+voCEAO1+Ph4DB8+HAsXLoRSqcTSpUtxySWXYMuWLRg1apTD46+77jr06tULr732GgRBaPR7d6HPwquvvooXXngB119/Pe6++27k5OTg/fffx8UXXyy/LkleXh7i4+Nx44034pZbbpE/D8uWLYOnpyfmzZsHT09PrF+/HgsWLEBxcTHefPNNAMBzzz2HoqIinDt3Dm+//TYAwNPTs95xN3bOJddffz2io6OxePFi7N+/H5999hmCgoLw+uuvAwCOHDmCyy+/HIMHD8ZLL70EnU6HU6dO1fryQERtRCAiaqI333xTACCkpKQ43J6YmCgAEO6++26H25944gkBgLB+/Xr5tgEDBggTJkyo9dyVlZWCxWJxuC0lJUXQ6XTCSy+95HAbAGHp0qWNHnddjxkyZIgQFBQk5OXlybf9888/glKpFG677Tb5Nm9vb2Hu3Ln1PveBAwcEAMJPP/3U6PEIgiCYTCYhKChIGDhwoFBRUSHf/scffwgAhAULFsi3xcXFCcOHD3d4/O7duwUAwldffSUIgiBYrVahV69ewrRp0wSr1SofV15eLkRHRwtTpkyRb1u4cKEAQLjpppsaNdYNGzbUeo31fRbOnDkjqFQq4dVXX3W4/dChQ4JarXa4fcKECQIA4eOPP651zvLy8lq33XfffYK7u7tQWVkp3zZjxgyhe/futY5tyZxL78+dd97p8JxXX3214O/vL//89ttvCwCEnJycWucnorbHMgMiajWrV68GAMybN8/h9scffxwAsGrVqgs+h06ng1Ip/tNksViQl5cnX7a1v6TfGjIyMpCYmIg77rgDfn5+8u2DBw/GlClT5NcDAD4+Pti1axfOnz9f53NJmde///67SfWee/fuRXZ2Nh544AHo9Xr59hkzZqBv374O79kNN9yAffv24fTp0/JtP/zwA3Q6Ha688koAQGJiIk6ePIlZs2YhLy8Pubm5yM3NRVlZGS699FJs3rwZVqvVYQxz5sxp9Hgba/ny5bBarbj++uvlMeTm5iIkJAS9evXChg0bHI7X6XSYPXt2redxc3OT/7ukpAS5ubkYP348ysvLcezYsSaPqylzLqn5/owfPx55eXkoLi4GADnDvHLlylrvLRG1PQazRNRqUlNToVQq0bNnT4fbQ0JC4OPjU2e9Z01WqxVvv/02evXqBZ1Oh4CAAAQGBuLgwYONqj1t6ngBoE+fPrXu69evnxwEAsAbb7yBw4cPIzIyEqNGjcKLL76I5ORk+fjo6GjMmzcPn332GQICAjBt2jQsWbLkgmNuaAx9+/Z1eM+uu+46KJVK/PDDDwAAQRDw008/IT4+HgaDAYBYtwsAt99+OwIDAx3+99lnn8FoNNYaU3R0dMNvVDOcPHkSgiCgV69etcaRlJSE7Oxsh+PDw8Oh1WprPc+RI0dw9dVXw9vbGwaDAYGBgbjlllsAoFmfh6bMuaRbt24OP0vlHFKN8g033ICxY8fi7rvvRnBwMG688Ub8+OOPDGyJ2glrZomo1SkUimY/9rXXXsMLL7yAO++8Ey+//DL8/PygVCrx6KOPOjU4uP766zF+/Hj8+uuvWLNmDd588028/vrrWL58OeLj4wEA//3vf3HHHXdg5cqVWLNmDR5++GEsXrwYO3fuRERERIvHEBYWhvHjx+PHH3/Es88+i507dyItLU2u3QQgv0dvvvkmhgwZUufz1Kwntc9+thar1QqFQoE///yzzk4CjRlDYWEhJkyYAIPBgJdeegkxMTHQ6/XYv38/nn766Xb7PNTXCUGw1Re7ublh8+bN2LBhA1atWoW//voLP/zwAy655BKsWbOGnRSI2hiDWSJqsvqC1e7du8NqteLkyZPo16+ffHtWVhYKCwvRvXv3Cz7Hzz//jEmTJuHzzz93uL2wsFBeYNRapPEcP3681n3Hjh1DQEAAPDw85NtCQ0PxwAMP4IEHHkB2djaGDRuGV199VQ5mAWDQoEEYNGgQnn/+eWzfvh1jx47Fxx9/jFdeeeWCY7jkkksc7jt+/LjDewaIWcAHHngAx48fxw8//AB3d3fMnDlTvj8mJgYAYDAYMHny5Ka8Hc1S3zzGxMRAEARER0ejd+/ezXrujRs3Ii8vD8uXL8fFF18s356SktLocdTU1DlvLKVSiUsvvRSXXnop3nrrLbz22mt47rnnsGHDhnaZB6KujGUGRNRk0h/7mo3yp0+fDgC1mte/9dZbAMQ6UPvnqKvRvkqlqrWi/qeffnJoU9VaQkNDMWTIEHz55ZcOYzl8+DDWrFkjvx6LxVLrknZQUBDCwsLklmPFxcWoqqpyOGbQoEFQKpUObclqGjFiBIKCgvDxxx87HPfnn38iKSnJ4T0DgGuvvRYqlQrfffcdfvrpJ1x++eUOwdfw4cMRExOD//znPygtLa11vpycnAu8K01T32fhmmuugUqlwqJFi2rNpyAIyMvLu+BzSxlN+8ebTCZ8+OGHdY6jMWUHjZ3zpsjPz691m5QVb2juiah1MDNLRE02fPhwAGJLpBtvvBEajQYzZ85EbGwsbr/9dnzyySfyJeLdu3fjyy+/xFVXXYVJkyY5PMdHH32EV155BT179kRQUBAuueQSXH755XjppZcwe/ZsjBkzBocOHcL//vc/9OjRo01ey5tvvon4+HjExcXhrrvukts0eXt748UXXwQgLjyKiIjAv/71L8TGxsLT0xNr167Fnj178N///heA2A7rwQcfxHXXXYfevXujqqoKX3/9NVQqFa699tp6z6/RaPD6669j9uzZmDBhAm666Sa5NVdUVBQee+wxh+ODgoIwadIkvPXWWygpKcENN9zgcL9SqcRnn32G+Ph4DBgwALNnz0Z4eDjS09OxYcMGGAwG/P777632/tX3WYiJicErr7yC+fPn48yZM7jqqqvg5eWFlJQU/Prrr7j33nvxxBNPNPjcY8aMga+vL26//XY8/PDDUCgU+Prrr+tsHzZ8+HD88MMPmDdvHkaOHAlPT0+HjLW9xsx5U7z00kvYvHkzZsyYge7duyM7OxsffvghIiIimtS3l4iayWl9FIjIpb388stCeHi4oFQqHVozmc1mYdGiRUJ0dLSg0WiEyMhIYf78+Q5tlARBEDIzM4UZM2YIXl5eAgC5TVdlZaXw+OOPC6GhoYKbm5swduxYYceOHcKECRMcWnm1VmsuQRCEtWvXCmPHjhXc3NwEg8EgzJw5Uzh69Kh8v9FoFJ588kkhNjZW8PLyEjw8PITY2Fjhww8/lI9JTk4W7rzzTiEmJkbQ6/WCn5+fMGnSJGHt2rWNGtsPP/wgDB06VNDpdIKfn59w8803C+fOnavz2E8//VQAIHh5eTm087J34MAB4ZprrhH8/f0FnU4ndO/eXbj++uuFdevWycdIraca21KqrtZcglD/Z0EQBOGXX34Rxo0bJ3h4eAgeHh5C3759hblz5wrHjx+Xj5kwYYIwYMCAOs+5bds24aKLLhLc3NyEsLAw4amnnhL+/vtvAYCwYcMG+bjS0lJh1qxZgo+PjwBAbtPV3Dlv6P1ZunSpw+tct26dcOWVVwphYWGCVqsVwsLChJtuukk4ceJEI95VImophSA0oUM2EREREVEHwppZIiIiInJZrJklIpdnMpnqXIRjz9vbu01aUBERkXMxmCUil7d9+3aHxWV1Wbp0Ke644472GRAREbUb1swSkcsrKCjAvn37GjxmwIABCA0NbacRERFRe2EwS0REREQuiwvAiIiIiMhldcmaWavVivPnz8PLy6tFe8gTERERUdsQBAElJSUICwuDUll//rVLBrPnz59HZGSks4dBRERERBdw9uxZRERE1Ht/lwxmvby8AIhvjsFgaLPzmM1mrFmzBlOnToVGo2mz81Db4Ry6Ps6h6+Mcuj7OoetzxhwWFxcjMjJSjtvq0yWDWam0wGAwtHkw6+7uDoPBwF9eF8U5dH2cQ9fHOXR9nEPX58w5vFBJKBeAEREREZHLYjBLRERERC6LwSwRERERuawuWTNLRERErk0QBFRVVcFisTh7KF2C2WyGWq1GZWVlq73nKpUKarW6xW1SGcwSERGRSzGZTMjIyEB5ebmzh9JlCIKAkJAQnD17tlV79Lu7uyM0NBRarbbZz8FgloiIiFyG1WpFSkoKVCoVwsLCoNVquQFSO7BarSgtLYWnp2eDGxg0liAIMJlMyMnJQUpKCnr16tXs52UwS0RERC7DZDLBarUiMjIS7u7uzh5Ol2G1WmEymaDX61slmAUANzc3aDQapKamys/dHFwARkRERC6ntQIqcq7WmEd+EoiIiIjIZTGYJSIiIiKXxWCWiIiIqJO54447cNVVVzXq2IkTJ+LRRx9t0/G0JQazRERERO2gOUGjqwea7YHBLBERERG5LAaz7UQQBLz42xG88sdRZw+FiIioUxEEAeWmqnb/nyAIjR7jHXfcgU2bNuHdd9+FQqGAQqHAmTNnsGnTJowaNQo6nQ6hoaF45plnUFVV1eBjLBYL7rrrLkRHR8PNzQ19+vTBu+++22rvZ0FBAW677Tb4+vrC3d0d8fHxOHnypHx/amoqZs6cCV9fX3h4eGDAgAFYvXq1/Nibb74ZgYGBcHNzQ69evbB06dJWG1td2Ge2nZzNr8Cy7WcAAI9P7QM3rcq5AyIiIuokKswW9F/wd7uf9+hL0+CubVwo9e677+LEiRMYOHAgXnrpJQCAxWLB9OnTcccdd+Crr77CsWPHcM8990Cv1+PFF1+s8zGBgYGwWq2IiIjATz/9BH9/f2zfvh333nsvQkNDcf3117f4dd1xxx04efIkfvvtNxgMBjz99NO4/PLLsX37dgDA3LlzYTKZsHnzZnh4eODo0aPw9PQEALzwwgs4evQo/vzzTwQEBODUqVOoqKho8ZgawmC2nRxKL5L/u8JsYTBLRETUhXh7e0Or1cLd3R0hISEAgOeeew6RkZH44IMPoFAo0LdvX5w/fx5PP/00FixYUOdjAEClUmHRokXyz9HR0dixYwd+/PHHFgezUhC7bds2jBkzBgDwv//9D5GRkVi1ahVuu+02pKWl4dprr8WgQYMAAD169JAfn5aWhqFDh2LEiBEAgKioqBaNpzEYzLYT+2C20mxx4kiIiIg6FzeNCkdfmuaU87ZEUlIS4uLiHLbjHTt2LEpLS3Hu3Dl069at3scuWbIEX3zxBdLS0lBRUQGTyYQhQ4a0aDzSmNRqNUaPHi3f5u/vjz59+uDEiRMAgIcffhj3338/1qxZg8mTJ+Paa6/F4MGDAQD3338/rr32Wuzfvx9Tp07FVVddJQfFbYU1s+3kUHqh/N8MZomIiFqPQqGAu1bd7v+zD0Lb0/fff48nnngCd911F9asWYPExETMnj0bJpOpXc5/9913Izk5GbfeeisOHTqEESNG4P333wcAxMfHIzU1FY899hjOnz+PSy+9FE888USbjofBbDsQBAGHztlnZq1OHA0RERE5g1arhcVSndDq168fduzY4bCQbNu2bfDy8kJERESdj5GOGTNmDB544AEMHToUPXv2xOnTp1tljP369UNVVRV27dol35aXl4fjx4+jT58+8m2RkZGYM2cOli9fjscffxyffvqpfF9gYCBuv/12fPPNN3jnnXfwySeftMrY6sNgth2kFVSguLJK/rmCmVkiIqIuJyoqCrt27cKZM2eQm5uLBx54AGfPnsVDDz2EY8eOYeXKlVi4cCHmzZsHpVJZ52OsVit69eqFvXv34u+//8aJEyfwwgsvYM+ePa0yxl69euHKK6/EPffcg61bt+Kff/7BLbfcgvDwcEyfPh0A8Oijj+Lvv/9GSkoK9u/fjw0bNqBfv34AgAULFmDlypU4deoUjhw5gj/++EO+r60wmG0HR9KLHX42MpglIiLqcp544gmoVCr0798fgYGBMJvNWL16NXbv3o3Y2FjMmTMHd911F55//vl6H5OWlob77rsP11xzDW644QaMHj0aeXl5eOCBB1ptnEuXLsXw4cNx+eWXIy4uDoIg4I8//oBGowEgdmGYO3cu+vXrh8suuwy9e/fGhx9+CEDMJM+fPx+DBw/GxRdfDJVKhe+//77VxlYXhdCUJmmdRHFxMby9vVFUVASDwdBm55E+pIdVPfHp1jPy7V/cMQKX9A1us/NS65HmcPr06fIvMbkWzqHr4xy6vtacw8rKSqSkpCA6Ohp6vb6VRkgXYrVaUVxcDIPBIGeNW0ND89nYeI2Z2XZw+LxjZpY1s0REREStg8FsGxME4IgtmPX30AJgNwMiIiJqH2lpafD09Kz3f2lpac4eYouxz2wbyzMCxZVV0KqVGBThjY3Hc5iZJSIionYRFhaGxMTEBu93dQxm21haqdiDrl+IF7z0Yp0QM7NERETUHtRqNXr27OnsYbQpBrNtLMRNwP0TohHq4y73mmVrLiIiopbpguvXO6XWmEfWzLaxMA9g3uReuC0uCnrbtndszUVERNQ8UjeE8vJyJ4+EWoM0jy3pcsHMbDvSa8TvDpVVrJklIiJqDpVKBR8fH2RnZwMA3N3dnbatbFditVphMplQWVnZKq25BEFAeXk5srOz4ePjA5VK1eznYjDbjqTMLGtmiYiImi8kJAQA5ICW2p4gCKioqICbm1urfnnw8fGR57O5GMy2IwazRERELadQKBAaGoqgoCCYzWZnD6dLMJvN2Lx5My6++OJW27xEo9G0KCMrcWowu3nzZrz55pvYt28fMjIy8Ouvv+Kqq65q8DH/+9//8MYbb+DkyZPw9vZGfHw83nzzTfj7+7fPoFugOphlmQEREVFLqVSqVgmG6MJUKhWqqqqg1+s73E58Tl0AVlZWhtjYWCxZsqRRx2/btg233XYb7rrrLhw5cgQ//fQTdu/ejXvuuaeNR9o65JpZZmaJiIiIWoVTM7Px8fGIj49v9PE7duxAVFQUHn74YQBAdHQ07rvvPrz++uttNcRWpVfbMrNcAEZERETUKlyqZjYuLg7PPvssVq9ejfj4eGRnZ+Pnn3/G9OnTG3yc0WiE0WiUfy4uFreXNZvNbVprIz239P8apdhLrdzYtuel1lNzDsn1cA5dH+fQ9XEOXZ8z5rCx51IIHaTrsEKhaFTN7E8//YQ777wTlZWVqKqqwsyZM/HLL780WL/x4osvYtGiRbVu//bbb+Hu7t7SoTfa4QIFPj2mQqSHgCcGs9SAiIiIqD7l5eWYNWsWioqKYDAY6j3OpYLZo0ePYvLkyXjssccwbdo0ZGRk4Mknn8TIkSPx+eef1/u4ujKzkZGRyM3NbfDNaSmz2YyEhARMmTIFGo0G20/n4fZl+9AryAOrHxrbZuel1lNzDsn1cA5dH+fQ9XEOXZ8z5rC4uBgBAQEXDGZdqsxg8eLFGDt2LJ588kkAwODBg+Hh4YHx48fjlVdeQWhoaJ2P0+l00Ol0tW7XaDTtMiHSeTzdtAAAY5XAX2YX016fFWo7nEPXxzl0fZxD19eec9jY87jUdrbl5eW1dp2QWnJ0kARzg3Rq9pklIiIiak1ODWZLS0uRmJiIxMREAEBKSgoSExORlpYGAJg/fz5uu+02+fiZM2di+fLl+Oijj5CcnIxt27bh4YcfxqhRoxAWFuaMl9AkbloGs0REREStyallBnv37sWkSZPkn+fNmwcAuP3227Fs2TJkZGTIgS0A3HHHHSgpKcEHH3yAxx9/HD4+PrjkkktcpzWXhq25iIiIiFqTU4PZiRMnNlgesGzZslq3PfTQQ3jooYfacFRtR68WE+GmKissVgEqZevtbUxERETUFblUzayrkzKzAGCsYqkBERERUUsxmG1H9sFspZmlBkREREQtxWC2HamUCmhUYmkBF4ERERERtRyD2XamZ3suIiIiolbDYLad6aSOBiwzICIiImoxBrPtzE0rvuWVXABGRERE1GIMZtuZXGZgYjBLRERE1FIMZttZ9cYJDGaJiIiIWorBbDvTa2xlBqyZJSIiImoxBrPtTM7MspsBERERUYsxmG1nOjW7GRARERG1Fgaz7ay6zICZWSIiIqKWYjDbzty4AIyIiIio1TCYbWdyzSxbcxERERG1GIPZdiaXGVSxZpaIiIiopRjMtjN2MyAiIiJqPQxm2xmDWSIiIqLWw2C2nenU3DSBiIiIqLUwmG1nzMwSERERtR4Gs+1MDma5AIyIiIioxRjMtjM3tuYiIiIiajUMZttZdWsuBrNERERELcVgtp2xZpaIiIio9TCYbWdyZpbdDIiIiIhajMFsO9OpmZklIiIiai0MZtsZywyIiIiIWg+D2XZWvQCMZQZERERELcVgtp1JmVlTlRUWq+Dk0RARERG5Ngaz7UzqMwsARrbnIiIiImoRBrPtTG8XzLKjAREREVHLMJhtZyqlAhqVAgAXgRERERG1FINZJ9CzPRcRERFRq2Aw6wQ6W6nBigPpmPzWJmw/levkERERERG5JgazTiC153pv/Smcyi7FX0cynTwiIiIiItfEYNYJ7BeBAUCZkeUGRERERM3BYNYJDHo1gOo2XeWmKmcOh4iIiMhlqZ09gK7oial9sOlEDsJ83LDwtyMoNTKYJSIiImoOBrNOMKZnAMb0DMBfh8Va2XITywyIiIiImoNlBk7koRPLDMqYmSUiIiJqFgazTuShExPjZayZJSIiImoWBrNO5KEVg9lydjMgIiIiahanBrObN2/GzJkzERYWBoVCgRUrVlzwMUajEc899xy6d+8OnU6HqKgofPHFF20/2DbgrhXLDLgAjIiIiKh5nLoArKysDLGxsbjzzjtxzTXXNOox119/PbKysvD555+jZ8+eyMjIgNVqbeORtg1PW5mBscqKKosVahUT5URERERN4dRgNj4+HvHx8Y0+/q+//sKmTZuQnJwMPz8/AEBUVFQbja7tueuqN08oM1ng7cZgloiIiKgpXKo112+//YYRI0bgjTfewNdffw0PDw9cccUVePnll+Hm5lbv44xGI4xGo/xzcXExAMBsNsNsNrfZeKXnru8cSgAalQJmi4Ciskq4u9RsdA0XmkPq+DiHro9z6Po4h67PGXPY2HO5VPiUnJyMrVu3Qq/X49dff0Vubi4eeOAB5OXlYenSpfU+bvHixVi0aFGt29esWQN3d/e2HDIAICEhod77NFDBDAX+WrsewfXH4+RkDc0huQbOoevjHLo+zqHra885LC8vb9RxCkEQhDYeS6MoFAr8+uuvuOqqq+o9ZurUqdiyZQsyMzPh7e0NAFi+fDn+9a9/oaysrN7sbF2Z2cjISOTm5sJgMLTq67BnNpuRkJCAKVOmQKPR1HnMhP9sxvmiSvxy32gMjvBus7FQ8zRmDqlj4xy6Ps6h6+Mcuj5nzGFxcTECAgJQVFTUYLzmUpnZ0NBQhIeHy4EsAPTr1w+CIODcuXPo1atXnY/T6XTQ6XS1btdoNO0yIQ2dR+o1a7SCv+AdWHt9VqjtcA5dH+fQ9XEOXV97zmFjz+NSK47Gjh2L8+fPo7S0VL7txIkTUCqViIiIcOLIms9d2jiBvWaJiIiImsypwWxpaSkSExORmJgIAEhJSUFiYiLS0tIAAPPnz8dtt90mHz9r1iz4+/tj9uzZOHr0KDZv3ownn3wSd955Z4MLwDoyT1tHg3LuAkZERETUZE4NZvfu3YuhQ4di6NChAIB58+Zh6NChWLBgAQAgIyNDDmwBwNPTEwkJCSgsLMSIESNw8803Y+bMmXjvvfecMv7W4G7bBYwbJxARERE1nVNrZidOnIiG1p8tW7as1m19+/btVKshpY0TuKUtERERUdO5VM1sZyRtaVvGMgMiIiKiJmMw62Se8gIwBrNERERETcVg1smkmtkyE8sMiIiIiJqKwayTedi6GTAzS0RERNR0DGadzIN9ZomIiIiajcGsk8kLwJiZJSIiImoyBrNOJrfmYjcDIiIioiZjMOtkXABGRERE1HwMZp2MrbmIiIiImo/BrJO5s5sBERERUbMxmHUyD7syg4a29iUiIiKi2hjMOpnUZ9ZiFWCssjp5NERERESuhcGsk0kLwACWGhARERE1FYNZJ1MpFXDTiNnZcnY0ICIiImoSBrMdgLylLXvNEhERETUJg9kOQO41yzIDIiIioiZhMNsBeMi9ZllmQERERNQUDGY7AA8te80SERERNQeD2Q5AzsxyARgRERFRkzCY7QA8uAsYERERUbMwmO0AqncBYzBLRERE1BQMZjsAqcygnAvAiIiIiJqEwWwH4G5bAFbKMgMiIiKiJmEw2wHImVmWGRARERE1CYPZDqC6NZcFgiDAahWcPCIiIiIi18BgtgOQMrOnsksR/+4WTH9vC8wWq5NHRURERNTxqZ09AKoOZo9nlci3JeeUoU+Il7OGREREROQSmJntALz0tb9TnMoudcJIiIiIiFwLM7MdwMgoP8wYHIoBYQacyi7F8v3pOJ3DYJaIiIjoQhjMdgB6jQpLZg0DAHy86TQAMJglIiIiagSWGXQwMYGeAFhmQERERNQYDGY7mJhADwDiAjC26CIiIiJqGIPZDqabnzs0KgUqzBZkFFc6ezhEREREHRqD2Q5GrVIiyl/MzrLUgIiIiKhhDGY7IKlu9jSDWSIiIqIGMZjtgHoG2YJZdjQgIiIiahCD2Q4oJkgsM2AwS0RERNQwBrMdUHV7rjInj4SIiIioY2Mw2wFJwWxuqRFF5WYnj4aIiIio42Iw2wF56NQI9dYDAE7nstSAiIiIqD4MZjsoaRHY0fPFTh4JERERUcfFYLaDGtbNFwCw90y+k0dCRERE1HE5NZjdvHkzZs6cibCwMCgUCqxYsaLRj922bRvUajWGDBnSZuNzppFRfgCAPWcKnDwSIiIioo7LqcFsWVkZYmNjsWTJkiY9rrCwELfddhsuvfTSNhqZ8w3t5gOVUoH0wgqcL6xw9nCIiIiIOiS1M08eHx+P+Pj4Jj9uzpw5mDVrFlQqVZOyua7EQ6fGgDADDp4rwp4z+bhySLizh0RERETU4Tg1mG2OpUuXIjk5Gd988w1eeeWVRj3GaDTCaDTKPxcXi4uqzGYzzOa2a30lPXdzzzEs0hsHzxVhd3Iepg8Ias2hUSO1dA7J+TiHro9z6Po4h67PGXPY2HO5VDB78uRJPPPMM9iyZQvU6sYPffHixVi0aFGt29esWQN3d/fWHGKdEhISmvU4RZ4CgAobDqdhlCqldQdFTdLcOaSOg3Po+jiHro9z6Pracw7Ly8sbdZzLBLMWiwWzZs3CokWL0Lt37yY9dv78+Zg3b578c3FxMSIjIzF16lQYDIbWHqrMbDYjISEBU6ZMgUajafLjR5UasfT1TcioUGDcpCkwuDX9OahlWjqH5HycQ9fHOXR9nEPX54w5lK6kX4jLBLMlJSXYu3cvDhw4gAcffBAAYLVaIQgC1Go11qxZg0suuaTOx+p0Ouh0ulq3azSadpmQ5p4n1FeD6AAPpOSW4eD5Ukzqy1IDZ2mvzwq1Hc6h6+Mcuj7Ooetrzzls7HlcJpg1GAw4dOiQw20ffvgh1q9fj59//hnR0dFOGlnbGtHdFym5ZdhzJp/BLBEREVENTg1mS0tLcerUKfnnlJQUJCYmws/PD926dcP8+fORnp6Or776CkqlEgMHDnR4fFBQEPR6fa3bO5MBYQb8tA9IyS1z9lCIiIiIOhynBrN79+7FpEmT5J+lutbbb78dy5YtQ0ZGBtLS0pw1vA4hxNsNAJBZXOnkkRARERF1PE4NZidOnAhBEOq9f9myZQ0+/sUXX8SLL77YuoPqYEK99QCAzCIGs0REREQ1OXUHMLqwEFswm11ihMVaf+BPRERE1BUxmO3gAjx1UCkVsFgF5JYaL/wAIiIioi6EwWwHp1IqEOQlthXLYKkBERERkQMGsy4gpEbdrCAIqLJYnTkkIiIiog6BwawLqF4EVgEAuO2L3Zjw5kZUmi3OHBYRERGR0zGYdQHBBjGYzSiuRFGFGVtO5iK9sAJp+Y3bs5iIiIios2Iw6wKkzGxWUSVOZpXIt5caq5w1JCIiIqIOgcGsC5Azs0WVOJZZHcyWMZglIiKiLo7BrAsIte0CllVciRNZDGaJiIiIJAxmXUBIPZnZUiMXgBEREVHXxmDWBQQZxD6zxior/jlbKN/OzCwRERF1dQxmXYBeo4K/hxaAGNBKuACMiIiIujoGsy5CWgRmj5lZIiIi6uqaFcx++eWXWLVqlfzzU089BR8fH4wZMwapqamtNjiqJrXnssdgloiIiLq6ZgWzr732GtzcxBX2O3bswJIlS/DGG28gICAAjz32WKsOkETBdsGsp04NgAvAiIiIiNTNedDZs2fRs2dPAMCKFStw7bXX4t5778XYsWMxceLE1hwf2YTalRkM7eaDLSdzmZklIiKiLq9ZmVlPT0/k5eUBANasWYMpU6YAAPR6PSoqKlpvdCQL8bYPZn0BAGUmBrNERETUtTUrMztlyhTcfffdGDp0KE6cOIHp06cDAI4cOYKoqKjWHB/ZSMGsXqNE/1ADAHYzICIiImpWZnbJkiWIi4tDTk4OfvnlF/j7+wMA9u3bh5tuuqlVB0ii2EgfdPNzx9VDI2DQi99BWGZAREREXV2zMrM+Pj744IMPat2+aNGiFg+I6mbQa7DpyYlQKBTyxgllXABGREREXVyzMrN//fUXtm7dKv+8ZMkSDBkyBLNmzUJBQUGrDY4cKRQKAICHrZtBSaXZmcMhIiIicrpmBbNPPvkkiouLAQCHDh3C448/junTpyMlJQXz5s1r1QFSbVJrrjKTBYIgOHk0RERERM7TrDKDlJQU9O/fHwDwyy+/4PLLL8drr72G/fv3y4vBqO146FQAAItVgLHKCr1G5eQRERERETlHszKzWq0W5eXlAIC1a9di6tSpAAA/Pz85Y0ttx0Nb/R2EHQ2IiIioK2tWZnbcuHGYN28exo4di927d+OHH34AAJw4cQIRERGtOkCqTalUwF2rQrnJgjJjFQI8dc4eEhEREZFTNCsz+8EHH0CtVuPnn3/GRx99hPDwcADAn3/+icsuu6xVB0h185C3tGVmloiIiLquZmVmu3Xrhj/++KPW7W+//XaLB0SN46lTI6fEyPZcRERE1KU1K5gFAIvFghUrViApKQkAMGDAAFxxxRVQqbgYqT1Ii8C4cQIRERF1Zc0KZk+dOoXp06cjPT0dffr0AQAsXrwYkZGRWLVqFWJiYlp1kFSbtAiMZQZERETUlTWrZvbhhx9GTEwMzp49i/3792P//v1IS0tDdHQ0Hn744dYeI9VB7jXLYJaIiIi6sGZlZjdt2oSdO3fCz89Pvs3f3x///ve/MXbs2FYbHNWPC8CIiIiImpmZ1el0KCkpqXV7aWkptFptiwdFF+aplzKzXABGREREXVezgtnLL78c9957L3bt2gVBECAIAnbu3Ik5c+bgiiuuaO0xUh2qt7RlZpaIiIi6rmYFs++99x5iYmIQFxcHvV4PvV6PsWPHomfPnnj33Xdbe4xUBy4AIyIiImpmzayPjw9WrlyJkydP4tixYwCAfv36oWfPnq06OKofW3MRERERtaDPLAD06tULvXr1aq2xUBOwmwERERFRE4LZefPmNfpJ33rrrWYNhhqP3QyIiIiImhDMHjhwoFHHKRSKZg+GGs+TwSwRERFR44PZDRs2NPnJz507h7CwMCiVzVpnRg3w0LE1FxEREVGbRpn9+/fHmTNn2vIUXZa0AIyZWSIiIurK2jSYFQShLZ++S+MCMCIiIqI2DmYvZPPmzZg5cybCwsKgUCiwYsWKBo9fvnw5pkyZgsDAQBgMBsTFxeHvv/9un8F2MFKZQbnJAquVXxqIiIioa3JqMFtWVobY2FgsWbKkUcdv3rwZU6ZMwerVq7Fv3z5MmjQJM2fObPTitM5EyswC3AWMiIiIuq4W9Zltqfj4eMTHxzf6+Hfeecfh59deew0rV67E77//jqFDh7by6Do2nVoJlVIBi1VAmdECL73G2UMiIiIiandtGsy2dZsuq9WKkpIS+Pn5NXic0WiE0WiUfy4uLgYAmM1mmM3mNhuf9NxtdQ4PrQrFlVUoLKuEv7uqTc7R1bX1HFLb4xy6Ps6h6+Mcuj5nzGFjz9WmwWxbLwD7z3/+g9LSUlx//fUNHrd48WIsWrSo1u1r1qyBu7t7Ww1PlpCQ0CbPq7SqACiwZsMmdPdsk1OQTVvNIbUfzqHr4xy6Ps6h62vPOSwvL2/UcQqhDSPOs2fPIiwsDCrVhbOGCoUCv/76K6666qpGPfe3336Le+65BytXrsTkyZMbPLauzGxkZCRyc3NhMBgadb7mMJvNSEhIwJQpU6DRtH4ZQPx723AqpwxfzR6OuB7+rf781PZzSG2Pc+j6OIeuj3Po+pwxh8XFxQgICEBRUVGD8VqjM7PXXHNNo0++fPlyAEBkZGSjH9MU33//Pe6++2789NNPFwxkAUCn00Gn09W6XaPRtMuEtNV5PG11spVV4D8Obay9PivUdjiHro9z6Po4h66vPeewsedpdDDr7e3d7MG0pu+++w533nknvv/+e8yYMcPZw3EqqaPBs78ewvzlhzB/ej/8a3iEk0dFRERE1H4aHcwuXbq01U9eWlqKU6dOyT+npKQgMTERfn5+6NatG+bPn4/09HR89dVXAMTSgttvvx3vvvsuRo8ejczMTACAm5tbhwm221M3f3fgFJBbagIA/LzvLINZIiIi6lKc2md27969GDp0qNxWa968eRg6dCgWLFgAAMjIyEBaWpp8/CeffIKqqirMnTsXoaGh8v8eeeQRp4zf2Z6+rC8+vmUYXrlqIAAgOafMySMiIiIial/N7mbw888/48cff0RaWhpMJpPDffv372/Uc0ycOLHBjgfLli1z+Hnjxo1NHWan5u2mwWUDQ1FcacbzKw4ju8SIkkoze84SERFRl9GszOx7772H2bNnIzg4GAcOHMCoUaPg7++P5OTkJm2CQK3DoNcg0Etc4MbsLBEREXUlzQpmP/zwQ3zyySd4//33odVq8dRTTyEhIQEPP/wwioqKWnuM1Ag9AjwAAKdzSp08EiIiIqL206xgNi0tDWPGjAEgLr4qKSkBANx666347rvvWm901GgxQeKuCczMEhERUVfSrGA2JCQE+fn5AIBu3bph586dAMRuBG296xfVjZlZIiIi6oqaFcxecskl+O233wAAs2fPxmOPPYYpU6bghhtuwNVXX92qA6TGYWaWiIiIuqJmdTP45JNPYLVaAQBz586Fv78/tm/fjiuuuAL33Xdfqw6QGicmQAxmU/LKYLEKUCkVTh4RERERUdtrVjB77tw5h61qb7zxRtx4440QBAFnz55Ft27dWm2A1Djhvm7QqpUwVVmRXlAhbqhARERE1Mk1q8wgOjoaOTk5tW7Pz89HdHR0iwdFTadSKhDtz7pZIiIi6lqaFcwKggCFovZl7NLSUuj1+hYPipqnRyCDWSIiIupamlRmMG/ePACAQqHACy+8AHf36kvZFosFu3btwpAhQ1p1gNR4MYFi3expLgIjIiKiLqJJweyBAwcAiJnZQ4cOQavVyvdptVrExsbiiSeeaN0RUqNJmdlkZmaJiIioi2hSMLthwwYAYjuud999FwaDoU0GRc1TnZllMEtERERdQ7O6GSxdulT+73PnzgEAIiIiWmdE1GxSr9ncUhPyy0zw89Be4BFERERErq1ZC8CsViteeukleHt7o3v37ujevTt8fHzw8ssvy/1nqf156tSI9HMDABzPLHHyaIiIiIjaXrMys8899xw+//xz/Pvf/8bYsWMBAFu3bsWLL76IyspKvPrqq606SGq8PsFeOJtfgeOZxYiL8Xf2cIiIiIjaVLOC2S+//BKfffYZrrjiCvm2wYMHIzw8HA888ACDWSfqE+KFtUnZOJ7FulkiIiLq/JpVZpCfn4++ffvWur1v377Iz89v8aCo+XoHewEAjmcWO3kkRERERG2vWcFsbGwsPvjgg1q3f/DBB4iNjW3xoKj5+oaIHSZOZJVCEAQnj4aIiIiobTWrzOCNN97AjBkzsHbtWsTFxQEAduzYgbNnz2L16tWtOkBqmugAD6iVCpQaq5BeWIEIX/cLP4iIiIjIRTUrMxsdHY0TJ07g6quvRmFhIQoLC3HNNdfg+PHj6N69e2uPkZpAq1bK/WZPZLGjAREREXVuzcrMRkdHIyMjo9ZCr7y8PERGRsJisbTK4Kh5eod44XhWCY5lluCSvsHOHg4RERFRm2lWZra+WszS0lLo9foWDYharm+IuAjsBHvNEhERUSfXpMzsvHnzAAAKhQILFiyAu3t1PabFYsGuXbswZMiQVh0gNZ3U0eAYg1kiIiLq5JoUzB44cACAmJk9dOgQtNrq7VK1Wi1iY2PxxBNPtO4IqcmkzGxyThnMFis0qmYl4ImIiIg6vCYFsxs2bAAAzJ49G++++y4MBkObDIpaJtzHDe5aFcpNFqTmlaNnkKezh0RERETUJpqVslu6dCkD2Q5MqVQg1FusXc4uqXTyaIiIiIjaDq8/d1J+HmIJSGG52ckjISIiImo7DGY7KV93MZjNLzM5eSREREREbYfBbCclBbMFDGaJiIioE2Mw20n52soMClhmQERERJ0Yg9lOys9DAwAoKGdmloiIiDovBrOdlA9rZomIiKgLYDDbSfm5S90MGMwSERFR58VgtpOSambzGcwSERFRJ8ZgtpPydbfVzJZxARgRERF1XgxmOylp04RSYxVMVVYnj4aIiIiobTCY7aQMeg2UCvG/WTdLREREnRWD2U5KqVRU7wLGYJaIiIg6KQaznZgP62aJiIiok2Mw24n5ybuAMTNLREREnROD2U7MlxsnEBERUSfHYLYT8+XGCURERNTJOTWY3bx5M2bOnImwsDAoFAqsWLHigo/ZuHEjhg0bBp1Oh549e2LZsmVtPk5XJW+cwJpZIiIi6qScGsyWlZUhNjYWS5YsadTxKSkpmDFjBiZNmoTExEQ8+uijuPvuu/H333+38Uhdk5+HbQEYM7NERETUSamdefL4+HjEx8c3+viPP/4Y0dHR+O9//wsA6NevH7Zu3Yq3334b06ZNa6thuiwfdy4AIyIios7NqcFsU+3YsQOTJ092uG3atGl49NFHG3yc0WiE0WiUfy4uLgYAmM1mmM1tdwleeu62PEdDDDox8Z5fanTaGFyds+eQWo5z6Po4h66Pc+j6nDGHjT2XSwWzmZmZCA4OdrgtODgYxcXFqKiogJubW52PW7x4MRYtWlTr9jVr1sDd3b1NxmovISGhzc9Rl5QSAFAjPbcIq1evdsoYOgtnzSG1Hs6h6+Mcuj7OoetrzzksLy9v1HEuFcw21/z58zFv3jz55+LiYkRGRmLq1KkwGAxtdl6z2YyEhARMmTIFGo2mzc5TnzN5ZXjn8DYYocH06SzDaA5nzyG1HOfQ9XEOXR/n0PU5Yw6lK+kX4lLBbEhICLKyshxuy8rKgsFgqDcrCwA6nQ46na7W7RqNpl0mpL3OU1OgQcw6lxqrIChU0KrZia25nDWH1Ho4h66Pc+j6OIeurz3nsLHncanoJi4uDuvWrXO4LSEhAXFxcU4aUcdm0GugVIj/zV6zRERE1Bk5NZgtLS1FYmIiEhMTAYittxITE5GWlgZALA+47bbb5OPnzJmD5ORkPPXUUzh27Bg+/PBD/Pjjj3jsscecMfwOT6lUyBsnFJSz6J6IiIg6H6cGs3v37sXQoUMxdOhQAMC8efMwdOhQLFiwAACQkZEhB7YAEB0djVWrViEhIQGxsbH473//i88++4xtuRrg4y6m6LmlLREREXVGTq2ZnThxIgRBqPf+unb3mjhxIg4cONCGo+pc/Dy0OJ1Txl6zRERE1Cm5VM0sNZ0vN04gIiKiTozBbCcX5iN2eUjKaFx7CyIiIiJXwmC2kxvfKwAAsPF4ToMlHURERESuiMFsJxcX4w+tWolzBRU4nVPq7OEQERERtSoGs52cu1aNi3r4AwDWH8t28miIiIiIWheD2S5gUp9AAMCGYzlOHgkRERFR62Iw2wVM6hMEANhzJh8lldw8gYiIiDoPBrNdQFSAB3oEeKDKKmDbqVxnD4eIiIio1TCY7SIm2rKzLDUgIiKizoTBbBdxUQ8/AMDh80VOHgkRERFR62Ew20XEBHkCAFJyy9hvloiIiDoNBrNdRKSvO1RKBcpNFmQWVzp7OEREREStgsFsF6FVK9HNzx0AkJJT5uTREBEREbUOBrNdSI8ADwDA6VwGs0RERNQ5MJjtQqJtwWwyt7UlIiKiToLBbBfSI7B6ERgRERFRZ8BgtgvpEShlZhnMEhERUefAYLYLkWpmzxWUw1hlcfJoiIiIiFqOwWwXEuilg6dODasApOWVO3s4RERERC3GYLYLUSgUcqnBaZYaEBERUSfAYLaLkUoNknPZ0YCIiIhcH4PZLiY6wNbRgJlZIiIi6gQYzHYxckcDtuciIiKiToDBbBcjBbOH0ouwYOVhHEgrcPKIiIiIiJqPwWwX0zPIE1H+7jBVWfHVjlRc/eF2HM8scfawiIiIiJqFwWwXo1OrsOaxCVg6eyRibFna/czOEhERkYtiMNsFadVKTOoThEv7BQMAjmUUO3lERERERM3DYLYL6xPsBQA4xjIDIiIiclEMZruwvqHVwawgCE4eDREREVHTMZjtwnoGeUKlVKCowozM4kpnD4eIiIioyRjMdmE6tUreEYylBkREROSKGMx2cX1DDQCAYxkMZomIiMj1MJjt4vqGSHWz7GhARERErofBbBcnBbPcOIGIiIhcEYPZLk4qMziVXQpTldXJoyEiIiJqGgazXVyYtx5eejWqrAKSc0udPRwiIiKiJmEw28UpFIrqulkuAiMiIiIXw2CW0DdELDVI4iIwIiIicjEMZgl9uAiMiIiIXBSDWUK/UJYZEBERkWtiMEvoHSwGs5nFlSgsNzl5NERERESN1yGC2SVLliAqKgp6vR6jR4/G7t27Gzz+nXfeQZ8+feDm5obIyEg89thjqKysbKfRdj5eeg0ifN0AVG9rW2m2oNJsceawiIiIiC7I6cHsDz/8gHnz5mHhwoXYv38/YmNjMW3aNGRnZ9d5/LfffotnnnkGCxcuRFJSEj7//HP88MMPePbZZ9t55J2LtAjsWEYxKs0WTHtnM+Lf3QJjFQNaIiIi6ricHsy+9dZbuOeeezB79mz0798fH3/8Mdzd3fHFF1/Uefz27dsxduxYzJo1C1FRUZg6dSpuuummC2ZzqWHyTmBZJdhxOg+peeVIyS3DzuR8J4+MiIiIqH5qZ57cZDJh3759mD9/vnybUqnE5MmTsWPHjjofM2bMGHzzzTfYvXs3Ro0aheTkZKxevRq33nprvecxGo0wGo3yz8XFYgsqs9kMs9ncSq+mNum52/IcraVXoDsA4Oj5Ylit1TuBrTmcgTHRPk4alfO50hxS3TiHro9z6Po4h67PGXPY2HM5NZjNzc2FxWJBcHCww+3BwcE4duxYnY+ZNWsWcnNzMW7cOAiCgKqqKsyZM6fBMoPFixdj0aJFtW5fs2YN3N3dW/YiGiEhIaHNz9FSWRUAoEbS+UIkZxUCUAAAViWmYaQyBQpF9bHFJsBdDaidntdvP64wh9QwzqHr4xy6Ps6h62vPOSwvL2/UcU4NZptj48aNeO211/Dhhx9i9OjROHXqFB555BG8/PLLeOGFF+p8zPz58zFv3jz55+LiYkRGRmLq1KkwGAxtNlaz2YyEhARMmTIFGo2mzc7TGqosVvzn8HqYqqwwWQFPnRpWQUChyYLuQ8ZhYLj4PqXml2Pau9swsXcAPr55qJNH3fZcaQ6pbpxD18c5dH2cQ9fnjDmUrqRfiFOD2YCAAKhUKmRlZTncnpWVhZCQkDof88ILL+DWW2/F3XffDQAYNGgQysrKcO+99+K5556DUlk7XajT6aDT6WrdrtFo2mVC2us8LaHRAL2DPXE4XfzgXNI3CKYqK/46kokNJ/MwNMofAJCUWQaLVcCh9OIO/5pakyvMITWMc+j6OIeuj3Po+tpzDht7HqdeKNZqtRg+fDjWrVsn32a1WrFu3TrExcXV+Zjy8vJaAatKpQIACILQdoPtAqSOBgAwdUAwpvQXyz8SjlZ/2ThfWAEAyC01wmLl+01ERETO5fQyg3nz5uH222/HiBEjMGrUKLzzzjsoKyvD7NmzAQC33XYbwsPDsXjxYgDAzJkz8dZbb2Ho0KFymcELL7yAmTNnykEtNY/U0UCjUmBC70CYLQKUCiApoxjnCsoR4euOjCKxn69VAPLLTAj0qp3xJiIiImovTg9mb7jhBuTk5GDBggXIzMzEkCFD8Ndff8mLwtLS0hwysc8//zwUCgWef/55pKenIzAwEDNnzsSrr77qrJfQaYzvFQi18hhmxobBSy+m9mMjfXAgrRD7UgsQ4euOdFtmFgCySyoZzBIREZFTOT2YBYAHH3wQDz74YJ33bdy40eFntVqNhQsXYuHChe0wsq6lT4gX9jw3GR666o9Fz0BPHEgrRFqeuKIwo8g+mDViQCOfOyW3DH8ezsAdY6Lgru0QHzsiIiLqBBhVkANfD63Dz1EBHgCAM7Zg9nxh9bbBOcVGNNZ/1xzHHwcz4OOmxazR3VphpEREREQdYAcw6ti6+Yl9eFPzylBptiC/zCTfl11SWd/DajmbLwbDZ/LKWneARERE1KUxmKUGRflXZ2bP29XLAkBOSeMzs1m2LO65gsY1QCYiIiJqDAaz1KBu/mJmNrfUiJPZpQ73ZTcymLVYBeSUisemF1Rc4OiuwWIVkFva+C8DREREVDcGs9QgbzcN/Gx1tDuT8wBA3tq2scFsnl1P2nMMZgEA85cfxMhX1yIpo3G7mxAREVHdGMzSBXW3ZWd3nBaD2Z6BngAaXzObZbdQLK/MhAqTpZVH6HoOpBVCEIDD6UXOHgoREZFLYzBLF9TdtgjsWGYJALH3LCDWzDZm17WsYsegN72QdbNS2UVBuekCRxIREVFDGMzSBXW3LQKTSMFspdmKEmPVBR+fWSOY7eqlBsYqCwrLzQDETDURERE1H4NZuqCoAHeHn2MCPOBl21ghuxG9ZrMZzDrILa0OYAsYzBIREbUIg1m6oJqZ2TAfNwQaxG1sG1M3m1Uj4E0v7NrBrH1Ls/wysxNHQkRE5PoYzNIFSTWzkhBvPYK8xGC2Mb1mpTKD3sHiwrGunpl1DGbZnquxtpzMwSt/HIWpyursoRARUQfCYJYuyM9DK5cVBHhqodeoEOSlB9C4YFZaADa8uy8AIL2Lb5xgn80uKGdmtrFeW30Mn21NwQ5bizgiIiKAwSw1gkKhQHdb3WyYjxsAINBLKjMwwmoVUNRAUCb1ox3aTQxmmZm1a1XGjRMa7ZxtS+Sm7DxHRESdH4NZahSpbjbUW8zISmUG2cWVeGb5QcS+tAZP/PRPreDMWGVBvm2Rk5SZzS4xwljl2Gu23FSF7adzseJAeqe/jGwfjBVXVsFs6dyvtzUUVZjlzhlcNEdERPbUzh4AuYa+wV5YhQz0sG2YEGRbALYrJR8ZReJl85/3nUPC0Sx8ePMwjO0ZAKC624FWrUSPAA+4aVSoMFtwvrAS0QFigPzyH0exbPsZeZewrOJK3DchptFje3/dSVgEAY9c2gsKaXuyDqxmZrGg3CSXbVDdztstGsxnb14iIrLDYJYa5Y6xUQj21mNq/2AAkIMvKZAd29Mf+WVmJGUU47EfEpEwbwK83TRyfWiwQQeFQoEIXzeczC7FuYJyRAd4oLjSjC9tgayXTo0SYxUSjmY5BLNn88vx+8HzOJZRArVSAYObBvdPjEGwQY+s4kr8N+EEAOCK2DA52O7Iam4DXFBmZjB7AfbBLDOzRERkj8EsNYqXXoPrR0TKP0tlBgCgUirw6lWDEOKtx/R3tyA5twxv/HUMr149CJlFYuAWYhCDtXBbMJtuq5vdciIXVVYBPQI98NWdozDu9Q3Yn1aAonIzPHQqPPJDIlYdzKg1HrVSgecv7++wHezWU7kuEczWzMzmlRkBeDlnMC7CITPLYJaIiOywZpaaJdAumL1ueASiAjyg16jw6tWDAAD/25WGfan5cieDIFswG+ErLiCTFoGtO5YFALi0bxAifN3RO9gTVgHYfDIHCUezsOpgBhQKYEyMP56J74t/DY8AAOxPKwAAHE4vlsex5WRuW77kViEIgryVbZit/riAvWYvKL2wugMEg1kiIrLHYJaaxdtNg25+7vDSqfHQpb3k2+Ni/HH9CDHgnL/8kBy0Btsuo4f7iF0RjmWWwGoVsOl4DgDgkr5i+cLEPkEAgI3Hc7Bs+xkAwP0TYvDtPRdhzoQYPDBRLD84fL4YpiorjpyvzszuPJ2HqgsspvrnbCFeW52ESrOlweMu5Gx+OV5ddRSFTazfLK6okhe49Q4Rs7GsAb2w9CbWzJYaq/DZlmT5yxQREXVeDGapWRQKBVbMHYuEeRMQbmvXJXl2ej/4e2hxIqsU3+5OBQCEeIuZ3PG9xIVha5Oy8PXOVOSVmeClV2NElNjpYGKfQADAn4czsCslHyqlArfGdZefOzrAA95uGpiqrDiWWYwj56szsyXGKvxzrrDeMZebqnDv13vxyeZkrD5Uu3ShKV787Qg+3ZKCpdvONOlxOaVicGXQq+XOEPmlrhHM5pQYsXDlYaTmlbX7uZtaM/vqqqN4ZVUSPtp4ui2HRUREHQCDWWo2Pw8tQrxrL1zycddiwcz+AIBKs5iFDLaVGQwM98Y1w8IBAIt+PwIAuLhXIDQq8aM4orsfPHVqlJvEzOllA0IQ6l0dLCsUCgyJ9AEAbDiWI2fsLu4tBsENlRp8viVF3lpXCo4EQcBXO85g++nGlyiUGqvk8xzLLL7A0Y6kxV9BBj38PLQAxG4GruCbnan4ckcqXv4jqd3PbR/MFlaY5c4XdckqrsQv+9IBAOe6+AYdRERdAYNZahNXxIbJWVigOpgFgKcv6ws3jQpSPHJJ3yD5Pq1aibE9/eWf7xgbVeu5pWD2+z1pAIDu/u6IHxgCANhaTzCbU2LEx5uqs3RSUHk8qwQLVh7BI98nQhDqD5DsrT+WDZOtnOFEVmm9xyXnlOKxHxJxNr86oJIWfwV66uDrLgazeS2oAf1mZyqGvrQGh84VXfjgRjhfWIFxr6/Hm38fq/M+QNxWtszW87U9mC1Wh3IBQRD7ztbn860p8vxwgwUios6PwSy1CYVC7HCg1yihUADd/Nzl+4INern2VaGoLi2QTO4n1s8OCDNghG2jBXtDuvkAqG4LNjDMG+NsfW0PnC1ESaVjoCMIAv675jjKTBZIbWil/rdpedW7StVsmVWfvw9nyv99Jq+s3vrbL7efwa8H0vGWrXWYdB5AXEDn72nLzDYzmLVYBby//iQKys34Zf+5Zj1HTSsS03GuoAK//XO+1n3S+2OssmLTiZxWOV9jZBVXwioAWpUSXnqxAUt9i8CKys34385U+edcFynhICKi5mNrLmoz3fzd8eN9ccgrNcnb4EruubgHjmWWoEegB/w9dQ73XTMsAlVWAeN6BtS5CcKQCB+Hn/uHGRDp544of3ecySvHp1tSMG9KbwBi8Dh/+UGsTcoGANweF4Vl288gy9b/NtMu43fkfJFDBrkulWYLNhwXn0upAKwCcCq7FAPDvWsdKwXb649lo8pihVqllIPZIK+WZ2a3n86VyyZ2p+Q36zlq2mhbkJdRWAmLVYBKWf3+22dH/z6SiemDQlvlnBcitXEL8xHnpqSyqt7SjK93nkGZyYIgLx2yS4zIKTE2OuNORESuiZlZalODI3wwya6MQKLXqLDk5mF4fGqfWveplArcNKobIu2yufZ8PbSI8q++TwokH7xE7Krw3rqTWHUwA9/vTsPUtzdhbVI2tColXri8P2bGhgGozsxKAScAHD1/4frXzSdyUG6yINzHDSOi/AAAxzNL6jw2yxa4FlWYsTdVbCWWbZ+Z9RCD+OZmZn/dny7/d1JmMYorW9biq7jSjP22cVZZhVqdAOwz1+uTsmttSdxWzhdJwawbfG11xvVlZhNsX1oetnXYMFmsKK5sv5IIIiJqfwxmySVJdbOAWI4AAP8aHoG7xkUDAOZ+ux/PLD+EgnIz+oZ4YeWDY3HXuGgE27bhlTJ2mfbBbEb9wezapGy8tjoJ7647CQCYNiAEfW2ttU5k1R3MZtsFg2uPZsnnBcRg1tdDA0AMzJqaPSwzVuGvI2K5g16jhCAA+2yBaHNtPyVuYCGxb4dlqrLKAaS0U9v203ktOl9jnbf1mA3zcYOfe8OlGVJ98rBuvjDYShJYakAdQVG5GZPf2oTFf7b/Akqizo7BLLkkKZgNMegRYFemMD++LybYOht46dR44fL++P2hcegXKga80mYPJosVheVmZBRVB2z1ZWYP5Cpw/7eJ+GRzstwKbMbgEPQOFoPZ41klMFZZMOvTnXjw2/0AAKtVcFh8lJCUJW6YUEdm1mSxoszUtCzn30cyUW6yIMrfHTMGidnmlpYaSCUGEvtOANJGDxqVAlcMEc9nXzvclqSgOszHTe4AUVdpRkmlWQ64u/m7y3OdW8pFYOR8+9LycSq7FP/bmQZrA904qP3llrIcydUxmCWXFD8oFD2DPB160AKAWqXEx7cMx/s3DcX6JybirnHRctsvANCpVfBxFzOi2SVGh8zsmbxylNZYpZ9XasRPKeLjp/QPxqOTe+GDWUMxvLsf+kiZ2cwS/HU4E9tP5+GPgxkoqjAjv9yEKqsAhUJcuJSaV45T2aVyUBjkpYebVgW9RnzuujKNFqvgkN2VCIKAn/aKC76uHhqB0dFiucOeeoLZPw6ex6hX12JHA5lUQRDkYFbqfyvVqgLV9bJBXnrEDxRrZdcmZbXaH+U/D2Vg8eqkOp9P6qIQ7mPXzqyO9yvNlpX199DCU6eWg1l2NGi5SrOFbc5aSNrau9RYheTc+rugUPvacyYfI15Zi1dWMWPuyhjMkksKNuixdt4EzJ3Us9Z9bloVZsaGOWy56/BY225kWcWVcs2sRiUudDpmV2ogCAIW/p6EsioF+gZ7YsmsYXh0cm9cPljMTPYOEoPZ80WV+HhTsvy4s/nlck2uv4cWY2ytxmZ9tkvOHEpjk7KzdWUan/7lIEYvXlcrCP1mZyp2JOdBpVTgmmHhGGkLZg+eK6qzs8LnW1OQXWLEWwnH63w/ALHFWGZxJXRqJa4cIvYBPmcXzEqvJ8igw6hoP3jp1MgtNcmbVCzdloKx/17fqLrjmkqNVXj8p3/wf5uTsTO5dsBdHcy6V9fM1rEATCoxkGqtpYx9Lre/bbGX/jiK8W9swJ4zrbPQsCuyX2yaeLZ1Wul1NqeyS7HiQHq7Zkl32f7N4WfbtTGYpS4nyFY3eyKrBEbb1rKjbAGhfd3sxhM5+PtoNpQKAa9fOxBateOvi7e7BiG27gdJdo87V1Aud0sI8tLLrcZySozQqBS4c2y0nGGU6mZrZhqPnC/Cz/vOQRCA5XZttxLPFuKlP44CEEsqpC4OgV46mCxW/HO20OF5CspMSLTdtudMgcP2v/akDg1xMf6ICfQA4Fgzmy2/Hh20aqW8ScW6pGyYqqx4b91JpBdW4L9r6g+Y67P6YIa8SUZyruPuYoIgOHQzaKhmVsrMSm3g5DKDkoaDWWOVBQ99dwDv2+qh20ul2YJT2XVn6PLLTA4bRTjbtlO5EARgzZHmlZak5pXV+UWlK7G/ynKwgZ0KnaGk0lzvQtb29NB3B/DoD4nYc6Zl9f9NkWprz2jfD5xcD4NZ6nKkIOegbaMBfw8thkaK/WztM4u/23qtjg0W0N9Wc1tTb1upgb1zBRXyH65ggw7XDovATaMi8fAlPbH16Uvk3dEAwK+ezOzbdr1p1x/LhsUqoNRYhQe+2QezRUD8wBB5sZtCocAoW2eFmtmFzSdzYJ/k+HpHKuryl63+9ZK+QQj3Fduo1VVmILUuu7Sf2KFi3bFsrEvKQkG5Wf655q5ogiCguNKMUmNVnR0Qftx7Vv7vmlvlHkovQpnJAqWiRjeD8tqdG6Q/SjWD2ZwL1MxuOJaD3/85j3fWnZTLTKx1dHOoqaDMhO92p9XbZ/hCHv0+EZPf2oS9NebMYhVw7UfbMfXtzQ3W+2YVVzp8iWqqKou1UR0pKs0W+Q99c4OM+77ehxs/2YlfD7ROP2RXZJ+ZrfmlU1JqrMK0tzfj2V8PtdOoRE/9fBDT3tlc77jaQ3ZJ9ef5dE77lWFI/24UlJtrlZmR62AwS12OFJBJ2ZEQbz362zoiSJlZi1XAhmNitnKIn7Xe5+oT7Cn/d6xtUZp9mYFUG7v4msGYN7VPrT62fu61M7MH0gqwNikbSgXgrlUhr8yExLMF+GHPWZwvqkSknxve+Ndghx68I6PEYLxmsLHJVgcrBbsrEtNRWOMSfXphBRLPFkKhAC4bGIJIXzEYPFdYIdewSq9HGv/EPkFQKsSM9JKNpwBAzlx/vLF6pzVBEHD70j0Y/OIaDFz4N/o8/xcGv/g3LntnM37Yk4ZT2aVy2zIASMktd3jsot/FLPQVsWHQa1TwqyeTDdTOzEplBnkX6GaQYOs0YbEKct3xO2tPYPRr6+TPACBmr+wD1w83nsL85Yfw2ur6a+2yiyvx0u9HHbLcgHg5VepGsatGrfOulDyk5Jah1FhVbzaz0mzBNR9ux+Xvb21yRs1YZcH/bTqNIS8lYOb7W2G21P/5BsSNQaRS5sPpRaho4mJFQRDkjPvzvx5GcjsGKq1l+6lc5LVwIaF9fX5SRglMVbXf9z1n8nE8qwS/7DvXbovEKs0WrLN9zg+lO6/8YWdy9e9BRjtelUjNr/4C7YzsbKXZgpQaV6So6RjMUpcTZMvYnbF9Iw/11suZ12OZJaiyWLE/rQAF5WZ4u6kRXXdSFgDkILhHoAeuGx4BQMzMSmUGUiuw+kiZWekyviAIePNv8VL9tcMi5BKFv49kYdn2FADAnAkx8NJrHJ5naDcxmP3nXKFcb2a1CvJOXY9N6Y1+oQZUmq3y4jHJn4cyAAAjo/wQ5KVHiLceSoXYjiu3TPwDnmXXhUEctxbDbOc8nC5+AfjPdbEAgN8PZsh/FHal5GNzjd3CiiurcCyzBE//cgj3frUXAOBrC+rtM7MrE89jX2oB3LUqPBPfz3Zc/WUG0jm7+Tc+M1tlsWL9sSz55+2nc2G1Cvhuj5gtlt6/MmMVJry5EVd+sE0+VtrK+Ke952p9QZC88fdxfLEtBZ9uTna4/asdZ+T/Pl2j1GC1bT4AYG89mdDvdqchvbACFqvgkNm+kLS8ckx7ezMW/3kMpcYqnMgqla9QCIKA45klteoV7UshqqyCXLbSWKXGKjlwKzNZ8OC3BxrMZlutApIyimFpZjB39Hwx3lt3stZOgM215kgmZn22CwtWHmnR89j3aTZZrLWuYADAsQzxi4mxytroHQlban9qgTw/dS04ranCZMEH60+2eEFghcmC+csPyVuQ268NSC+88DhaQ7mpSt54BnBcJ9Be3vjrOCb9Z2OtfyepaRjMUpcT5OWYHQ3x1qObnzu8dGqYqqxYdyxb7gt7ca8AqGpvQia7fHAYnrqsDz66ebicETxbUC7/Axl0gR3Fetsyu9/vPouz+eVYuu0Mtp/Og1alxMOX9sLk/mIwu2z7GZzNr4CPuwbXDI2o9Tz9Qg3QqpUoLDfLQfqh9CLklZngqVNjRJQvbr1I7PxQc6taKXiaYdvRS6NSyrXA0j/u2TXKDADgkn7Vm2GM6O6LK2LDML5XACxWQd7C97MtYhB306huOPbyZfhnwVQkPHYxHrFtaiBl7KSFfKn55bBaBZSbquR+nHMn9USIrcOCVGtcYhcgAWJWVRqrXGYgLQBrIDO7L7VALpEAgO2n83DgbIHcAUEK5A6nFyG/zITjWSUosh0v/TGvMFvw3e7aAaXFKmC9LeNlH6QXV5rx877qLxT2l1SrLFa55AOoe1FKhcmCJRuqs98rDqRfMLsKiAH5vV/vxZm8cgR56eTPnpT9Xbb9DKa9sxlvr3WsHT6d7Zg1qlkWUVNRuRmLfj8i12dLix61aiX8PLQ4mlHs8Pprev3vY4h/dwt+2FN/kH4ssxi3f7G7ztrTV1cfxVsJJ/DA//bXel/WHs3Cl9vPNGmBkZS5b0kfZ2OVRX4fYiPETV7quqR/3C7APZPXPtm6badz5f+2D+zq88W2FPxnzQm8ZLtq0lx/Hs7Ad7vT8OgPiTBWWRyuQti3TGxLaTUysc7IzG63vf/SuoWm+mTzady1bA/KuniJBINZ6nJqZktDvd2gVCrkNl+vrkrCGtsfsEvr2L3MnkalxAMTe6JPiBcibLWmZ/Ora2aD6umoIPnX8AiM6O6LEmMV7ly2Rw7gnr+8HyL93DGhdyDUSoUcuM0a1Q1uWlWt59GqlRhoyxInnhX/6Eqttsb29IdGpcTEPuKiraMZxfI/fOcLK7A/rbrEQFKzblbKEtm/d1LWGACuGyEG2I9N6Q2lAvj1QDreW3dS3kb47vHR0GtU8HbXoFewFx6b0htvXR8LtVKBcB833HJRd/l1ZhRX4q/DmcgqNiLC102uDQYAg14jb7Frv6VtRlEFqqwCtCqlHHBLmdm8MhPqS/JJgcr4XgHye/O9XWB6MlvMlB232xgjNb8MgiA4ZHG+3H6mVuC0P61ADmDO2h37456zKDdZ5BZxp3PK5OBqd0o+cktNcLfNcVJGca0M4zc7U5FbKr43AZ5a5JWZ5HKS+giCgCd//gfHMksQ4KnDbw+Ow82jxc+7lBH70Zax/2JrCooqqs95yhZsh9u2pN5TI6hbfSgDn21Jll/D/3anYum2M/hgvVh+In2ZCDboMGtUN/l11SWzqBJLt50BgAYXjC1ceQSbTuTgkxoZb/G5xbnacjIXC1YekcdlqrLioe8OYOFvR7A/rbDe57YnCAK2nRKDjcziSof3pSmkMh2tWin3wf7nXO1L+sfsSkZq1o+3FfvNT6QrSg0fL74f207lyp/5jKKKOjPNDZG+dOeWGvHZlhSHS+3ttfhRqpeVnG1GtvlwehFu/XxXg60P62OxVpfgHG5Gice5gnK8/tdxrDuWjb+buTizs2AwS11OzcysFPzMndQTwQYd0vLLkZJbBrVSgYt7+Tf6eaUAsMJskS9B16yRrUmtUuKdG4fAoFfjZHYpzBYBlw0IkbOo3m4ajO4h1ruqlQrcFhdV73MNsS1iO2D7Qy1905/YRwzIw3zcEOath8UqyFkhKQs4oruvw1ilwCW9sMIhq2T/3vUK8sS4ngGICfTADFu7smHdfOUsq5SdndwvCDGB1bXFkmuGRWDjkxPx24Njodeo5Ixqam6ZXPs7fVAo9Jrq4F2pVMglCfZb2qbZ/ihF+LrJwa6fhxYKhfgHo9wuabHhWDbmfrsfW07mICFJDGZnjeqG3sGeEATgZ7vuEVnFRhRVmOUACRD/AOaUGmGsskKpEGtzM4srHcoDgOpd3wDxj44gCBAEAV/vFBfhPTa5N1RKBUqN1Zc6V9meY+bgMET6ucEqVM8nINbXfbxJzMo+fEkvuY3aL/sbXlj1za40rD6UCY1KgY9vGYYQbz3iYsTP9t7UfJzMKpEDzFJjFf63q3qhoFQGccPISADiZWmpBKDCZMGjPyTilVVJcrmCVHd83lYjKtWa+nvo0MPWKSM5p+5A7aONp+QvbvV1ejiQViDXGR+oEZTmlhrlz4VCIZZjfG/L8B45X4QKW3mDfWlJQ87klcuvQxxT81b8S4u/Qgx6uba+ZmbWVGV1eM01A62mem/dSfnKSH1KKs3yvAEXzsyaqqxyhrrMZMGBtEJUWay47uMduOL9bU3KqNpnQaUFr1Kde0ZRZbu055K+MEjLD87mNz2I/mD9KWw5mYu7v9xTb0BaVG7GRxtP1wrSz+aXy5/3I+ebXlrzxdYz8mO2nMy9wNGdG4NZ6nKCamVmxQDNQ6fGfFttJiC266pZm9oQnVolZy6lP5oXCmYBIMLXHW/8azAAINLPDa/XWNx1RawYKF49NFy+3F6Xod18AIjtu1Jyy5B4thBKhdihQDJC7nog/kGSgqfpthID+zEBYhAmXXLXqKoDSUDsovDN3aOx7vGJ8NSp5dsfvrSXw3bDd4/v0eBr97f9Aetuq3U9k1eOfalisDKiu2+tx0h1s+cKKvDqqqNYl5RVvfjL9hzieJXyscW2uHfPmXzc9/U+rDqYgVs/343UvHK51diYGDE7KwiATq2Ev62k4VR2qcPl37T8cjkrG2LQ43ZbRv/5FYcdAtgEu/+uNFuRW2pCTqkRqXnlUCrErHx3WwB/KrvUocRg+uBQjOwuzpX9Zf2jGcXIKzMhwFOLa4aF49phYkZ8XVJ2vXW7FquATzaLAfDTl/WVPwO9gjwR4KlFpdmKf/95DADkefxi6xlUmi2wWgW5wf/0QaHw0qlRaqySA99dKXnyH+Otp8R6Y2lBX44tgJM6dQR4ahEdIAazdS14ySiqcCjXSM4trXMRlBTMA+KXLfs6T2lr6e7+7pg3uTeA6m4Z9mUC65Iad0l36ynHAEH6ktpUWXbdTQZH+AAQM972q+eTc0sdtpOuGcyaqqz4eseZRtWqni+swFsJJ/DKqiT53IlnC3H9xzscSjN2p+TDYruiAVy4ZvbguUJUmquvQGw5mYPNJ3NwrqACJou1Sd017C/xS6/7itgwKBRizXB+O/SHlrLDA8PE0o+m1gFXmCzYeEL8LJWZLJi9bE+dpQrf7k7D638dw0d2i2MBxy9s5SZLkxZHFpab8P2eNPnnLSdzW/QFwFRlldduuCIGs9Tl6DUqGPTVwZd9gHjlkDA5gIq3u+zeWFInAED8th/gqW3U4y4bGIrVD4/HigfGwtvNMYC+fkQkfpoTh1euHtjgc0gB5NHzxfifLft3ce9Ah4B6hK3rwd7UfJzNL8e+1AIoFLWDWfsygyy7zgz2QXZ9NCol3rtxKIK8dJjQO1DeoexCuvuLgc4/ZwvloGF4XcGsLch87tdD+HRLCh767oAcnEvZXYlUN1tiViAltwz3fLUXJosVfUO85I0yxvcMgIdOjTEx1Vn48b0C5cV9J7JKHDoGpOaVyX+wIvzcccfYKAzr5oOSyirc/dVeLP4zCYfTi5CcWwaNSiHX+Z4tKJf/eHXzc4eHTo0etoz16ZxS7DlTgLwyE3zcNRgT4y8HnfbdHqRMUq8gL6hVSvQPM6BfqAEmixW/7E+v833deDxbrre+5aLqHfMUCgVG9xBfs7Sa/fGpvRHmrUduqRG/7D+H9MIKVJqt0KgUiPJ3xzDbfEgB9la7bND207k4kV2CkkoxQMsuMcJqFRwys1Iwm1lc6VDjV2aswku/H4XJYsXIKF9oVUpUmq21ukCczimTS4Ck360DdhnOk7bPTa8gL/zLVvryz9lCFJWbHYLZY5kljbqUvd0WzOpsnTpOZDWcmc0pMeLrHWdq9eOVOhkEG/QI9NIh3McNguCYnZUWf9kuLNSqmf2/TafxwsojePDbAxcMWuwDYeny95INp7D7TD4Wrz4m37ftlHif9IU3r8xUZ5cFiZQRd7NdLdl8Mhe/7Kv+3DUlmywdO8z2JRwALu4dIGdnz7fDIjDpis44W5nR2fzyJgWEm07koNJsRbiPG/qGeCGnxIhnlh+sdZz0e1szWD5VI3htSjeJr3ekotxkQZ9gL7hpVMgtNTqUqTTVwt8OY/Rr65zanq0lGMxSl2S/MCvE7r8VCgU+u30E3r1xCGaN7l7XQxsk1c0C4h9vtarxv2L9wwxyltKeQqHAyCg/6NS1a2VrnjvAU4sqq4CvbP1k/zXccbHYCFu270BaIZbbgp+xMQG1MshSmcG5ggrkNLIzg71u/u7YMf9SfHnnqEYFwADkQEe6XN8jwKPO90PaOEGq4y03WeTL7LWCWVvdbJEJuP/bRBSWmxEb6YNfHxiL9Y9PxPMz+slfEkb38JcDicsGhqCXbYe3jcezUWbXjio1rzozG+HrBi+9Bt/fG4c7xkQBAP5vUzKu+Wg7AOCiHv7oaQtYz+ZXB7M9gzwd/v9UdinW2UoeLukbBI1KKbdbO5BWKNcmnrG1LosKqH6dUknKp5uT6+wbu2z7GQDADSMiHUo2ACCuR3UAr1CIiwDvsmXS/29TshzER/l7QK1SypuLSAGlfeZyz5kCh+C2yiqgoNwk18z6e2rh466Vg3spWNtwPBuT39qEPw9nQqEAnpzWV/4s1Cw1+HRrCgRB3Fp6im1x5P606iBVCjZ7B3si1NsNvYI8YRXEcUpfCrxsX2TXH2s4O2uxCnI96dVDxXKOk/VkZkuNVbj3q724aPE6vLDyCO77Zp9D4JJlV2YAQP5SsL9GgA1Ub+CSllcdWFWaLfjS1gEj8WwhdtezdbXEPju47VQujFUWufZ3R3KevOhQqn+dMThU/nLXUHZOqmO+Y2wUADFTa38ForHBbLmpSu6hvGDmAKiUCrhpVBgZ5Ycw278951uwCGzt0SzMXrr7gi2vpM/gWNtVmTKTBYV19LCuj1SnGj8wBO/fNBQAsCeloFa3DulLWc3OKlIJj9r2D09jg9nMokr59/qBSTFyKdrWZpYaVJotWHHgPAQBLttVgcEsdUnSwiyDXg0Pu0vkAODjrsWVQ8Ll2sumiLQLppoS/LUGhUIhZ2dNFisMerXDIi0A6BPiJV8qlurprhwSVuu5pKA8vbBCzirVrDW+kKa+f1KZQYktY1dXVhaozswCwOyxUbCPlSNrBLNS9m5blhKnc8rg467BZ7eNgJtWhUg/d9w9vgdCvcXX6u2mwc2ju2NIpA+mDQhGL9tq/w22xVXSH3v7MgMpE69VK/HiFQPw8S3DEeqtl7NbU/oHI8Kv+ouBFJzF1AhmT+eUysGVNGcxgZ7wcdegwmyRN/OQMjxSFhsArh0ejlBvPTKLK+W2a2l55TicXoRT2SXYcjIXCgUcsrIS+2z08G6+CDLocdOoSAR4apGWXy7XPUvjvHJIGJQKcdHQ9tO5OJZZAoVCbK1mqrLii60pDs+fVWyUywykLyb2pQbZJZW47+t9yLD1T/7ijpEYFe3nEORLMsuBXw+InTjunxgjb3RiXzdbHcyKX0Skner+tytV3oFvtu1Lh/R+n84pxXvrTuKqJdvkBZiAWGNbVGGGl06Na21fCuvLzP51OBNrjmbBYhXgplFBEBxbrGXV6NMsZSPtA3FpAdWU/iFQKMTfA+lS+8rEdIeuHHUtfLNnv5Bp++k87E7Jl3fZA4DvdqVhw/FsHMssgVIh7vwXJG/zXXfdrNlilVvFXTUkHL2CxBpzk93Cx4YWrX20KRn/l6REhcki16Ya9GoMifTBt3ePxv/uGQ0PnRphtitlLVkE9n+bT2PD8Rzc/82+etvAmaqs8jl6h3jKfxMauwjMVGXFWtsX0MsGhqBnkCf8PbQwWay1dlqUzpNd472VMrPSosDGLAI7X1iBGz7ZgbwyE3oEemD6oFCM7yU+fvPJHBw8V4gZ721xKMe5kO2nc+XSuGMXuPrQUTGYpS5J+odLCmRai32ZQWPqZVub1G8WAK4cEl4rE6dSKjDUFiSWGKugUysduhhIwnzcoFUrUW6y4DNbgNLWwbkU5EikkoiaBttaG80eG4WFMwfg+uGR8n31ZWbPlIqB6C2ju8u31eXlqwZixdyx8NJr0MsWUEmBqVRTm1lcKWdU7DPxgPhHbd3jE/Dwpb1w9dBwXDMsonoTCrsyAynrK20dvC+1QC5LkDorKJUKDLfNp3SJXKrxi7KrDdapVbjvYjGb+tHG03hrzXFM+M8GXP7+VsS/uwWA2JWjZqAPiO+5NK/S58Bdq8acCTEAqjcRkRbwRfi6y8H2kz+Jl1MHhBkwybbIUFosJX2PySqplMsMpC8WcjCbU4ZdyfkwVVnRM8gTax6dID9PTB3B7G9pSlgFYNqAYAzr5ivXiB88Jy5CEgRBLk+RvohI76WUYR0Q5o3pg8WSmm2nxEU7l/53E95KOIHEs4X4ZHOy3LFAugQ/uoc/+tp2+ssuMcqt2exJi97uGR+NZ2eIdfd/HKwOZqUFYMG2QE36orY/rVCuC5bKDGIjvBFq+7cj1XbZ+7Mt4u/gTaO6QaEQy0JONhB02NejphdWYJmtQ4T0+/Hz/nN45hdx/u4YE40AT538OaivbvbgOXEBna+7+LshBVCAuMgTqM7MWqwCViamy8+1LzUfb609haOFSmw+mVurxn10D3+5b7X0b3JGUfPKDOw/B8cyS/DKqrpbiJ0rKIdVEDemCfTUyb8fjV0EtiM5DyWVVQj00mFYN18oFAr539/9qYUO45FeS26pUV6wJQiC/Pm+ypb5P5ze8CKwvFIjbvxkJ1LzyhHh64av7hwFjUopf853p+Tj9i9248j5Yrz+1zEcsPuy1JCEo9VXKTrCtsbNwWCWuiQp0GxoQVVz2Ac3F2rL1RbsF17VLDGQ2C+qmtw/uM5FbnqNCs9c1hdAda/ZC/XMbalwHzf5chtQvVitphtHRmL7M5dg4cwBAIAnL+sDPw/x8nWUv2NAbB+4alQK3Dam8aUjUnZQMibGH546NQQB8sYBEb61A0R3rRrzpvTG2zcMgadO7fBH8mSNMgMpaDPaAuaLevg7zMfAcDFwP9JAZhYAbhzVDQGeOqQXVuC99acgCGJdo9ki/mGcPTYadVEoFHgmvi8uGxCC60ZUfym4uUbQb/9e3G7LbEqXTsf2DMCYngHy/SqlQp67nGKjvAObVF4gBbPJuWXy5fJxPQMcWs7JmVlb5mpXSj6OFCihUirwlO1zGRPoCS+9GpVmK45lliCnROw8oVRUB9+jo/3lnekA8bPfJ9gL4T5uMFZZ5Z32JvYJRKCXDoJQfelfuiw/rqc4J1LG8EQdHQ2kfsAX9fBH/MAQKBVi8CfVZNYsM+gXaoBeo0RRhRnJuWUoLDfJAW/vEC95flPzyrDpRA5OZpfCU6fG/Ol9Ma2/+KXj/xrIzkplBtKvk1QT/fRlfRHmrUdhuRlZxUb0CPTAU5f1EcfmLWVm6w4id6WIwf2oaD8olQqM7y3OuVqpwMO2vtFnC8phsQpYcSAdj3yfiKuWbMPZ/HKHDScOnC2Ug9nufh6oKcynZZlZ6XOgUIilM9/sTMNXO2r3Frbf/lqhUCBSaq3YyMystFhzav9gKG1v9LDuPgAcM+5FFWY5K24VgDzbRjQ5JUaUVFZBqRCvxrhrVagwN7wI7JudaUjLL0eknxt+uC9O/venV5Angg06GKusKCg3Q6tWQhCA+csPXbAHtdUqyCVOgHjFpLlbdDtThwhmlyxZgqioKOj1eowePRq7d+9u8PjCwkLMnTsXoaGh0Ol06N27N1avXt1Oo6XOoI8t09I31KtVn9c++9XWwV9dhnf3xaBwb0zpHyxnMGuyz3heZWvtVJc7x0XjoUt6yj+3dXCuVinlLwN+Hlr0CKj9hw4QAzCprg4Q2/n89eh4/PnI+Fo9eAPsam5nDg5tUqmEj7vWIaDrG2qQM1vSpdVIvwtn9qXXlJRRLHeGkDKyBr3G4X29pEZf4wG2RWhHzhehqNwsb/DQ3d8xiNZrVJgzQczOumtVeOeGITj44lR8f+9F+Pbu0RhrF2zWdPXQCHx863CHhYduWhXut2VnxfFWB7NjYvzl8QPA+J6BGNuzulxhYJhBzhxnFVfKf7z9bbvd9bALZqUA6aIejl9cpDrjU9mlEAQBb/wtljvcOCJCHotSWV1WcyCtQM7GRfl7yFck3LQqhwWII6LEDNqtcd2hUAAzY8Ow5rEJWDZ7FCbaLvXuOZOPSrMFu20BqvTe9bKVLtQsNcgpMSI5twwKhViTHuCpk9ue/XHoPARBcOhmAIiLJAeHi2Pfn1Yg18tG+LrBoNdUd/bILcf/bRKD1htGRsKg1+A+2zz/vO8c/jjouAGKROprbP950qgUuLh3AG6y9fpVKoD/Xhcrv1dymUE9O49J2efR0eJrG98zALdc1A0LZvbHgDBvaFVKmC0CzhdWyPN6vqgS09/dIn8ZA4ADZ4uQZvtSVtfVArlmtrACxZVm3P/NPvzUhF3u7D8H0md4wcojuOervQ71wNVfDMUxSIGhfb3x8cwSPPTdgVoLEQVBwEZb68OpA6qvbElXUvanFcjBc83Hyhuy5FQvBnXTquRdKBuqm5Xm+5FLe8vrGgDx30SpVKF3sCf+fGQ8fNw1OJZZgqXbUup8Lsmh9CJklxjhoRUXRlusQr1t8ToypwezP/zwA+bNm4eFCxdi//79iI2NxbRp05CdXXdxvslkwpQpU3DmzBn8/PPPOH78OD799FOEh9f/R5mopiuHhOOX++PwmK19T2uRtoIF2r9mFhCDmt8fGodPbxtR78KroZG+CPPWo0eAh/wPYH3mTemNByf1RO9gT7n+sC1F2QId6bJdYwV56ess67APRu9sQlZW0ssuI9k3xMshiFQpFQ6LB+sj/cGWakdDDHqH7Kt91rNmjfMAW2b2VHapvIFDkJcO7lrHOm8AuHNsND68eRj+fvRiXDU0HBqVEhf18HfImjbFrNFi791wHzf5sj0g/uGUsrM6tRIjonwR6u0mB6kjo/zkucgsrpTrPuUyA1sgfDyzWA48RtbIwvcI9IBCIWa1ftx7FgfTi6FVCnhwkmObN+my7oG0QjnItB8rAFxsdzlcWnh138U9cPrV6Xj/pqHy+y+NYe+ZAuyzbfEa5KWT75d2TKu5CEzq7NAn2AvettZ1l9v6Lv/xTwaKK6rkdlb2n1H7RWDSBidSOYOUmV11KAM7kvOgVipwp23zkKHdfHGnLdM+78d/au3IVmGyyAHTjSO7ybePjBJbDd4a1x2X9g3CoisHOpQlSWOrKzMrCIIcZEnlHWqVEq9cNQi3xUVBpVTIX+xS88rlOmadWinXwN92kTiWQ+lFDoFcTVIwm1FUie93p+HPw5lY/OexOtu01UX6HPQM8sQTU/vgmfi+0KqUWJuUjds+3y0HmdUlO+J7HWlX2y7575rj+P2f8/hmZ6r9KZCWX46MokpoVAqMsvvsDo7wgVqpQFaxUQ5iM2p0ZZAWrp6ucZVGugqzv57SgOOZJTiZXQqtSompA4Jr3f+47bV+e89FiAn0xLPTxXKXd9eerLUwtNJswRM//YNFvx/Bt7vE9l4T+gSiny2grq/U4GRWKZanKDtk5tbpwexbb72Fe+65B7Nnz0b//v3x8ccfw93dHV988UWdx3/xxRfIz8/HihUrMHbsWERFRWHChAmIjY1t55GTK1MpFRje3a9WTWlLaVRKuearqQum2oubVoWEeRPwx8PjHC7B1kWhUOCJaX2w5rEJ7VIDLK3kntq/9j/WzTEk0ge9gjwwNtgqZ+ObQgpmfd3FDKr9H98wH32julWEGPQO5RM1gy0p09g72LNWpirMWw8fdw2qrILcQaBmKYVEqVRg+qDQOrNdzaHXqPDbg+Ow8cmJtX5P/jU8AjNjw/DUZX3l+26N6w5vNw2uGhouZ5tPZJXIu69JC/ek8UsBXq8gz1pdK/QalVxr/MoqcVHW+BDBIdMOVJfMrDqUgZWJYncOafGXZHL/YGjVSgwK95Z/JxUKhXxpWDLS9tlLPFeIDbbL8uN6BshfqurLzErtquwD8mkDQqBSKnA0o1juQ+rjrnF4H6VFYAlHs/CpbTHmNba+wVJmW8qQXTMs3CET99yMfpjaPximKitmL92DhSsPy6UvUhcFL70aF/cOlHeTk7K0Pu5afH7HSLkLhqS6ZrZ2ZjajqBK5pSaolQo54KlJCsAPpRfJ5TT/u3s0egaJX4Sfuaw3PNQCzBYBu5LzbY+pI5i1K3eQtjTOLzM5ZHftSYHZd7vFoEw6d+9gTyiVCsyZEIPfHxoHD60KxzJLsDdVzJpKm8pIV+ekz5tUZmCxCnL3hrQaXRqkdmdDIn0crga5aVXy+yPtMlezK0NOseNW2dLv/zjbl85f9qXXufnE77ZtyC/uHQhDHaVhwQY95kyIkX9HrhseAW83DcpMllpfwNYmZeHnfeewdNsZ/GDLek/uFyx/mTqeVQKzxYpX/jgq92muNFsw76eD2JSpxOu2KyUdSe2v9+3IZDJh3759mD9/vnybUqnE5MmTsWPHjjof89tvvyEuLg5z587FypUrERgYiFmzZuHpp5+GSlV3YGI0GmE0Vv+CFheLvxRmsxlmc/O2J2wM6bnb8hzUtpozh7eMjsTfR7MwLMKrw8691haDdbTx3XFRJCb09EOvIM9WGZteBaycMwoJCQnNer6YQPEPXN8QL1RVVSHcpzqYCvfWN/o5Q7318qXfaH93h8dN6uOPr3em4uZRkXU+X/8QL2xPzscq2yXGSL/Gn7elVABgBcxWx0yMRgG89S+xpZk0lltGReCWUWIwlmbbaEFaQObjpgGsFpitFqggBizSYrGRUT51vp6YQHek5ZejpLIKerUSE0Orah03urs3Lu0biHXHcuTtYXv4uzkcF+GtxR9z42Bw0zT4voUbNPD3ELcHlgKji6J95cf08K8uFykorZA3mNhtu6Q+LNIgH+ulVWBS7wCsPZaDp20LrYI8dQ7nHxwmBjFSxn7GoBBM6RsAs9mMMO/qjh1KBXDPuO61xv6fawfijlIj9qcV4ssdqfhyRyr+c+1AeNnKRSJ83KAQLLh1dDesOZqF+AFBDb5+f3fx9WQUVdQ6LtG2iUnPIE+oYIXZXLsOM9JXDEKlLxURvm6IDffC6gfjAABVVVWI9hJwuEAhb5IQaqg9J946JTQqBcwWAaftdorbcCwTfYNrB78/7z2Hn/edw6qD5zFzYBBO2LpC9LD7Pevhr8dlA4Pxy/7z+GlPGowmM1LzyuGhU2FSL3+H9/xsfjnyS8qRmleBYlvP5NS8ModxbjslZtJHRfnWGv+QSG8cSi/C3pQ8xPcPxNkaHR4yCsthNpvlBXxRts/rxT19MaK7D/amFmLxqiT897pB8mMEQZCD2ekDG55He/1CPLEzpQCHzhWgT1D1e5doy/76umtQUG6GQa/G+Bg/lNm2zk46X4Tl+9Lkxb8VRjNScstwLKsUnmoB94yt+9+qttDY8zg1mM3NzYXFYkFwsGMWJjg4GMeOHavzMcnJyVi/fj1uvvlmrF69GqdOncIDDzwAs9mMhQsX1vmYxYsXY9GiRbVuX7NmDdzdWyeL0ZCEhIQ2Pwe1rabMYRiA2RHA1g2c9+Y61QbP2ZzfQ10VMCFUieHu2Vi9ejUyCxWwhXgQSvMaXavvZlVCuhBWkZWC1asdF++8NRpQ5R7C6tWHaj1WXyE+Nt12ubIy5yxWr06rdVxHcqYEANQoM4pBsA4mh/fKE9Xvh6YgFatXn6n1HIqS6mNGB1TBoK17DuO9gXPeShwvEo/NPH4Aq88daNa4w3VK5JUp5b7CFWcSsTojEQBgsgCeahUKys245t11uK+vBQoASRkqAAoUn3Y87wQPYL9OhXyjGPgpjcW1Pi8BOhVyjQp4awSM1Z3D6tVia7VKCyD9eY71s+Lork2oa03+LWHAKHcF1mcocKJIiS/WHUQ/HwGAChpTEVavXo3+APr3BvZuWdfga88sF895Pr+01jhXpYlz4W0pqvczX5oh/m5I9b9ByrJax0Z5KXDYdhVdCQGJ2zfiUB3VRAa1CnkW8Q6dSoDRosDKXSfRvax2XPDZIfH9rzBb8f6Pa3A0XQlAgawTB7A6vXo+QivF8f2WeA7HUs4CUCLW24xN69YAEHf+C3ZTIasC+O/3a1FiBqTf9eSs6tctCMDmJPGcyD6J1asds5SKPPE8Gw6lYrgiGftOiO+dVinAZFVg7+ETWF1+DEfOis+RffIfrM78BwAw0QDsgwq/HcxAjPUsomwXGc6WAqn5amiUAqpSG//51leK5/5rxyG4284BAJuOiLdPC6lEL28BSkUVtm9MQL7t9/af1FycycgVXyOAF38/CsH23zf1tCJxx2YkNmoELVde3rgFeU4NZpvDarUiKCgIn3zyCVQqFYYPH4709HS8+eab9Qaz8+fPx7x58+Sfi4uLERkZialTp8JgqPuSSWswm81ISEjAlClToNE0fltU6jg4h66vpXN4jd1/Dyoox4dJWwEAFw3qhemTYup+UA3bTEdwwrZT0pWTRjd6VzQAqPonA+t/rg5yp8YNxfRBTd+drj1lFFXi7cOb5Z+7Bfth+vSR8s97rEk4sUu8fHnv1ZfUubiwfH861v96BBqVAi/eOA4Hd22udw6nTK3Ck78chrHKitnXDm1Wj2gAyPJJxcE/jwMQF6rNunqsw/09hhZh9pf7kFJShaVpPhgS6Q0B6YjwdcOsq8fXer4RY0pxw6e7UVxZhYExkZg+fYDD/afdTuOLbal476ZYh56/APBpyjak5pfjpRvHypd/63N5ZgkuX7IDp0vVGNwrDDhzDiP7RWO6rVNBY5RUmrH4nw2osCgwcfJUh7rsX77aByAP8RcNwPRRkXU+3uNEDn45Ux1kTR/dD9PjqksZzGYzTv1S/WUk3NcdM2fUfs8A4JuMPciz9bR9bkZ/LPgtCallSoy/5FJ50wtAvFR/Zsd2+edkRSgqLGKHituumgadXVmH1Srg17e3IL2wEgfzxc/H41fHOSyUTXY/jffWn0aaIgjQAoCYdS+3KDB20hR4u2mQkluGop3boFEpMOdfU2uV4AwuqMBXb21BRoUSEydfiq/S9wN5hRgc6Yu9qYVw9w/B2EkDULRjAwDg1iunONTQp2gO45f957H0tBsu7hUAg5sa69JyAFTi0n4huHpm40sqjQfOY+Pyw6jQ+8u/f1argOf2bwBQhVmXjUM/u0XQpcYqvH14PYrNChSbxf7ZMwYG49dEsc3crJHhGKhObde/h9KV9AtxajAbEBAAlUqFrKwsh9uzsrIQElL3P9ahoaHQaDQOJQX9+vVDZmYmTCYTtNra24fqdDrodLX/sdRoNO0yIe11Hmo7nEPX1xpz2M3fC2qleJm0e4Bno5+ve0B1nWy/MJ8mjSPWbpEOAMQEGzr8ZzHEx/EPfKCXzmHMMbY+u9EBHgj3c6whlkwbGIYf9qbj8sGhiPD3xEHUP4feGg0+uW1k7SdpootiAgCIwez4XgG1zjU8OgDf3XsRbvlsF45nleK4rRZxVLRfnePqF+6LL+4YiXfXncSsi7rXOmbe1L54ZHKfOoPvb+6+CCWVZrlWtyEDInwR7uOG9MIKrDoktoyKasLnEwB81Wq4acT2UAUVVpzIKUG4jxtCvfU4cl7Mtg7pVvfrBICYYMfuKSOia79/3Twg//5EBXjU+1zhPm7YgwKEGPS4+aJoLN2ehpTcMuxJK8I0u+4BK/4RX2uApw65pUYkJIl1sFH+HvB0r13jf+2wCLy3Xrzu0zfEC8Oi/B0Wml49LBLvrT+N7cn58iYpKqUCFquAjGIzAgzu2JMmlrMM6+YLrzrOERWoludi/7kSucfs0G5iMJtbZkZyXqX8Ov28HK8OPx3fD7vPFOBsfgV+s+tVrNcocfuY6CbN6SDbxiJJmSVQq9VQKBRIzilFqa3HeL9wH2js6v59NRpE+rnJvXavjA3D69cORpivO7KKjZgf3wfrE1Lb9e9hY8/j1AVgWq0Ww4cPx7p11Zc/rFYr1q1bh7i4uDofM3bsWJw6dQpWa3XNzokTJxAaGlpnIEtE1FrUKqW8YMO+XdWFSO25fN01dW7R25DoAE+42WV/utWxaKaj0aqV8LfbqU1qyyWJHxiKQeHeDu2/avLz0GLF3LG4e3yPeo9pbf1DDfKCqframQ0I88YfD4/HM/F9ET8wBEMifeStjOsyIsoPX981Wt4UoKb6ssgh3vpGBbKAuKBtUl+xa0OJrc4zookLARUKhdxr9tMtybju4x244ZMdOJNXjrwycfFXQxnicB83uZOLVq2UW03Z06ogZwIbWqgYa2u5dstF3aBSKnCxbVMA+61WzRYrfrFtyb1gZn+H35Gaiywl0gI7QNqAwvG9jw7wwOAIb1isAirNVvh7aOXMrdQbd2dydU/huigUYgs0ANh4LFtudTbEFlhml1TiuK2ut65FqUFeeqydNwHf3jMacyfF4Pa47vjk1uHY9/wUueVbY8UEekKjUqCkskru0iB1pegfZnAIZCV9gqvn7fYxUVAqFXhyWl/8x66NW0fk9G4G8+bNw6effoovv/wSSUlJuP/++1FWVobZs2cDAG677TaHBWL3338/8vPz8cgjj+DEiRNYtWoVXnvtNcydO9dZL4GIupC3bxiCt66PrbePb11GRPnBXavCJX2b3qVBpVTIK679PbR1rmTuiOz7LPt7OiYaQrz1+P2hcbh+ZN2XrJ1FrVLi+Rn9ccOISEzsE1TvceE+bpgzIQYf3TIcK+aOxeAIn/YbZD1q9iiuq+3VhUjlHv+ztWs6m1+B+cvFBWy9g70aDGa0aiXCbV/aBoQZ6u2UckkfMegeWc8OfwBw60Xd8fuD4zB3ktjnWmoLuOlEjtyia11SFnJLjQjw1CJ+YAjG9ar+8iHtsFdTVIAHbhoVidhIH1w9rO52nlfEVm/vHRfjL3ffSLPtxiZ1MmgosJTawa385zwsVgFqpULuGZ1dbJTrimt23pDo1CqMiQnAk9P6YtGVAzF1QEitbdcbQ6tWyu+FtBjzoG2h5ODwuv/96m/7t2ZklK/cLswVOL1m9oYbbkBOTg4WLFiAzMxMDBkyBH/99Ze8KCwtLQ1KZfUvRWRkJP7++2889thjGDx4MMLDw/HII4/g6aefdtZLIKIupH+YAf3DmlZrH+7jhv0vTIHuAq3Q6jMgzIADaYV1tjLqqIK8dEiyXSW1z9J2dLNGd8Os0d0ufGAHMyYmAHqNUm55Zt/Kq7Hs2+/5uGtQWG6WM5GDGhHYRPl74Gx+BYZG1h+ozrk4GpcNCmswy6tWKTHI7sviRT38odcoca6gAu+vP4UbR0Xi+RXirmL/Gh4JjUqJyf2CkGBrX1dfZhYAFl8zuMHXcPngMLy6OgmCIL6n0s5safnlOJ1TitxSI3RqpcNuizWN6RkAlVKBQtsmJyHe1X2wjVVW7LXVA1+oFro19A8z4GhGMZIyijFtQAgO2YLZQfV8Abt9TBTyy024Y0zduwZ2VE7PzALAgw8+iNTUVBiNRuzatQujR4+W79u4cSOWLVvmcHxcXBx27tyJyspKnD59Gs8++2y9bbmIiDoCvUbVpI0g7I2NEbNOQxoIEjoa+01DmlpaQU2n14jZPEDsbdycS8LSnLlpVPh5TpzDhh6DGnElQuqHW99W2oAYqPYLNTTpd8FDp8aiK8TFc2+vPYEbP9mJ3FIj+oZ44RHbVrqX9A2G9JT1ZTwbI8Rbj2uHRSDEoMfk/kFyhjstvww7bIH9sG6+Db6/3m4ah2A3zMcNbloVvGzZ1eO2tlzN6X3dVFK5x9HzxbBYBRw+b8vM1jOf/p46vHLVoFrbeXd0Ts/MEhFRwy4bGILVD493qT8w9puGuFJm1pVd2i8I649lo4fddsNNET8oFOuSsvHYlN7oGeSFp6b1wb1f7wPQuMzs1UMjcPXQ+gPZlrhhZDckZZRg2fYzSMktg5dOjY9uGS5vWhDopcOz8f2QXVLZ4oznf66r7hggXQ1Jyy/HzkaUGEgu7hWIfaliBlbaCCLQoENJjljTrFYqmlR331zSVaSjGcVIzilFuckCN42qXc7dnhjMEhF1cAqFosmlDc7GzGz7u35EJIorqi64TXV9hnXzxfonJso/T+kfjJtGdUNRhUmu+XSm52f0w7mCcmw9lYu3bhiC6ADHoP2ei1t/saCUmT1fWIkyY+OD2Ql9AvH2WrEHrbRFb5CXDsm2jSB6BHpccAfG1iDtSHauoAIfbToNABgYbmh2+7qOisEsERG1ukC7zGyAJzOz7UGjUuL+iY3rfdwYCoUCi68ZdOED24lapcSnt41Amcki78DW1gI9ddCplTBWWZFfZoJeo2zU4s9B4d5y3XF1MFv9O9EnpH2+HHi7aeRWYctt3R+GdXedcqXG6hA1s0RE1LlImVm1UuEyHRio41MoFO0WyAKAUqlwaCM2orsfdOoL1yOrlArcMro7PHVquc1boN3mIH0aWKTW2u6b0AN9gr0wfVAI5sf3lbtEdCbMzBIRUavrFeyFQC8degd7QtnJLmlS19Ldzx2nssUNMprS6/Xxqb3x+NTe8mI3+53u2iszCwC3xUXhtriodjufMzCYJSKiVuepU2Pr05OgUfICILk2+8zsRT0avxV1zY4NQXZ15O3RlqsrYTBLRERtojGXY4k6OmkRmLtW1aINMqSaWQ+tqll9gKl+DGaJiIiI6jEyyg8KBTC1f3CdW8A2VmykDwaGGzCuZyBLb1oZg1kiIiKiegyK8MbmJychoIUt5jx1avzx0PhWGhXZYzBLRERE1AD7ulnqeFiZT0REREQui8EsEREREbksBrNERERE5LIYzBIRERGRy2IwS0REREQui8EsEREREbksBrNERERE5LIYzBIRERGRy2IwS0REREQui8EsEREREbksBrNERERE5LIYzBIRERGRy2IwS0REREQui8EsEREREbkstbMH4AyCIAAAiouL2/Q8ZrMZ5eXlKC4uhkajadNzUdvgHLo+zqHr4xy6Ps6h63PGHEpxmhS31adLBrMlJSUAgMjISCePhIiIiIgaUlJSAm9v73rvVwgXCnc7IavVivPnz8PLywsKhaLNzlNcXIzIyEicPXsWBoOhzc5DbYdz6Po4h66Pc+j6OIeuzxlzKAgCSkpKEBYWBqWy/srYLpmZVSqViIiIaLfzGQwG/vK6OM6h6+Mcuj7OoevjHLq+9p7DhjKyEi4AIyIiIiKXxWCWiIiIiFwWg9k2pNPpsHDhQuh0OmcPhZqJc+j6OIeuj3Po+jiHrq8jz2GXXABGRERERJ0DM7NERERE5LIYzBIRERGRy2IwS0REREQui8EsEREREbksBrNtZMmSJYiKioJer8fo0aOxe/duZw+py9q8eTNmzpyJsLAwKBQKrFixwuF+QRCwYMEChIaGws3NDZMnT8bJkycdjsnPz8fNN98Mg8EAHx8f3HXXXSgtLXU45uDBgxg/fjz0ej0iIyPxxhtvtPVL6xIWL16MkSNHwsvLC0FBQbjqqqtw/Phxh2MqKysxd+5c+Pv7w9PTE9deey2ysrIcjklLS8OMGTPg7u6OoKAgPPnkk6iqqnI4ZuPGjRg2bBh0Oh169uyJZcuWtfXL6zI++ugjDB48WG64HhcXhz///FO+n3PoWv79739DoVDg0UcflW/jHHZ8L774IhQKhcP/+vbtK9/vsnMoUKv7/vvvBa1WK3zxxRfCkSNHhHvuuUfw8fERsrKynD20Lmn16tXCc889JyxfvlwAIPz6668O9//73/8WvL29hRUrVgj//POPcMUVVwjR0dFCRUWFfMxll10mxMbGCjt37hS2bNki9OzZU7jpppvk+4uKioTg4GDh5ptvFg4fPix89913gpubm/B///d/7fUyO61p06YJS5cuFQ4fPiwkJiYK06dPF7p16yaUlpbKx8yZM0eIjIwU1q1bJ+zdu1e46KKLhDFjxsj3V1VVCQMHDhQmT54sHDhwQFi9erUQEBAgzJ8/Xz4mOTlZcHd3F+bNmyccPXpUeP/99wWVSiX89ddf7fp6O6vffvtNWLVqlXDixAnh+PHjwrPPPitoNBrh8OHDgiBwDl3J7t27haioKGHw4MHCI488It/OOez4Fi5cKAwYMEDIyMiQ/5eTkyPf76pzyGC2DYwaNUqYO3eu/LPFYhHCwsKExYsXO3FUJAhCrWDWarUKISEhwptvvinfVlhYKOh0OuG7774TBEEQjh49KgAQ9uzZIx/z559/CgqFQkhPTxcEQRA+/PBDwdfXVzAajfIxTz/9tNCnT582fkVdT3Z2tgBA2LRpkyAI4nxpNBrhp59+ko9JSkoSAAg7duwQBEH8QqNUKoXMzEz5mI8++kgwGAzynD311FPCgAEDHM51ww03CNOmTWvrl9Rl+fr6Cp999hnn0IWUlJQIvXr1EhISEoQJEybIwSzn0DUsXLhQiI2NrfM+V55Dlhm0MpPJhH379mHy5MnybUqlEpMnT8aOHTucODKqS0pKCjIzMx3my9vbG6NHj5bna8eOHfDx8cGIESPkYyZPngylUoldu3bJx1x88cXQarXyMdOmTcPx48dRUFDQTq+maygqKgIA+Pn5AQD27dsHs9nsMId9+/ZFt27dHOZw0KBBCA4Olo+ZNm0aiouLceTIEfkY++eQjuHvbeuzWCz4/vvvUVZWhri4OM6hC5k7dy5mzJhR633mHLqOkydPIiwsDD169MDNN9+MtLQ0AK49hwxmW1lubi4sFovDRANAcHAwMjMznTQqqo80Jw3NV2ZmJoKCghzuV6vV8PPzczimruewPwe1nNVqxaOPPoqxY8di4MCBAMT3V6vVwsfHx+HYmnN4ofmp75ji4mJUVFS0xcvpcg4dOgRPT0/odDrMmTMHv/76K/r37885dBHff/899u/fj8WLF9e6j3PoGkaPHo1ly5bhr7/+wkcffYSUlBSMHz8eJSUlLj2H6jZ5ViKiNjB37lwcPnwYW7dudfZQqBn69OmDxMREFBUV4eeff8btt9+OTZs2OXtY1Ahnz57FI488goSEBOj1emcPh5opPj5e/u/Bgwdj9OjR6N69O3788Ue4ubk5cWQtw8xsKwsICIBKpaq1+i8rKwshISFOGhXVR5qThuYrJCQE2dnZDvdXVVUhPz/f4Zi6nsP+HNQyDz74IP744w9s2LABERER8u0hISEwmUwoLCx0OL7mHF5ofuo7xmAwuPQ/8h2JVqtFz549MXz4cCxevBixsbF49913OYcuYN++fcjOzsawYcOgVquhVquxadMmvPfee1Cr1QgODuYcuiAfHx/07t0bp06dcunfQwazrUyr1WL48OFYt26dfJvVasW6desQFxfnxJFRXaKjoxESEuIwX8XFxdi1a5c8X3FxcSgsLMS+ffvkY9avXw+r1YrRo0fLx2zevBlms1k+JiEhAX369IGvr287vZrOSRAEPPjgg/j111+xfv16REdHO9w/fPhwaDQahzk8fvw40tLSHObw0KFDDl9KEhISYDAY0L9/f/kY++eQjuHvbduxWq0wGo2cQxdw6aWX4tChQ0hMTJT/N2LECNx8883yf3MOXU9paSlOnz6N0NBQ1/49bLOlZV3Y999/L+h0OmHZsmXC0aNHhXvvvVfw8fFxWP1H7aekpEQ4cOCAcODAAQGA8NZbbwkHDhwQUlNTBUEQW3P5+PgIK1euFA4ePChceeWVdbbmGjp0qLBr1y5h69atQq9evRxacxUWFgrBwcHCrbfeKhw+fFj4/vvvBXd3d7bmagX333+/4O3tLWzcuNGhnUx5ebl8zJw5c4Ru3boJ69evF/bu3SvExcUJcXFx8v1SO5mpU6cKiYmJwl9//SUEBgbW2U7mySefFJKSkoQlS5awJVAreuaZZ4RNmzYJKSkpwsGDB4VnnnlGUCgUwpo1awRB4By6IvtuBoLAOXQFjz/+uLBx40YhJSVF2LZtmzB58mQhICBAyM7OFgTBdeeQwWwbef/994Vu3boJWq1WGDVqlLBz505nD6nL2rBhgwCg1v9uv/12QRDE9lwvvPCCEBwcLOh0OuHSSy8Vjh8/7vAceXl5wk033SR4enoKBoNBmD17tlBSUuJwzD///COMGzdO0Ol0Qnh4uPDvf/+7vV5ip1bX3AEQli5dKh9TUVEhPPDAA4Kvr6/g7u4uXH311UJGRobD85w5c0aIj48X3NzchICAAOHxxx8XzGazwzEbNmwQhgwZImi1WqFHjx4O56CWufPOO4Xu3bsLWq1WCAwMFC699FI5kBUEzqErqhnMcg47vhtuuEEIDQ0VtFqtEB4eLtxwww3CqVOn5PtddQ4VgiAIbZf3JSIiIiJqO6yZJSIiIiKXxWCWiIiIiFwWg1kiIiIiclkMZomI6P/bt5uQqNY4juO/Y3UWOjqTNKTFgUNU1OhQ9rLIRUlJERS20iImFFHsZTELae0iLYNEK6JVaNKiIChwUdRUA7moqSB6EQJ10IUYoxgNLQzHu5B7aDLutZrJe7jfDwzMPOc88/+fsxh+PPMcAHAtwiwAAABcizALAAAA1yLMAgAAwLUIswAAAHAtwiwA/E/Ytq3Ozs7FbgMAMoowCwBZUFtbq0OHDkmSKioqFA6H/1jt7u5u+Xy+eeOxWEyNjY1/rA8A+BOWLnYDAICFmZ6elmmavzzf7/dnsBsA+G9gZRYAsqi2tlbRaFRdXV0yDEOGYSgej0uS3r59q/3798vj8WjlypUKhUJKJBLO3IqKCp06dUrhcFgrVqzQvn37JEkdHR0KBoPKy8uTZVk6ceKEksmkJOnJkyeqq6vTp0+fnHotLS2S5m8zGBkZUVVVlTwejwoKClRdXa3x8XHneEtLizZv3qze3l7Zti2v16vDhw/r8+fP2b1pAPATCLMAkEVdXV3asWOHGhoaNDY2prGxMVmWpampKe3evVtlZWV68eKF7t27p/HxcVVXV6fN7+npkWma6u/v19WrVyVJOTk5unjxot69e6eenh49evRIp0+fliSVl5ers7NTBQUFTr3m5uZ5faVSKVVVVWlyclLRaFQPHjzQ0NCQampq0s4bHBzUnTt31NfXp76+PkWjUZ07dy5LdwsAfh7bDAAgi7xer0zTVG5uroqKipzxy5cvq6ysTG1tbc7YtWvXZFmWPnz4oPXr10uS1q1bp/Pnz6d957f7b23b1pkzZ9TU1KQrV67INE15vV4ZhpFW73uRSERv3rzR8PCwLMuSJF2/fl0lJSWKxWLavn27pLnQ293drfz8fElSKBRSJBJRa2vr790YAMgQVmYBYBG8fv1ajx8/lsfjcV4bNmyQNLca+retW7fOm/vw4UPt2bNHq1evVn5+vkKhkCYmJvTly5cF1x8YGJBlWU6QlaRAICCfz6eBgQFnzLZtJ8hKUnFxsT5+/PhT1woA2cTKLAAsgmQyqYMHD6q9vX3eseLiYud9Xl5e2rF4PK4DBw7o+PHjam1tVWFhoZ4+far6+npNT08rNzc3o30uW7Ys7bNhGEqlUhmtAQC/gzALAFlmmqZmZmbSxrZs2aLbt2/Ltm0tXbrwn+KXL18qlUrpwoULysmZ+3Pt1q1b/1rvexs3btTo6KhGR0ed1dn3799rampKgUBgwf0AwGJjmwEAZJlt23r27Jni8bgSiYRSqZROnjypyclJHTlyRLFYTIODg7p//77q6ur+MYiuXbtWX79+1aVLlzQ0NKTe3l7nwbBv6yWTSUUiESUSiR9uP6isrFQwGNTRo0f16tUrPX/+XMeOHdOuXbu0bdu2jN8DAMgWwiwAZFlzc7OWLFmiQCAgv9+vkZERrVq1Sv39/ZqZmdHevXsVDAYVDofl8/mcFdcf2bRpkzo6OtTe3q7S0lLduHFDZ8+eTTunvLxcTU1Nqqmpkd/vn/cAmTS3XeDu3btavny5du7cqcrKSq1Zs0Y3b97M+PUDQDYZs7Ozs4vdBAAAAPArWJkFAACAaxFmAQAA4FqEWQAAALgWYRYAAACuRZgFAACAaxFmAQAA4FqEWQAAALgWYRYAAACuRZgFAACAaxFmAQAA4FqEWQAAALjWX+DV/ifKys1OAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: graphs/total_loss_over_iterations.png\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAArwAAAHWCAYAAACVPVriAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/OQEPoAAAACXBIWXMAAA9hAAAPYQGoP6dpAADCjUlEQVR4nOzdd3xT9foH8M9JmtG9By2FDvaeLXvIkioiLlRkqTiAnwMXKEO9KlevIk6G94KKC0RUVEBqFRDZe89SCt17pU3T5Pz+OPmenIy2aZrOPO/X676upMnJNzlJznOe83yfL8fzPA9CCCGEEEJaKVlTD4AQQgghhJCGRAEvIYQQQghp1SjgJYQQQgghrRoFvIQQQgghpFWjgJcQQgghhLRqFPASQgghhJBWjQJeQgghhBDSqlHASwghhBBCWjUKeAkhhBBCSKtGAS8hTezw4cMYMmQIPD09wXEcTpw40dRDapU+//xzcByHlJSUph4KIY2G4zi8+uqrTT0MQpocBbyENCGdTod7770X+fn5eP/997Fhwwa0b9/eqc/xzTffYOXKlU7dZnNw9913IyEhoamHQVxESkoKOI7Du+++K9527tw5vPrqq01+ErVt2zYKagmpBQW8hDShq1ev4vr163j++efx2GOP4aGHHoK/v79Tn6M1Brw6nQ6JiYm47bbbmnooxIWdO3cOr732WrMIeF977TWbfysvL8fixYsbeUSEND8U8BLShLKzswEAfn5+TTsQO/E8j/Ly8qYeBv7++2+UlJRQwFtHzWX/NVdlZWVNPQQAzh2HWq2Gm5ub07ZHSEtFAS8hTWTWrFkYOXIkAODee+8Fx3EYNWoUAODUqVOYNWsWYmJioFarERYWhocffhh5eXlm2ygpKcEzzzyDqKgoqFQqhISEYNy4cTh27BgAYNSoUfjtt99w/fp1cBwHjuMQFRVl9xijoqJw++234/fff8eAAQPg7u6ONWvWAAAKCwvx7LPPis/dtm1bzJgxA7m5uQCAXbt2geM4bNq0CW+++Sbatm0LtVqNMWPG4MqVK2bPM2rUKPTo0QPnzp3D6NGj4eHhgYiICLzzzjs2x/Xbb7+hW7duZq/lwoULuO+++xAcHAx3d3d07twZr7zySo2v78iRI5gwYQKCgoLg7u6O6OhoPPzww3a/P4AQfN97771o164dVCoVIiMj8eyzz9oMLO0ZY1paGh555BGEh4dDpVIhOjoaTz75JCorKwEAr776KjiOs9q2rRrlmvbf+vXrccsttyAkJAQqlQrdunXDqlWrbL7G7du3Y+TIkfD29oaPjw8GDhyIb775BgCwbNkyKBQK5OTkWD3uscceg5+fHyoqKmp8D//8808MHz4cnp6e8PPzw+TJk3H+/Hnx75s3bwbHcdi9e7fVY9esWQOO43DmzBnxtgsXLuCee+5BQEAA1Go1BgwYgK1bt9p8v3bv3o25c+ciJCQEbdu2rXGclo+/9957AQCjR48Wv1+7du0S77N9+3bxdXl7e+O2227D2bNnzbYza9YseHl54erVq0hISIC3tzemTZsGwL7P1qxZs/DJJ58AgDgG6efDVg3v8ePHMXHiRPj4+MDLywtjxozBgQMHbL4///zzDxYsWIDg4GB4enpiypQpVvvaGd8jQhoanfYR0kQef/xxRERE4K233sJTTz2FgQMHIjQ0FACQmJiI5ORkzJ49G2FhYTh79izWrl2Ls2fP4sCBA+IB7YknnsDmzZsxf/58dOvWDXl5edi7dy/Onz+Pfv364ZVXXkFRURFu3ryJ999/HwDg5eVVp3FevHgRDzzwAB5//HHMmTMHnTt3RmlpKYYPH47z58/j4YcfRr9+/ZCbm4utW7fi5s2bCAoKEh//73//GzKZDM8//zyKiorwzjvvYNq0aTh48KDZ8xQUFODWW2/FXXfdhfvuuw+bN2/GSy+9hJ49e2LixIlm9922bRtuv/128d+nTp3C8OHDoVAo8NhjjyEqKgpXr17FL7/8gjfffNPm68rOzsb48eMRHByMhQsXws/PDykpKdiyZUud3p/vv/8eGo0GTz75JAIDA3Ho0CF89NFHuHnzJr7//vs6jTE9PR1xcXEoLCzEY489hi5duiAtLQ2bN2+GRqOBUqms09gA2/sPAFatWoXu3bvjjjvugJubG3755RfMnTsXBoMB8+bNEx//+eef4+GHH0b37t2xaNEi+Pn54fjx49ixYwcefPBBTJ8+Ha+//jo2btyI+fPni4+rrKzE5s2bcffdd0OtVlc7vj/++AMTJ05ETEwMXn31VZSXl+Ojjz7C0KFDcezYMURFReG2226Dl5cXNm3aJJ4kMhs3bkT37t3Ro0cPAMDZs2cxdOhQREREYOHChfD09MSmTZtw55134ocffsCUKVPMHj937lwEBwdj6dKldcqsjhgxAk899RQ+/PBDvPzyy+jatSsAiP+/YcMGzJw5ExMmTMDbb78NjUaDVatWYdiwYTh+/LjZyVpVVRUmTJiAYcOG4d1334WHhwcA+z5bjz/+ONLT05GYmIgNGzbUOu6zZ89i+PDh8PHxwYsvvgiFQoE1a9Zg1KhR2L17N+Lj483u/3//93/w9/fHsmXLkJKSgpUrV2L+/PnYuHEjAOd9jwhpcDwhpMn89ddfPAD++++/N7tdo9FY3ffbb7/lAfB79uwRb/P19eXnzZtX43PcdtttfPv27R0aX/v27XkA/I4dO8xuX7p0KQ+A37Jli9VjDAYDz/Om19a1a1deq9WKf//ggw94APzp06fF20aOHMkD4L/88kvxNq1Wy4eFhfF333232faTk5N5APxff/0l3jZixAje29ubv379us2x8DzPr1+/ngfAX7t2jed5nv/xxx95APzhw4ftfDdss7Wvli9fznMcZzYee8Y4Y8YMXiaT2RwTu9+yZct4Wz/dlq+P56vff9WNe8KECXxMTIz478LCQt7b25uPj4/ny8vLqx334MGD+fj4eLO/b9myxWo/2dKnTx8+JCSEz8vLE287efIkL5PJ+BkzZoi3PfDAA3xISAhfVVUl3paRkcHLZDL+9ddfF28bM2YM37NnT76iosJsrEOGDOE7duwo3sber2HDhpltszrXrl3jAfD/+c9/xNu+//57m6+xpKSE9/Pz4+fMmWN2e2ZmJu/r62t2+8yZM3kA/MKFC62e097P1rx582x+Jnie5wHwy5YtE/9955138kqlkr969ap4W3p6Ou/t7c2PGDFCvI29P2PHjjXb188++ywvl8v5wsJCnued9z0ipKFRSQMhzZC7u7v43xUVFcjNzcWgQYMAQCxXAITa34MHDyI9Pb3BxhIdHY0JEyaY3fbDDz+gd+/eVtkyAFaX22fPnm2WmRw+fDgAIDk52ex+Xl5eeOihh8R/K5VKxMXFWd3vt99+g6+vL4YNGwYAyMnJwZ49e/Dwww+jXbt2NY5FitVN//rrr9DpdNXerzbSfVVWVobc3FwMGTIEPM/j+PHjdo/RYDDgp59+wqRJkzBgwACr56nptdTE1v6zHHdRURFyc3MxcuRIJCcno6ioCIBwpaGkpAQLFy60ytJKxzNjxgwcPHgQV69eFW/7+uuvERkZaZWRlcrIyMCJEycwa9YsBAQEiLf36tUL48aNw7Zt28Tbpk6diuzsbLOSgc2bN8NgMGDq1KkAgPz8fPz555+47777UFJSgtzcXOTm5iIvLw8TJkzA5cuXkZaWZjaGOXPmQC6XVztGRyQmJqKwsBAPPPCAOIbc3FzI5XLEx8fjr7/+snrMk08+aXWbPZ+tutDr9di5cyfuvPNOxMTEiLe3adMGDz74IPbu3Yvi4mKzxzz22GNm+3r48OHQ6/W4fv06AOd9jwhpaBTwEtIM5efn4+mnn0ZoaCjc3d0RHByM6OhoABCDEQB45513cObMGURGRiIuLg6vvvqqVYBYX+x5pa5evSpeQq6NZYDHulAUFBSY3d62bVuroM7f39/qfr/99hvGjx8vTsRhr9fe8TAjR47E3Xffjddeew1BQUGYPHky1q9fD61WW6ftpKamigGbl5cXgoODxSCP7St7xpiTk4Pi4uI6v47a2Np/APDPP/9g7NixYt1scHAwXn75ZbNxswC2tjFNnToVKpUKX3/9tfj4X3/9FdOmTasxUGdBEyuzkOratStyc3PFMoNbb70Vvr6+4qV0QChn6NOnDzp16gQAuHLlCniex5IlSxAcHGz2v2XLlgEwTRSt7f2pj8uXLwMAbrnlFqtx7Ny502oMbm5uNuuH7fls1UVOTg40Gk2177fBYMCNGzfMbq/t++us7xEhDY1qeAlphu677z7s27cPL7zwAvr06QMvLy8YDAbceuutMBgMZvcbPnw4fvzxR+zcuRP/+c9/8Pbbb2PLli1Wda+OkmaZHFFd9ozn+TrfT6PRYNeuXdVOrqoLjuOwefNmHDhwAL/88gt+//13PPzww3jvvfdw4MABu2qd9Xo9xo0bh/z8fLz00kvo0qULPD09kZaWhlmzZpntK2epLoDU6/U2b7e1/65evYoxY8agS5cuWLFiBSIjI6FUKrFt2za8//77dR63v78/br/9dnz99ddYunQpNm/eDK1Wa5axry+VSoU777wTP/74Iz799FNkZWXhn3/+wVtvvSXeh437+eeft5nVBoAOHTqY/bu+n29b2Dg2bNiAsLAwq79bdk1QqVSQyczzT03x2bKltu+lM75HhDQGCngJaWYKCgqQlJSE1157DUuXLhVvZ1kjS23atMHcuXMxd+5cZGdno1+/fnjzzTfFgNfRS+E1iY2NNZsV31j+/PNPaLVas2CeXZp1dDyDBg3CoEGD8Oabb+Kbb77BtGnT8N133+HRRx+t9bGnT5/GpUuX8MUXX2DGjBni7YmJiWb3s2eMwcHB8PHxqfV1sAxbYWGhWTs7li21xy+//AKtVoutW7eaZfAsL7XHxsaK47YMFC3NmDEDkydPxuHDh/H111+jb9++6N69e42PYYusXLx40epvFy5cQFBQEDw9PcXbpk6dii+++AJJSUk4f/48eJ4XyxkA0/usUCgwduzYGp/bGar7brH3LSQkxOFx2PvZqmkcloKDg+Hh4VHt+y2TyRAZGenQeOvzPSKkMVBJAyHNDMuoWGZALReP0Ov1Vpc1Q0JCEB4ebnY50dPT06HLnzW5++67cfLkSfz4449Wf7MctzNt27YNAwYMELtZAMJBfMSIEVi3bh1SU1PtHktBQYHV3/v06QMAdl+OtbWveJ7HBx98YHY/e8Yok8lw55134pdffsGRI0esnovdjwVTe/bsEf9WVlaGL774wq4xVzfuoqIirF+/3ux+48ePh7e3N5YvX27VWszyvZs4cSKCgoLw9ttvY/fu3XZld9u0aYM+ffrgiy++QGFhoXj7mTNnsHPnTquV9MaOHYuAgABs3LgRGzduRFxcnFlJQkhICEaNGoU1a9YgIyPD6vlstU6rDxaMS8cOABMmTICPjw/eeustm3Wt9ozD3s9WTeOwtc3x48fj559/Nmtfl5WVhW+++QbDhg2Dj49PrWOTcsb3iJDGQBleQpoZHx8fjBgxAu+88w50Oh0iIiKwc+dOXLt2zex+JSUlaNu2Le655x707t0bXl5e+OOPP3D48GG899574v369++PjRs3YsGCBRg4cCC8vLwwadKkeo3xhRdewObNm3Hvvffi4YcfRv/+/ZGfn4+tW7di9erV6N27d722X51t27Zh9uzZVrd/+OGHGDZsGPr164fHHnsM0dHRSElJwW+//YYTJ07Y3NYXX3yBTz/9FFOmTEFsbCxKSkrw2WefwcfHx+4li7t06YLY2Fg8//zzSEtLg4+PD3744QerumN7x/jWW29h586dGDlyJB577DF07doVGRkZ+P7777F37174+flh/PjxaNeuHR555BG88MILkMvlWLduHYKDg62C6eqMHz8eSqUSkyZNwuOPP47S0lJ89tlnCAkJMQsUfXx88P777+PRRx/FwIED8eCDD8Lf3x8nT56ERqMxC7IVCgXuv/9+fPzxx5DL5XjggQfsGst//vMfTJw4EYMHD8YjjzwitiXz9fW16h+rUChw11134bvvvkNZWZnZMr/MJ598gmHDhqFnz56YM2cOYmJikJWVhf379+PmzZs4efKkXeOyR58+fSCXy/H222+jqKgIKpVK7G28atUqTJ8+Hf369cP9998v7p/ffvsNQ4cOxccff1zjtuvy2erfvz8A4KmnnsKECRMgl8tx//3329zuG2+8gcTERAwbNgxz586Fm5sb1qxZA61WW23f65o443tESKNo3KYQhBCp6tqS3bx5k58yZQrv5+fH+/r68vfeey+fnp5u1mJIq9XyL7zwAt+7d2/e29ub9/T05Hv37s1/+umnZtsqLS3lH3zwQd7Pz48HUKcWZe3bt+dvu+02m3/Ly8vj58+fz0dERPBKpZJv27YtP3PmTD43N7fG18baO61fv168beTIkXz37t2tnmPmzJnieM+cOcMD4A8dOmRzPGfOnBHfM7VazXfu3JlfsmSJ+HfLtl3Hjh3jH3jgAb5du3a8SqXiQ0JC+Ntvv50/cuSIvW8Pz/M8f+7cOX7s2LG8l5cXHxQUxM+ZM4c/efKk1Wu0Z4w8z/PXr1/nZ8yYwQcHB/MqlYqPiYnh582bZ9ba7ejRo3x8fDyvVCr5du3a8StWrKi2LVl1+2/r1q18r169eLVazUdFRfFvv/02v27dOqttsPsOGTKEd3d35318fPi4uDj+22+/tdrmoUOHeAD8+PHj6/Qe/vHHH/zQoUPF7U+aNIk/d+6czfsmJibyAHiO4/gbN27YvM/Vq1f5GTNm8GFhYbxCoeAjIiL422+/nd+8ebN4H/Z+2dtOy1ZbMp7n+c8++4yPiYnh5XK5VYuyv/76i58wYQLv6+vLq9VqPjY2lp81a5bZZ2zmzJm8p6enzee097NVVVXF/9///R8fHBzMcxxn1qJM+pvBHDt2jJ8wYQLv5eXFe3h48KNHj+b37dtndp/q3h/2vWav01nfI0IaGsfzDXj9kRBCnOSdd97BihUrkJGR0SB1yaT+Tp48iT59+uDLL7/E9OnTm3o4hBAiohpeQkiLEBUVhffff5+C3Wbss88+g5eXF+66666mHgohhJihGl5CXFBOTk61bawAYdEH6UIAzcF9993XqM+Xn5+PysrKav8ul8sRHBzciCNqvn755RecO3cOa9euxfz58806KxBCSHNAJQ2EuKCoqKga21iNHDnSbEUrVzRq1Cjs3r272r+3b9/ebKa7K4uKikJWVhYmTJiADRs2wNvbu6mHRAghZijgJcQF/fPPPygvL6/27/7+/uLMb1d19OhRmzPiGXd3dwwdOrQRR0QIIcRRFPASQgghhJBWjSatEUIIIYSQVo0mrdlgMBiQnp4Ob29vmhFOCCGEENIM8TyPkpIShIeHQyarOYdLAa8N6enpDq8nTgghhBBCGs+NGzfQtm3bGu9DAa8NbIbxjRs36ryueF3odDrs3LkT48ePh0KhaLDnIQ2L9mPLR/uw5aN92PLRPmz5GnsfFhcXIzIy0q7OMBTw2sDKGHx8fBo84PXw8ICPjw99uVsw2o8tH+3Dlo/2YctH+7Dla6p9aE/5KU1aI4QQQgghrRoFvIQQQgghpFWjgJcQQgghhLRqFPASQgghhJBWjQJeQgghhBDSqlHASwghhBBCWjUKeAkhhBBCSKtGAS8hhBBCCGnVKOAlhBBCCCGtGgW8hBBCCCGkVaOAlxBCCCGEtGoU8BJCCCGEkFaNAl5CCCGEENKqUcBLCCGEENKCff7PNTy36SSq9IamHkqzRQEvIYQQQkgLVaipxJvbzuOHYzdx4kZhUw+n2aKAl7RIV7JL8erWs8gurmjqoRBCCCFNZvuZTOj0PAAgq1jbxKNpvijgJS3Sun+u4fN9Kfju8I2mHgohhBDSZLaeSBf/O4uSQNWigJe0SIWaSgDAzQJNE4+EEEJIXaUXluOrA9dRXqlv6qE4RX5ZJb46cB3FFbpGfd7MogocuJYn/ju7hDK81XFr6gEQ4ohSrfAjmV5IZ7OEENKU9lzKQftAD7QP9LTr/peySjDtvweRU6KFTm/A7KHRDTzChrd691Ws3ZOM/LJKPDWmY6M976+n0sHzpn83RplfTokWx1ILMLZrKOQyrsGfz1kow0tapDJtFQAhS0AIIaRxFJXrcPR6PnhjlHUpqwQz1h3CvG+O2fX4M2lFmLpmP3KMmcjWMsnqWm4ZAOBcenGjPu/Wk0I5Q59IPwCNk+F99ZezeHzDUfx1IbvBn8uZKOAlLVJphRDwphWWiz+8hDAVOj1SjAcgQojzvLzlNO5etR/7k4XL6JeySgAA1/PsKy97/vuTKNDoEOSlBACcbeQAsaFkFgmZ1Ss5pY32nOmF5Th1swhyGYfZQ6MANE4Nb6pxX1+VvNYb+RqUNHI5R11RwEuatfyySpsBbakxw6utMiC/rLKxh0WauQWbTmDUu7saJdvC8zzySqlujriGC5nCd+psmvD/NwuEq2wlFVXQVtVcj8vzPJKNJ6KrHuoPAEjOKW0VdbwZxoA3JbcMukbqhctONmKDPdGtjQ+AxsnwFhjn0GQag+vMogrc8t4uzFx3qMGfuz4o4CXN1oHkPPT7VyLe+f2i1d9YwAuYfmhI49NW6Z2aYed5HpVV9T9YXM0WDqpn0ovqva3arN6djP5v/IE/zmU1+HMR4gy1BaY1YW2vbhgnDEsnDheU1Zzh01Tqxe9393AfBHmpYOBNQXRLpa3SI9d40ltl4HE9z/6rS3oD73CAzMooYoK8EOKtBiCUnFToGvYEosCYZGLZ5POZxdDpeZxJL27WV1wp4CXNFqvtOnWz0Ox2nufFGl5AKGsgjS+9sBz9//UHFm057bRtzvnyKAYtTxJ/UB1VVil8PjIb4WTodFohAOBUWsMH16R5aM4H9dp8eygV3Zf+7tAJWqm2Skw2pOYLgW5agen3N7eWKx3sapzKTQZ3hRzdw4WsZEsva8i26H17Jdv+soa7V+3D2BW7HTrRT84RAt7oYE/4uLtB5SaEdDkNmOWt0OlRZszIs99XNpemssqAwvLmW9ZAAS9ptnKNX9pCjfkXSFtlQJXBdMChiWtN49TNIpRqq7D7Uo5Ttmcw8NhzKQf5ZZU4kJxX+wNqoDH+IDdG9r/EWE9epKHSGlegrdLj1pV/4/ENR5p6KA7541wWqgy8WINbF9L60Bv5LMNr+v2trbyMXQr391CC47hWE/Ba/s7YG/BqKqtw4kYhrudpkJpf9zkHLMMbHeQJjuMQ4qMC0LB1vNLjMcv2S4/BzXnhC2pLRpqtnFLbAa+0nAGggLepsF7IWcUVqNIb4Cav3/lzTqkWlcZLeyduFmJizzYOb4t9RjKKGv6zwQLe5pzZaC6W/XwG5zNKsOHROKjc5E09HIdcyy3DxawSXMwqQXGFDj5qRVMPqU7YpCpHaj2zJIHdjYJyGAy82RW2vLKat1lg/C339xQmrHUzBrzn6lF6tP9qHp7deEK8qsP0beeP9bMGNkrbLMvfmct2Brx5paYThLTCCnQI8a7T87KANzZYaAcX6q3GjfzyBq3jlZ7UZBVXwGDgzdqDNueFLyjDS5otdnmsyCKQKLMKeJvvF6w1YwcvAw9kOeEHVpopOnXD8QOgTm8QLw+yS26l2io89N+D+HJ/Sr3GaAsLri1PzIg5nufx7eEbOJSSjzNpLTejl1tiOuBfyixpwpHUzmDg8dLmU1iReAmAcDmaZWYdCUwyJY+prDLgYlaJeDUFMA/gbGGlSgGewklC93BfAMCFzBJUOVjHuvVkOjKLK1BSUWX2vz2XchqtRRjL8PqohRyivRneAslVoYw6Jm4qdHrxZCM6yAsAHM7w8jyPxT+dxqtbz9ZariMdc5WBR15ZpVnSqTkvfEEBL2m22IGlVFtlVtTPMmpMeiNk8Zxt+bbzWPnHpaYeRr1If/ickWWXTn45nVYEvcGxOknpAZgdiP6+lIO9V3Lxv73X6jdIG1grHsrw1qy4vEo8EanLpJ7mJqfUFExcaGYB78HkPMz58ogYCJ3LKMbGIzfwYdJlFFfocC23DOxr5UidZ6ZFILX/qnlZRF4tJQ0sO+jvIWR42wd4wEvlBm2VQezeUFfss/RyQhf8+dxI/PncSAztEAgAOJyS79A264qdWA/tEARAaNdlsOP3S/p+sd/Q9MJyPPnV0Vr7E7Psrq+7Av4ewgkEm7hW16Bz39U8fHUgFZ/vS6m1DMyybCWruMLsGJzZjEsaKOAlzVaOZAKENMtrneFtWQFvZlEF1uxJxso/LluVZ7QkBTZ+rOtDemm0VFuFZAf7WWoklzaLynXQVFaJB9Os4gqnTzhiPaGL6xnw3sjX4KXNp+o04aUlkQaK9vZsdYYb+Rq8uPmk2MKpvswyvE7aprN89vc1JJ7LwqbDNwAAFyUB+embRWaX2h3J8GZZBEP7LALe/NoyvBqW4RUCXpmMQ9c2wmX8sw6WNbDPUv/2/ogJ9kJMsBeGxAqB55HrjRPwspKGuOgAKOUyVOgMdk2mzrcoaQCESYXbz2TilR9P1/hbJXZoCBbqdwHHM7zSK18XazmJK7CYq5BeWG42Obg51/BSwEuapSq9weyLJQ14WZAY4ecOQDibdUYrq8ZyQ5LJrOtlLGfbcOA6tp/OcOixBZJL+M7olCEtaQCAkzcdOwCWac1b8mQWVYgHhwqdAcUVzjvJ0Bt4ccZyYT0nrX1zKBUbj9zAp7uuOGNozY50JntjZngX/3QGm47cxHObTjrlZEd6It7cMrzsfWWTwC5KAvITNwrNTqY0lfo6n3CzYIbVxR66Zpnhta9LA8vwAqayhrMOlLloq/RidrFdgGlZ4wHt/QEAR1IKnHqCq63S44M/Llu1UWNZ0bb+HogOEsZhz4mrratkrPPC2fRiHEstrPax0glrTKgxw1uX7H1aYTkSJR07avtMW7aeO5MutCRjqIaXkDoSFpww/VtaH8l+pCMD3KF0k4Hnm/eXzJK0jU9TtlRLKyzHkp/O4PnvTzr0+EKz+rO6v/9l2ir89+9kq9neLPtz0sElRy2vAGQUVZhli+uy1nx6YTk27E+ptk+mNGAoKtfZdRmzOuzk50hKgUOPzyquwBf7UurVY7UhSQPFlEbK8B5MzhO7iJxOK8LvZzPrvc1cSTBxMbPE7oAqu7gCHyZdxvLt5/H2jgsOZzSrYzDwuG78Lp3PKBbHx5y6WYirFkFYXX83WUlDD+NkM3byGGOcNFVbSYNlhheAuGDCRQey5Tfyy8HzgKdSLq7cBgC9I/2gkHPILtHiRr7zfmM3Hr6B9/+4hOc2mf9msoC3ja8aHUKEelp7Al6zkgZj4C4t7dhQw5wDtspZjCTgdSTD+83B65D+bNV21cIyw3s81fz3ijK8hBhlFlXg5xNptU5QsKxBKio3fclYBs9brRCzvI0VOCadz8KZevZbldaqNuWEu5vGg2NZpd6hIKm+NbxbT6bjjd/O49/bLwAA0ozvy4TuoQCs+y/by3K2doYkwwvU7Qd52dazWPLzWWw+etPm36VLaRp4oKQeJSrsM5+ar6lTUM7837fHsWzrWfxy0rGMfUNriAxvSYUOP59Is9lon+d5vLtTWLQm0BhgvbvzksO14YxlqZU99ZLX88ow5dN9WJF4CWt2J2PVrqtYsNGxE83qZJVUiFe60grLUVBWaRbwnrxRZBWEWfaPrfU5jJ/LAVEBZrf3busHoPa2ZGKGVxLwRhuD5RQHPhOslVf7QNNlfQBQK+ToGSFkjp1Z1vD35VwAQvaVvZeVVQZxgnWYrxqxdQh4pSUNGYUV0Bt4XMs1PW7b6cxqs7WmkgYv8bZQn7rV8Gqr9PjukFD+cne/tgBqz/CyfehnrBtmtcbeKmHCXlZJ800+NYuA95NPPkFUVBTUajXi4+Nx6FD1y9PpdDq8/vrriI2NhVqtRu/evbFjx45q7//vf/8bHMfhmWeeaYCRk7p6/dezePq7E/jjfHaN97NsYG6e4RX+20vlhja+whe8MdpPXcgsxiNfHMFdn+6rV6ZIGpw3xrirI52AUurAZf7CepY0sCD5WKpw2ZFt47ae4QCECTe2AvHMogoculb9QUxjUdJwPqPYrPzCcuJNdQwGHgeNvUotsxiM5SXhonp0apBmZY5cr1uW93xGsfieNNeadmmgWKDRWXVfYdIKy+2ebPTfv6/h6e9OYMlPZ6z+tutSDg6nFEDlJsPGxwfB112BK9ml+PF4mmMvwMgyAKktQLicVYJ7V+9HWmE5ogI98OiwaHCckNF05pWplFzzrPn+5Dzxsy7jhM/95WxhrG39WTmY/c+vN/BiIDXQIuDt1VYILmvr0sB+M9gkK0CYuAYIV75qKk07er3AKqBmrzkqyMPq/iwoP+zgFRNLVXqDWX/wrSfTAQjvIc8DSrkMgZ5KdDQGvBfsyFjnS5IGlXoDzqYXoUJngJuMQ6+2vqjUG7DxcKrNx9oqaQjxFjK8hRr7VlvbdzUPeWWVCPVRYf4tHQAAV7NLa1z5jSU6uoQJtddsEnmfdn7CayrToblWGDZ5wLtx40YsWLAAy5Ytw7Fjx9C7d29MmDAB2dm2A6LFixdjzZo1+Oijj3Du3Dk88cQTmDJlCo4fP25138OHD2PNmjXo1atXQ78MYid21ivNctqSa/HDaR7wCl9kL5Ubwo0Z3sbIlO6+KFwardQbMPfrY/j5hGMHzpvNpKRBOhu3rrV8BgNv1pXAkSCLXc7LKKrAuYxiVOgM4DhgYLQ/AjyV0Ol5nM+wPmjM++YY7luzv9rJFZYZXsuJNfYGGZezS8VLttU1xrc8USgsd7yOV5qVqevs8i/3Xxf/u7YsW1OxDBRTqylrePSLI7h39X6s2Hmx1nIBFsD9cOwmruaYMoSXs0rw0uZTAIBZQ6LQIcQbT46KBQCbnToMBh6nbxbZdaWD/TaxQONiDcvink0vwtS1B5BdokXnUG9semIwFt/eDT2Mdav7rubW+nz2ssyabzkm/D5F+LmjU6gQnBh4wF0hR992Qo1rXTK8eWVa6A08ZBzQr72f2d96GTO8pdqqGgMtWzW8wd4quCvkMPDV/x7uvpSDu1ftw5KfzU9s2GuW1u8yrI73qJMyvGfSi806BP1yMh08z4u/o2G+anAchz6RfgCE3sLllTV/niy/q3uvCJ+HdoEemDUkCoBpP1o+jh0TowJNr93XXQFlDautFVfozDLP141Bc792/mgf4AFPpRyVegNSauiYwcbcJczH7PZu4T5QGnuxFzfThjVNHvCuWLECc+bMwezZs9GtWzesXr0aHh4eWLdunc37b9iwAS+//DISEhIQExODJ598EgkJCXjvvffM7ldaWopp06bhs88+g7+/f2O8FFILnufF+lXLOiBLll9WaXDFggxPScDbGIHjP8bAKcLPHXoDjwWbTtYauNsireFlgSLP87iRr6nxAM/zPG4W2L6P3qIBvD0yLLoi1EVJRZXZpeHiiqo6b0Pa5WH7aSFjHuqthspNjt7GjNERG4HfZWPmhAU7liwnrbF6RqamcoHsEq3Y5UEadF7OKhWzT4WaSvGgbtkiz9FevOWVerNtHa1DhreoXIefJFnL+k6es+ez6AjLbKKtS9jFFTpxf3345xW8+dv5GsfBZrYbeOCDJGGy39n0YrMgc+4oIXM1uY9w5eBSVolVYPvLqXRM+ngvVuysuVWg3sAj3zgxi7W+uphp+9L1sdQCPLD2APLLKtEzwhffPTZIbBs1xPjYf67Ub0VBKVYXzSaU7booJI06h3mLJQeAUG8bVk2tZ4VOj7xqlgfOKhJuD/ZWIdhLBU+laeGQrm28oZALz1vdCRfP8zZreDmOQ/tAIUPLapAt/WrMpibnmH9m2GuOCrTO8PY3BryXskod/k5oKqvEY9E/xmB0aIdAqBUyXMstw+m0IrOAFxCy5yHeKuj0PE7WUpbF3iu2z/YaSyZigjwxzNjiLCWvzCrzzcoeIvzc4S7ZDxzHiVne7JIK3MjXiOWDegOPB9YewPj3d4tBLztmRPi5Qybj0MmYta2pnpr9brPaa6atn7tYQ1zUPM+5m3altcrKShw9ehSLFi0Sb5PJZBg7diz2799v8zFarRZqtdrsNnd3d+zdu9fstnnz5uG2227D2LFj8cYbb9Q4Dq1WC63W9CUvLhZ+cHU6HXS6hjtVYdtuyOdoTorLdeKM9twSbY2vO7vYPHgrKK0Q719SIXyb3N04BHsLl8Zu5pc16PtYWWXAYeOM5E8f7I2lW8/j5M0i/Hk+E/f0CQNg3340GHjclASaaQXl0Ol0+OpgKl779QJendQV0+IibT7255MZeH7zaYzpEowP7usFlUL4oeN5HvO/O4md57Lxxaz+GBIbaNdrkmZlC8sqoNNZHzSqk1MsHGg8lXLIZRyKK6qQmlsiXs6zh7Rs5bdTwgEt3E8NnU6HIbEB+OtiDn47lY6Zg0zvR4VOL2Zdq9vnJeXCdv3cFWYnSgo5B52eR0ZRudXjdDodjuRweO7dPegW7o3Nj8XjkOTyZaXegAvphQjyUuK2j/chNtgT3z4ah8Iy84Ahr8R62/Zg9csyTgjezqYXo7C0HJ6q2n+iNx2+jnJJVi2/rObvVm2+2H8db2y7iH/d0Q33D2xb6/3LtFVwV8ghq2VFK3aiEeqtQlaJFtdySqzGefqGcJKhcpNBW2XAf/deQ4dgD9zdL8LmNtMlJ5zbz2YhM1CG0wcPosrAo0e4D9bN7AcPhbB/A93l8FG7obiiChfTi8R2WABwxhiYnE0vqvG9yy3VwsADHAfEtffDVwdScSHT+jFF5TrMWncIxRVV6N/OD59N7wsvJSfeLz7KD2t2C0FUZWWlWf2po1KMQdDgmADsvZInLr/eMdgTEf6mY2ZskCeCjAs/ZFp8F57fdAq/n8vCr/OGiBPRtFXCJfa0fGH7Id4qVFVVoa2/Oy5mlSLAUwEFxyPAQ4msEi2yizQI9rT+3JZUVImz+b0UnNnztgtwx4XMElzLKUUozH9LDQYef17INm7D/JjMMrxt/VRW+8BHJUNMkAeSczV4+PPDmDcqBsM7BNbpvX7ws4M4n1GCrx8ZiH8uC1f4bukcDD+1Ar+dycRPx24i2BhghnorxTH0b+eH7WezcCg5F/0jfardPjt56hDsiYtZpWIpU/sAd/ipZXBXyFCuM+B6brFZJvei8aSwfYC71esO8VbhZkE55n9zHBlFFRjZKQhrp/XF1lMZ4pWqYyl5aO+vQqrx/WvjK7x/HYM9cTy1EOfSijChazCKynXwdTdfSZCVYXQM9rB4XiVCjc9dWMk1WlxTl+dp0oA3NzcXer0eoaGhZreHhobiwoULNh8zYcIErFixAiNGjEBsbCySkpKwZcsW6PWmH/zvvvsOx44dw+HDh+0ax/Lly/Haa69Z3b5z5054eNgfBDgqMTGxwZ+jOUgrA9hH7nxyKrZtS6n2vqcuywDI4KPgUazjcO7qdWzbJlyKvJwi/C316kVoPQBAjos3c7Ft27YGG/uVYqBc5wYvNx7Jx/YiHBxOQo4t/5yFb85pANb7kSWmpL+vxZVAZZXpa5deqMGvv23DDxeF17T9wFn45562OYafkoX7JF3Iwd0fJOKRzgao5MC+LA47k4Xg9387DqMw2r4CqgupcgDGbNA/B5F7zv6MXkoJALhBxVVBJQOKwWFr4t/o6m//Nm5kmZ7/mjFTw2nysW3bNqgqAQ5yHL9RhA1btiHQeLzOrRCeFwD2n7yANkXnrLZ74gYHQA5/t0oUwvTmt/Mw4GoJh0upWVaflX1ZHDYly8CDx6mbxfjou+3Ye1UYn1LGo9LA4bvf96KKBwo0chy7XoBff9uG/dnCc4nbOXICspvW5VW1uVosvK4AFQ+9ASioBNZuSURn39rfz/+eEMbZzc+Ac4UypKTX77uw3fjd2/T3GfjknKrxvgVa4K0TcnT35zGrU82fu7R8YZzhynJkQYZ/Tl5C+zLz3/ldGcL72dG7CmHuwB/pMny96zTcM60neFUZgJxS4bPQxdeAC0UyHM+TAeDR1c+AhyLysX/XH2aPCVbIUVzBYdPOvRgYbHpvjxlf85W0mt879hvmKeeRdfEYADdcyijGr79tgzTe/yudQ3GFHKHuPKaG5eLvP81/Gyr1gJyTI6OoAl9s2Y4Q9xrfOrucvia8vxGGbEg/k5qMKygp4sG+N/qCm0grBwA5LlzPwLZtwoRMPQ/sPCuHzsDhv7/swbAwHnkVwPKTcvQK4BHjzQvb1RRi27ZtUFYK75knKrFt2za46YXn3/HXP7hu43eAfXeVMh5//fG72d+qCoVt7T1xEXdHm/+WppQAeWXC2PNLNOL+0fNAqvEzdfXEAeSdt35PhvpxuJ4nw7HUQjzy5THc0U6PMRF2dtUoB07cEJ73yS8OoFALAByqbp5BG53wOd14KAXR3jwAGTS56eJ76V4m/H37EevPOKM3AEXlwvZ9DcUAZGImtzQjGdu3X4W/Qo5yHYfNO/agm+Q9/emK8H65V1h/XvWlwt9Y5nn3pVy8tG4H/s6Ugf3eJh08BVXGCZy7Lrx/GVfOYlv+GVTlCeP++9QVZF6/jC0pciRE6jGhrfDclXqgQieM+eLRvVDIhM8LAFw9dRiGMuG5iyobL67RaOy/ytqkAa8jPvjgA8yZMwddunQBx3GIjY3F7NmzxRKIGzdu4Omnn0ZiYqJVJrg6ixYtwoIFC8R/FxcXIzIyEuPHj4ePT/VnZ/Wl0+mQmJiIcePGQaFoWeuxO+LPiznAKSEYUPkEIiFhYLX3/W79ESA3H90jA7E/OR9e/iFISOgHAPgh9yiQl4eBfXthYJQ/Pj23F4U6OW69dXytWSZHCZdLkzGySxvcflsvtLlegO3/PYzr5SqMGTsUSX/8YbYfDQYh63ryZhF+njsIQV5CFuD4jULg6CGEeKuQW6qFnucQP2IM3jl/EEAFFL7BSEjob3MMWzYcA7KES14Xi2RYneyF+wdGYuuRywCEH8pczhcJCYPtek1vnN4FwFiP1aMPEnq3sfv92HUpBzhzHG0CfRDspULGpVxEdOqJBDsygsxrp/4CYH52HtctFgnjOgIAdhQcwb7kfJQFdsH0kTEAjJf6jwsnsu4BbZCQ0Ntqu6d/vwTcTMGATm1x7ajpUv/E/rH4eFcyKuXuSEgYId6+72oeNu4/CgAI9lIip7QS+0qDkK8thIwDJvdti++PpsEtOBrp+RoAuTCAw9DRY5FxLA1Ivixuq21MZySMirH7PWB+O50JnD2FqFB/hPmo8evpTLiFdULC6NgaH3ejQIOs/XvhJuPw3B0D8MiXx8ArPZCQMLzOY2C+WXcYyC1AdpU7EhJG1njfX05loPLYaSRrFEhIuKXa+2mrDNDsF4LPSYO64fhvF6D3CEBCQpzZ/XZtOQOkpGN0nw4Y2SkYf6w5iJRyJcZPGAU3uXkF3vV8DXBwL1RuMqx6ZDie/OY45NpivHznAMTFBNkcx0H9OVw9dBMebWKRML6TePuG/x4Ccguh4RVISJhQ7evYeyUPOHUU4QHemD5lMN47m4QKnQHd4kaKGVGDgcd7H+wFUI7547pjSjXfie+zD+NQSgGUkT2RUM1VHXvxPI+Xj/4JQI9Ztw3HH+uOiDXy900YhthgT3x47k9oqwyYOLQf/DwU+PLyEegVnkhIGAZAaGFWeUC4suoeGo2EhC7YeOQmdMfP4Wguh5DQUABZ6NWxPRISuuI4LuD0/lR0jwpDQkJvbMo+irSreYjt3hsJxvIRqZM3i4DjBxHkbf79A4Diwzfx59ZzgFcQgGyz39IVf1wGICQ7Kg0yTJw4HhzHITVfA8MBYf/fP3mizd/+BACPFVfgk13J+O7wTfydq8Zbs0aIV8dqsn7fdeCE0OUjp8K4sIO3CrPvHgednsfuVftxObsMZwqEvw3t2w0Jg9oBANqlFWPL6gO4WaHErbeOtjm2nBItcHA3OA4YN7ArDm27KP5t0qh4xEcH4NfCE0g/n42Q2O7itnmex/J39wDQ4qFxAzGsg/kVPa5dJt7+/RJu7R4KL5UbPvjzKn66bv56lYHhSEjohVdPCr/Bk8cOQ5cwbwQk52PL+iNIrVDiQrEeAI8dN+WYeWsc+rXzE4LoQ3ugkHO4a9JErLr6j1iGMvX2ccj+6yqO709FkZbDuHFjGyWuYVfk7dGkAW9QUBDkcjmysrLMbs/KykJYWJjNxwQHB+Onn35CRUUF8vLyEB4ejoULFyImxnhAPHoU2dnZ6Nevn/gYvV6PPXv24OOPP4ZWq4Vcbr7zVSoVVCqV1XMpFIpG2WGN9TxNLbvUFNwUanQ1vmb2Y90p1Bv7k/NRVFEl3l9TKQR3fp4qtAvyhlzGQVtlQKHWILZlAYQJIQt/OI137umFQTH2XeavzoFrwqWmYZ2CoVAo0D86CJ5KOQo0OlzNE86kpfvxsz3JSDR2oth1OR8PxAk/VlklwnvQPtADchmHjKIKXMzRiGfj2SXaat8Xdp9nxnbEF/tSkJyrwVvbhR/JHhE+OJNWjAuZJdAaOHjVcim8ssqAXEmtXYWer9NnsEQr7IMATxXaGmdZZ5dW2r0Ng4G3We8aGeglbmNy3wjsS87Hb2ey8H9jOwMA8jSmKzmZxcJ7lVFUjvvXHsADce3wxMhYVBizJOF+HvBQysWlhod2DMHHu5KRXaKFXO4mHoSOpApt5nr6G/DWg/0x6ZP9OGps+N61jQ8GxQTh+6NpOHGzyGyiXLGWh6bSPFtUotXb9R4YDDye/PooOHBY9VA/5GmEMo0wX3fhQHc6E8dvFNW6rUMpwth7R/ohKli4RF9YXvN3qzbZxpXEskq0yNPoxdpEW1ILhM9kUXkVynQ8/CSTkaRyyoTyGYWcQ9/2wuz51Pxyq3FeMNbD9mzrj77tA+GtdkNJRRUu5pSLk4HEbZaaFqCJDvHBL/OGYNu2bYiLCar29XcN9wNwE5ezy8zuk2Ws0yyuqEIVLzOri5QqKBeeM8RHDbVKidhgL5xNL8b1ggp0DhfG99fFbKTml8Nb7Ya7B0RCobD9XRzWMRiHUgpwMKUAM4eanySVaavw4GcHMDg2CAsndrH5eKncUi3KKvXgOCAm1AfdI3yx51IO3GQcOrfxg9JNhgfi2uHvyzkY3ilUXCBC+ntzLtNUH3stTwOFQoErOeYlIwAQ7u8BhUKB8d3bYMvxdNzaow0UCgWCWIeActvfgRLj73aAl9Lq77Ehwmf3RkEFEAy8+0cydl3OxYf398WuS6bSoioDDz3kcFfIkWYsFG0X4AGVyvbnDgAiAxV4486e2HMpF+lFFdh5IRd39av9xPyvi0JyYUL3UPxufO1DYgOhVCqhBPDtY4Px0H8Pil06IgI8xdfVM9IfHko5SiqqcK2gwmqCl/B+CN8dP3cF2geal4J1CvOFQqEQWo6dz8aNggpx28k5pcgs1kIpl2FQbDAUFsH7HX0jcUdf4QSK53kcv1mMPcZe1EM7BOKfK3lILaiAjufE7jXtg72hUCjQLcIPgPB9BoSStbJKPV744Qy2PT0cxcbffT8PJZRKJUJ91bier4GnUo4Ab3eE+wvHgsLKxo2f7NWkk9aUSiX69++PpKQk8TaDwYCkpCQMHlxzlkqtViMiIgJVVVX44YcfMHnyZADAmDFjcPr0aZw4cUL834ABAzBt2jScOHHCKtgljUdaM1rbpDU2E5o18ba10pqnyg0KuUxsTXbDYsLDz8fTkZqvwQbJDHZHlGmrxF6DQ41LVirkMsRFCwfu/cnmE6vOZxTjP7+bztbZZAfA1KGhrb+HOOFOuspNdT1ihQlrwmMn9Q7H7hdH48VbOyPQU4kQbxXWTB+Atv7uMPDVt9CSEpbYNf27rm3J2A+ln4fSoYmDReU6sdk565cJmNolAcCt3dtAIedwIbNEDDSlE5/Ycpa7Lubgep4GvxgntpRJuniwYE0h59C3nR84Tjho5ttYxa+Nh9BqJ07ScmlgVAC6GZvsn7pZBK1k8kheqVb8LLIJO4V2Li+ckleG389mYcfZTNwsKBfrW0O81ehhfD/s6ePJJlIO7RAkznwXaiUd6wvE87zZRKbaJt1IJxHVtJgE60AR5KVCTJCXeJt0GWhtlV6ckNgt3AdyGYfBMWxyl3U3A/Z7wj5/9mCtlKQnLgYDL07IAmru4sHqztkiB6wH6jWzxQKE35t7+0fCQ1n9iSeb9Lbvap7VpKrjqYU4ebMI/9ubbNbruTqsljXc1x0qNzm6Gz+z0UGe4qz9V+/ojqTnRsHXQ4EQY2KgTLLa2gnJvmafPcsVxQBTr9chHYJwctl43NlXqK8O9BQC3uoWnyiw0aGBaW/seHGzsByaKmDDwVQk55Th/rUHcD6jGDLOVBpWYmxLyTKL7SW1rdVxk8swbVB7AOYdTapTVK4TJ62+nNAV80d3AMcBUySBcpCXCt89NggD2vvDS+VmdkLmJpehXzvTam+2sJOOAE+l2WfYS+Um1gWz15YqObax73y/9n7VnpgxHMfhP/f0QhtfNbqEeeOlW4WTp+t5ZeLkaR+1G3zUQtAY6KUSr0YGeSnx21PDEeHnjtR8DZZvO2+adGjch2HGz0K4nzs4jhM/G0WVDXOltb6avEvDggUL8Nlnn+GLL77A+fPn8eSTT6KsrAyzZ88GAMyYMcNsUtvBgwexZcsWJCcn4++//8att94Kg8GAF198EQDg7e2NHj16mP3P09MTgYGB6NGjR5O8RiIwD3h11c6+1ukN4uxV1sRbekCQBryAcIYPmP8oAKaVa45cz3doxvm20xmY9NFeTPpoL6oMPNr6u6OdZDbwUOMs2v1XhR9Gnuex/2oe5n1zDJV6g7gCzv6reeIKXGmFwhgj/NzFH7mk86aAt6jcdv/EQo1OzFRG+LnDR63A3FEdcOiVsfj7pdGI8HM3W06zNpa9aC07LFzLLcMTG47it1O2FzEwHbxMi3/UpTUZOyh6q90wIMrURSVCEvD6eigwqnMIAGDrSaE0QXpCkF1SgSq9QQw2WMaYrbTmoZKLJ0PtAjygVsjFg7I0qGEBr4ebsI+mD24v/m1AlD86hHiJQYNUflklio3BCHsPquvScC69GI98flhc8e2cpHPEhcwSMSAM9VGJ70FWcUWNgavweTPOHI8NhK+7QgwKHO0WUaKtEj9nQO2r3UkDPcu2WGmF5Xj488M4kpIvznQP8VbB10MhNq2XfmcvZ5WiysDD1930mWLfMVvtu0wBr32lawDQyZhJTC+qEPd7vqYSlZL3uaY+zexEnAUFrDUZex9u5Gvwl7E7gvRzZEuvtn5o46tGoUaH+9ceMOtMw07IdHpeXOygJqwfLet2MLZrCGQcMKZrqM37e6ncxC4L7GRLuq8ziipQUqETTww8JIFVmOQqmnQCWKDxJKC6Lg/sN13aoUG6TaVcBp2ex4FsTpzcxn6X+rXzFxc2YN1MWEstWx0abJk6MBJKuQwnbhTaXNTmZoEGM9Ydws8n0rDnUg6qDDw6hHihfaAnnp/QGedfvxUjOwWbPcbPQ4nvnxiMI4vHml1dBExdImx1mpG+H4GeKvHzDgifKfa+sv0p7Wiyj3WLiLVdtmMp1EeN3S+Mxq//N0xMIBVqdOIEtgh/8/dvZKdgKOQc/nNPb0QFeeLfd/cEAPx8Il38jPobJz2yhAI7lrHPRlEznYff5AHv1KlT8e6772Lp0qXo06cPTpw4gR07dogT2VJTU5GRYTroVlRUYPHixejWrRumTJmCiIgI7N27F35+fk30Clqf4godntt0UmyR4izSgEhv4MXZ9pbYD4GMMx1QpMu2sh9B9gMYafzCWi4hyTKOWcVas9639tDpDXjtl7M4nVYkLvU4vpt5mc0Q4w/O4esFOJXPYepnh/DAZweQnFOGIC8Vvp4TD3eFHHlllWKbF1OG1108UFtmdTOLrA+47HFBXiqoJZew5DIOKjfh36zRuj0rC2UUVR/wXsgsxr2r92PH2Uy8sPmk1SIggClD7y/J8NalF7L04CdtmRRhka27tbvwnh8wZtGlGV4DLyxmwLKMbEwsYPNUuiHMR9hetDGrGGpsmyPtP8qCQw9jMm5C9zC0D/SAl8oNg2ICoZDL0DnUNKPfRy3cMbesUsyMtzV+Bouq6cP7xb4UJF3Ixmd/JwMw7+t7MbNYDMBDfFQI8lRBKZfBUMuS2RezSpBbWin2VZXLODFTU6ipBM/zWLHzIr47ZLtxvS1ZFp+LUzerX1WQ53mLgNf8hPOrA9fx54VsrEi8JO43MXNlPEmV9vtkS+12D/cRD/gsC3okpcDqRJCd0NYlw+vroRBPglg22XJZbPaebz56E8u3nTc76WAHfPY62Ekt+wzuupQDngcGxQSYLQhgi0Iuw+ez4xDsrcKFzBJMXbtf/B5KW/b9ITkhltLpDfj39gv4Yl+KeLLBMoL92wfg2JJxeF5Sp2xJuipXhU4vBrcq48ndweR8FGh0kHHAvNEdJI+zLv8DTIFsdW3JpL8ZluQyDpEBwn4UJlcBD8S1w/COwm/sHX3C4W38bLPvHLui0L6W95kJ8lIhoafwe2Iry/vjMSHQffq7E+IVujFdQ8S/q6up++U4zubfBtay8IWYNPAUTgDdjduQfm5YwHsjXwO9gYfBwGO/sXvMkA72BbwAoHSTwU0ug4fSTWxbxk4ipVfVAODtu3viwKIxGN1FeO1DY4Pg56FAqbYKu4y96Nm+ZlcS2FU6FgAXVcLpbQ2dockDXgCYP38+rl+/Dq1Wi4MHDyI+Pl78265du/D555+L/x45ciTOnTuHiooK5Obm4ssvv0R4uHWBvNSuXbuwcuXKBhp96/PHuSz8cOwmPvnrilO3axkQFVTzw8gOKoFeKvHHUbpsa5llhjfQOsPL87zZgay6ILCgrBLLt5236qe782wWsoq1CPJS4suH4/DdY4Osaum6hHkjwFMJTaUe/7sodBRQuskwY3B7bJ0/FG183THQWPbALsmmSUoaLIM7xlaGiWWGLX+cpFim9HhqYa1LN2darPDGDiKXs0pw/9oDYpCrqdTj07+uWj1eumISCzgyiyrEk5LaSAPegdEBcJNxiAn2tDpwdDZegmaBkWWj/PTCCiQb2zFpjEsks4UnPJRysfUU6+vLDvK2M7zCv5VuMvw0dygSF4wQM3ms56SHUo6x3YST8fzSSjHbxPZLdZlVFpyxg59ZwJtVasrweqshk3HigcPyxCS3VCuciN0sEnu4DowOEDPQbAWrAo0Ol7NL8eGfV/DKT2dsnrTYwj57bHsnbxZWu09zJCUdgHVf3XPG13jkeoF4MsoCRVbT+Nnf18TPKntP2EEUAGKDvRDirYK2yoBjFr2JWQ/ecN+6tThgnylWe2m52iH7HC/9+QzW7EnG90dMy0qbShqMAa9xoho7KWavmV3Otmcsmx4fjGBvFZJzyvCXsf2WtORr18Uc6A08jl4vwGu/nBWvdn13+AZW776KZVvPYt0/KQDMs51+HkqriX5SbF9kFVfgbHoxqgw8grxU4th/NbYKjAr0xEPx7eHrroCP2s3sKowUW765upKG/DL2m2G73pa13crXCic7k3q1weez47DtqeF4KL69OC+BfedYAqWm30RLDxnLGrafzrDqxSxdGY0dS8Z0sZ0ht0ffdn5CO7fCcpsLOeSJv4EqcByHNsYEiDTgbePrLma+0wvLcS6jGIUaHbxUbuJvWl2x95n9fli+f25yGQK9TCc1MklpETv5Yvvwjt7h+P2ZEXh6rDDRmP2+6gxctQmtptQsAl7SvLDlIVmNkTPoDbx4MGUZhPxq6nhzJAcVtUIOtcK4eku5DtoqvamXozHTxr6wNyRBa6FGZ9abtLqz7I/+vII1e5Lx8Z/mwf2X+1MAAPcPbIcRnYIxKCbQ6rK2TMZhtPGSu0rGY86wKOx9aTRen9xDDAKHSer0pHW4Ef7uaGNxoGYHT1tZPWlmuDqdQrzhrXaDplJvc4UyKRZIsYMIC1y+2J+CQo0Ovdv64uMH+wIQMnWW9blitsZT6L0o44Retbk2PjMbD6fimEVdselynhIRfu74ad5QbHgk3uqx7Mc/r6wSRRqd+N6wSc83CzRmK3YVanTi0sJeKjfMGByFbx6Nx5wRwqQglp2SnlQUGwNedzdTYOfvqTTbP+zEZUzXUDHAyiszBXxsv9S0XC4g1EYWlFXiXLopc2qe4WWXCIX/tywT+erAdaz/JwVT1+7Ht8bM7VBJ32U2aaxAUymexOkNPLadtl2aYoldXejfzh9qhQwlFVVmgeyFzGJ8uT8FBgNvtQiAZYaXBbCVVQZxKe5g48IL82/pAC+VG45eL8Dq3VfN7t893HQg5zhOLGv4x6KsIcOBGl4AYraeZTQtTzCzirVIKywXrxR8kHRJzC6zk3E2QSvK+PnMLdWiuEIn7lfpa6hNdJAn4o2fL7Z9aTIgv6wSiecyMefLI1j/Twpe2HwK5ZV6fJRk6g7CPof21LMyLDjJKdGK5Qx9In3RMVS4GsKWf+8U6g1fDwV+/b9h+Hn+sGrrksWShmqOGwVigGd7kpG0XMxb7YaB0QGQyzh0C/eBTMbBW81+q4TvWKFFPak9+rXzR4i3CmWVehy0mHvBPg8sqywE/352b9uSp8pNnOeRdMF65dh8i/eD9TCXnvBJM9+p+RpxJbb46IAaT2ZqwrLG0kUnasOyyew7wTK8HMehc5g3FMaxqBVy+LoL+8mZy2Y7CwW8xAoLRNkZuTNkl1RAb+DhJuPEOqLqMry54gQX4Uvl5y78f6FGZza5ylNpXsMrnbRmGaAdrSbgZZnXC5JJLBczS3DwWj5kHPBgfLsaX9fSSd3w0f298Wp/PV6c0ElcSYlhZQ8Hk/OQU6IVg/BwP7VZ7WGQl0q8LFRTSUN12RVACMBZHW9tS9Oy7DfbF+yAyQ649wyIxG0922BQTAAq9QazgytgPmlNmhGwzMCevFGIl344jce+PGq2Mpvl5c0eEb42f3g9VW5ikJqca8qEsqVSj6QUiA322XZLxRpeNyjdZBjSIUjMHLP9Iy0jKbTI8NpyV98IrJneH29M7iE5sFeKE4oijZ/BwnLr2nSe580C121nMsyWzr6aUyZmrdhKRdVNBDyTJgRUmkq9OLFoqOTSJsvwFmoqxQwoAGw9kV79i5NgB6kIf3cxaJNOXHt161ks/fksfj6ZJpYzsGyntIY3u7jCLKvM7suyipEBHnh9cncAwMo/LuM/v18Qs6Pdws1ntA82BvTSk1bpe1qXGl7AlOFlAQ678sSWRc0qrjCbMJhVrBUnorHXFGx8zT5qhfj6L2eVir8j3cOtZ+XXhG2DbT/f+P1iJ3ZPf3dCDJASz2VhxrqDyC7Roq2/O965u5d4Pxas2iNEkuFlNa292vpZ/Saw9ysywKPGMg1WH59fWk2GV3KSbIt0YYURHYLEIIphCQ6WOWTf2+oyxrbIZBxuMV6q/1MShGqr9OJn9D/39MaGR+Lw9aPxDgeVDKuhTrJRlpIvyfACwOuTe2DVtH4Ya1F3zU5iUvLKsOOMcOI4tA7lDJbaW9Q8t7Wo4bVlqMViRjW958+M6YCpMXqzLHFzQQEvscICUVYH6Azs4BTmqxZ/3Kur9WLBADs4sgkuheWV4gx8d4VcXI6RBRuZxRXiZap0i7PXi1klKLK43JxTohVra69ml4qvdcOBFABCzW5t2SNfdwVu7R5abbDUrY0P/D0UKKvUi3VhoT4qqNzkZgFet3AfseDfVkmDtLtDTVgdr61Z7VIZxbYD3gLjSU6AhxIcx+GFCUI7sO+P3hQnXAGm7AoLsKTLWUqxoCK3VCt2ugBMVxECvGo/WLGD7PmMEjGDyup+LSczFZTpxJn/njZmMLNSATZRh+d5U0lDDROeZTIOE7qHwddDIWY3pF0aWIa3ssqACp15OUmBRmd225f7hOCpQ4gXvNVu4omAu0Iu1qVXNxGQBYUsSxngqTRb4pMFEwUandnS0UeuF+BmgQZFGh1+PZVutVQpwz57YT5q8T0+ecOUjWbZ9MRzWWKAMKqzMJEnt9R0siEt2ZAKlhwEp/SNwG0926DKwOOTv66iXKeHh1Iu1sUy7PVJg9Di8ipx1cY6Z3gly6fyPC+W93Q1BqmZkoCXZRU/3XUFRRqd+JsV5G363LIrM3+cz4K2ygAvlZt4Em6vIHHCl7B99hvMJklpqwxiazHAFPw/M7YT7hsYiU2PD8aHD/RFbLD9AS/L8B5OKcDBa8IJcu9IP3Sw2AbrbFEb9l0uq9Tji30p+PlEmllpVW0ZWWkgNrpLsNXfpTW82iq9mG309ahb6ysWhP5xPkv8zb+aXQa9gYePWjjBHt4xWPyc1McYY3B96Fq+OMGVkV7lAoT9MbFnG6uevex9+fVkBk7cKIRCzmFS75rLOGtieRXAnpKQ6CBPsfYdME1as+Wh+HYYEsqLr6s5oYCXWGHZtyoDL9bN1pe03i7A03TZ1RZxYojx4MiWNizU6MSWNOxsHxB+MDyUcvC8qUaWBQo9I3zFA6jlZXVpsFSirUJ2iRZ6A4+fjwvZsBm1zLK2h0zGiVne748KtYDsYOjrrhBnP3cP9zFNIrHRmoxdnm5by8F9nLG+dM/lnBrXj2cHeXYZjWXOTVkY4T3v3z4AY7qEQG/g8f4fpiwv+7FmZ/rVjT1ZUrv25wVTloMtqWnPjyJr/XTQuLSzyk0mZrKuWlxWL9BUioGQh41exCxbnGUMzMsq9WLAWVOGV0p6wsayTSHeaklrMvP33TJoZSdZPcJ9zCbDhfioxMlatiYCFpRVIt2Y/f/usUF4JaErPnqgr9kB0l9S0mD5vP/9+xqmrPoH8785LpYRWGKZ71BfNXpHChlellU2GHix3GjPpVzxZKZ3W1/xO82yvGwCGrukK32NDMdxWH53TzwxMhbT4tthWnw7fPRAX6usGgso88sqxQ4ALPMd6KmsdjJRdWKDvSCXcSgq1yGtsFws7+lrbCuVWVSBy9nCa5s5OAoxQZ4o0Ojw8k+nxWWFpUEb+31hHU26tvGu8wI4LBvGygHY9+vOvhHiif1Lt3bBG3f2EN/TDiFemGJsCTYgKgB31DEIYjWjJ24UIqOoAhwH9IrwFU+CmU52Bn7eKjdx4tWyrWfx9Hcn8O7OS+LfxRrear7zLFiXgceIjtZ906U1vCx5IUzUrNtyAsM6BEHlJsPNgnJcyhJObC5mCSdoXcJ8nLLMMxMV5InYYE9UGXjsuZQDnd6AEzeEunjxN7SW30CW+WaT1RJ6thGTQQ6NyYGAV1paBNQtq96cUMBLrBRIShmqKzuoqwzJ5UeWsa2uZIJdVmWBhSnDqzPrscpwHGfVmowFBuF+7mJ7mN9OZ2D/1Txxksq+K6aG5oBwSfJydglKtFXwVMoRX8/FKphnxnbEnX3CMbFHGG7v1QYvTOgijptl8rq18RGzj7Ynrdk3QaNTqDe6hHlDp+fFy1+WdHqDVWmA5exwaeug58YLWd5fTqbjXHoxyiv1Yj9aP4sMr2XHCWlWOOm86RJivsb+y5EsoDhg/MEP9VFXm9XLLdWK2UtbGV7LkgaW3VXIOSjs/DVk701GUYX4XD5qhdmJmVS6JDiT6hbuY5ZFCpWUw7BsijRoZVnT9oEe8PdUYs6IGKtLm+KktbJKMVhmnQ4+35ci1t1uOHDdZpY3S5LhZZkgNoYCTaVZuyhWTxgT7CVmoVgdLxvr2K4hZhnbYIvLnD5qBRZO7II3p/TEm1N62myj5aF0E78nLPPqSA9eRq2Qi72O91/NE79vfY31mtklFbhsfJ7OYd5YdodQesEC2kBP88lg7AoE++2pS/0uwz4bOSzDazzxjA7yxH/u6YUXJnTG7CFRkMs4fPxAX0wf1B4rp/YRg2FHjOkSipmD22NijzBM7BGGf03uAX9PJYK9VWIQqXKTWQVI1eE4Dm/d1QMTe4SJmc01e67i0DWhLaTYw7WaAC8ywANLbuuCaR0MNn8XfCQ1vKykSmjFV7f3wF0pxxDjJfok40k4K0VxRlbXEitR+PVkBmavP4w7P/kHr/96zirDWx3LEoT6JmKktdKeSrn4u1WboZIV3arbh80dBbzEinQyWXVlB3UlPUCx7Eh1wTS79N3FOMOe1fAWaSrFCQuWK4mxS/03LDK84X5qsT3M5qM38cBnBzBuxR6k5JaJk2DYD+mV7BKcMl6+7dnWt14HE6mOod5YeX9frHqoPz5+sJ9Z1uvZcZ1wV78IjOsWKmZJLWt4i8p1Yo1nTTW8zB3GZT1/rqZuM6dEC54Xgjz241daUQWDgbfZOqhbuI94Ce29nRfF+yjkphXdQsQWR+Zjl7atupBZImaqxQxvHUoaWJAa4q0yu7wGQJzYmCZpP2drcg17j3NLtajSG8QsuJ+kh21t2AGqRFJP7qV2qzXgjYsOEC9dA0JgJD3ABkuyn7ZKGqRtu6pjmrSmE0+SHh4aLWaf2wd6IMhLhZwSrTiRTIp99sJ81Ag3vsdZxisf2SXmJzMsMx4d5CkGRSlihtdYj9vGF0MkB0pHM1Mso3/FeALFTlotPwf2YjWJ/1zJFTO8vYwlHDo9L2a1O4R4YWSnYLMgI8giaI+xKAGwrEG2B5sEl1eqBc/zZldQ7urXFvNGdxCzxiE+avzrzh5i0O4od6Ucr03ugVUP9ceqh/qLHQw4zjTPomOoV51+B6f0bYtVD/XH/2YNxL3924LngWc3nsDhlALx8+JXQwnCjEHtMCDYdhmdNMMrfm/rWM7AmGprhZPwS8aA195sdl2wmuEdZzPFk8SvDlyXdGmoLeA1nXB0D/exuwNIdXzdTWVZbf097D5hGBJLGV7SCkkD0dpWRLMXK2lo4+cuqTO03nZGUTnSCssh44C+xi+2mOHV6FBqzPB6qsyzd5YT16Q1vBN6hGF4xyB0DPFCkJcSpdoqPPrlEdwsKIebjBOXmbySUyquNtTbYhnThpLQsw1W3NcHaoXc1CO2xLy9FwviAjyVNa7cxEzqJQSnB67l2ZwAx4KFUB+1aeZzZRWKK0yrn1keSBaM6wS5jEPShWxxAoafsc4XsJ3h1Rt4MePHMtNsoohYK+xZewBkGVCE+qitOlywYIXVOivlMpuLRQR6KiGXceCNPXxZhtfHziwHYH0J0lMp1JOzYNOyF6/0asOA9qaTne4WJQ1mGV5jwFtcUSVOjGOLVdSUQWQHovyySjFz2S3cB8+P74yxXUPx/eODxYmYlisQVukN4qSpUF8VAr1UcJNx0Bt45JRobc66VitkxmywMcObq0FxhU6S7fQRG+R7q93qXH7AsLpSluEVS6QcyPACwmVtQOhEwDLdEX7u4gmJTs+b9QFfNLGrWFphGfBaTuSq64Q1AAhiq5SVVqJcZ7qCUtvl7obCAt7OoXV/LczSSd3Q1t8daYXluG/NfgDCd4X1Da8r9ltVUlFlmjRbh++tFOuveyy1AOmF5WJ5jr31ynXRv72/eDLsrXZDjwgfYYlk449tbQFvhJ+7eNIxY3B7p5RcsOOlPQkUJtRHjSdGxuKe/m3r1AquOaGAl5iRZvkA53VqMAWg6hpreNkqYd3CfcQzel+zkgYhq+alMv+hY61bTAGv6YDo667AhkfikbhgJH6aNxTeKjfxwNmvnT96GfsZXs4qNbXnkSyG0FjY5Xad3nzpW7F+184fmcgAD/Rv7w+eF8oQ9AbebPIhe2/a+KrhbXwfed4ULHqp3KwOStFBnrhvgHBiwCbf+UuCYlOLI1NQlFZQjkq9MNmGBVms1ZG4rKYdmYK2/u5wk2SZgr1VCPZWmWWeWNbjpvFz5qGyfVCVyTiz4Jy1JLP3sh4gLBggvT+rJ2cHX8vWZGmSqxusV3KEnzv8PJTmJQ2SDK+XypQxZhnIs9V0MZBi++RSVgn0Bh5yGYcQbzUeHxmL/84cgBAfNR6Mawe5jMOhlHyzpWNzSrUw8EJdZKCn8P6y/ZpRVC7WZw+M8hdPJqKDvCCTcWKG93p+Gc4bxxnuq4a/pxKjOofgli4hmDM8psb3tSYsALMsabCnpZIt/dr7Q+UmE0t5grxUULrJzFbLijSuzgcI2dCPHuiLXm19cd/ASLNttQvwED+LCjmHjiF1D5rYlY5ynd7spM1WWU5juLtfW3QJ88bd/SMc3oa3WoEPH+hrvKqgRJCXUswiO8LLOGmtRFslnlQ6mmls4+uOQTEB4HngzW3nxZPSTqHOD3jd5DL83y0d0DPCF9/OGYTlU3qJf/NQyms9CVS6yfDosGiM6RKCyX0c3x9SrF9zXQPXhRO74N17ezu1zrkxUcBLzEizfIB12cGfF7IQ9+YfNpf6/DDpMoa/86fVZJm8Uq2Y8Qn3czfLQhVX6HDryj1YsOkEAOCosbm8NBMmljSUm9qSeVWT4U3N10CnN4iTktpYtCxq6++B14ztkABgSIdA8WB6IbNEPNPv1UgZXimlm0zMMEkzs3Xpl8hMNpY1vLntPGJf3obR7+4SZwmfMV4ajw32glohEw/W7GShuhm4/3dLRyjdZOJELT/JwUacDCbJ8LIFIaICPTDeOJnuwFWhPRvrWmBPlwaFXGY26z3UR20M5ITnDPBUij/gLBvuWUMmXLr4RKFYC1i3iS/SUgw2e9zXw3ZJQ4bkZO/2XuHoEOKFaYOEEwA/D6X43oVYrGAVLilrKK/Ui/XQ9pQ0sHKLMON7JRXmq8aE7sL+eOH7U/jnSq6xW4GxF7DkZEKsKy+qEMtVogI9xUb0rD5XWsNrCsyFE0l3pRzrZg3EU2M6Vjvu2oglDWKG1/EaXkCo45Uuac1am0mXzbXsVtA93Bdb5w+zmhymdJMh0hg4dAzxtnlloTaekglfl40Tqfw9616f6izxMYHY8cwIs8vYjujXzh+7XxiNI4vH4cjicViU0NXhbZkyvJIaXgdLGgDT3ARWmx3uq67TiW9dPDo8Br/83zD0iPBFz7a+mNhDWPHN3lrYRQld8b9ZAx2+QmLptl7hCPFWWa0e2tpRwEvMWNbsWi4OsWZ3MrJLtEg8Z95XUKc34LO/k3Ejv9ys52BmUQXuWyMsmdnGV43oIE9JhleHvZdzcSGzBFuOpeFcerHYP1Z6MGKX14s0OjEj42lRwxspCXgziyrA80KGJMjGJfMpfSNw/8BIeCrlmNQ7XJwdXFSuE1cbCnewNrC+bK0EZs+iE5YmGX/QmJQ8DfZcEpaFZFn0AVEB4DhTHS47Kaku6xru547pkgyNNMPLstM5pVqxHIPV70YHeSLWOLGpUm/A1pNCbbHSzf4MFrucLDyX8LpYMBYd5CkGeuySvEcN2zUtL1whZmN91XU70EknmrD3T+wXXW5Zw8sy6u4I81XjjwUjMXeUaanW23qGw1vlhv7tzDsaRIiLT1TgfGYxDLyQibTs9SxleQCt7iTpyZEdoHKT4XRaEab99yAe23BUDHilWU7pim9iBwcfNWYNiYLKTYZbjQduVmeYUVSBf++4AMCxS/vV6RDsLW7/ZoFG7Btbn0lG0mCOBbqhku99hzr0tGVlDfV5zewkinWIaKl1kg2FtewrraiSrPTo+Hs0MCoAozub2p81RP1udV6Y0BnB3ipxcl9jG9ctFIdeGYthHet3QtPSUMBLzBRYZKekra2KNDocMWZgLXvaHr6WL2aV2IzXUm0Vpq7dj6s5ZWjjq8bXj8ZD5SYXM4iFmkocumZaIGHV7qs4b6xTlGZ4xclA5aY+n14WrWjaBXjAUylHSUUVfjgmtP9q46e22R6I4zj8++5eOPXqBMQGe8FT5WYW4PaJ9G2yzEqYGPCaMqUsa1mXDK+/pxL/LLwFx5eME4PUf67koUKnx+mbQoaXLVLBAja2Ul1NdYNzR8WKQar0YBPkpQTHCXW7bDIG6wgQE+wFjuPEZTq/P3IDgBA02vs+S+skWUDGVjyLkZxEMbZaklk+PqvYsRpewDywZJknPxsZXunVhuqykUsndcPxpePMZk8DEOuU0wvLbS67a4tl7bXlFQ6mZ1tfJD03EjMHt4dSLkPiuSx8vi8FgHmWM1wMeMvFDG+Ijwqju4Tg4hsTxcmM/h4KdDX2y62sMoDjgJGSYKK+fD1MCzx88tcV6PQ8urbxsWqhVRfSDhds8ltNGd6aDOsovNbR9QhgWGsy1iGipc6EbyjsSorZpLV6ZmSfN/YZBxqmQ0N1YoK9cHDRGLw2uUejPSehgJdYsCxhkGZ8d13KFgvtLetvpUsnXjL2Gd19MQfX8zQI9VFh0+ODxclHLFAy8Ka2MIBQb2rghUxmmCQAZQFvgUZSw2txyVqtkGNKP6G+6bM9yQBMAVF1pJd6YyUHzl5NUL/LhFq0Jiuv1IvlI3WtL1PIZcYaSuFgvO9qLk6nFaFSb0CQl0q8DG3K8AqBdU1Zk0AvFebdImQne0rWcneTy8SVllhgJM3wAqaJIuyEqC7Zmegg0/5hl/5Z7fXAqACzbDNguyUZI3bDkGZ461zSYMqes4DXV6zhNX03xKsNbrIa2w/ZWtFJWtJwNq32Dg2A8D1wl1z2rOmSv1De0wNzR8cCgLj4gPS7F2b8DkkzvLYyzBzH4ed5Q5H03EgkPTcSR14ZW+/Z5JZYz+jvjwgntHXtO2upZ4SvuO/Y6zQLeOsQTD8yLBonl45HQs82Do8n2JjhvSKWNFDAK+UltiUzZXj96vkedQ/3xT39hbkJwzs47wTNHnXt1UzqjwJeYsayhEHak1e6FKP0si3P82ZlDBcyhRWM2GXHsV1DxZIDQAjE2IHmRr51eyHWRowJ93OHjBPaabGMsGWGFwBmDI4CAIdWYJJONGmsDg22sJn6WcbLyz+fSENxRRXaBXhgkIN9geOMa9Jfz9Pgp+NpAISJRyy7yt7Lm6yGt5ZAdO6oDvj7xdF4YKD5ssumUgEhMGIBL6vzHBgVIF6WBOxrScZISxrYezRneAx2PT8K9w5oa1ZPDFiXvEhJl1QtdGDSGmBe0sAm/olLIEsWwxDb4/navtpQE1ZXejS1AD+dEPabPUGkNPi35zvwyLBo8/IUSS1xG2kNbzErebDdWUPpJkNssBdig70aZFlRFoCypaQn9XY8uASEE94J3YWSDLbIhvS1x9Yxe1yfelLAtDQvq323Z0KnK/GWBLwsEVPfDC8AvH13L/z94miXu7zviijgJWZYhpcd6FgAXKU3YNfFHPF+0su2V3PKkJKngULOwU3GoaSiChlFFWI/XVsBpDSoCvdVY/4tpppGaf0uIFzam9JXOAtnK3fZCmg6hXpjUIwpWI6o5nKuLdJsTu+21bd9amhhvsJBL6O4AjzP40tj66iHBrVzOCPgrVaIr4llx9hiHIApw3tTbH9W+0EkMsDDajzS5YXLK/XixCKW2Ve6yTCikymLUpdLtp1CvYWabC8lfIzZWJmMQ1SQJziOs7qUX1OG17S8sGNdGgCLGl7jgZidqF3ILBFriTOKai5nqAkrYbmep0GFzoDhHYPEnp41kQb/9nwHvNUKs5riMBs1vOmF5eIqayE+9n+vnEn6HR3Q3r/WZbbt8a/JPbD96eFiPW9ssBc4TjhJ86ljXXd9BUpaogGU4bUk7b3OflucUecsl3FmCRnSelHAS8ywAJdN5GIB8JHrBSgq14nN+aW1vWzJ2EExgeLl6/MZxWLz9t42SgSkP+YDogJwZ58I46o5sJnJfGZsR7F5PmC98ATDsrxA3YIMdnm8c6i3VbawMbFL93sv5+Bfv57HuYxiqNxkuG9AZC2PrBmrV6w0rm0vzaKz95L9zdEDrbQ2li1A4OuuMMsesrIGoG4HqwBPJTY9MRjfzhlks+5XIZeZZY/tquEtqXC8htdGSUOgl0rs47n/qrAqHDswW/YNtof08+vrrsB/7ult10mPtMuGvd+B6YPbi4FurKR2lZUFpRdViIGY5WppjUUa8LLFVerLXSkXa48B4UTuuzmDsH72QKdsvy4s+/talum4OrVCDqWx9If1End04QnimijgJWZYgBtrvIRcoKmEwcCL5QysiXxRuU6cjc96q47pEiIW/v92OgNllXp4KOU2a+ECJD9UA6L84alyw7dzBuHz2XFmB1wmMsADD8aZLqFXF/CO6xYqZqctG8LXpEeEL9bPGohPH+pn92MawsAof9zTvy0MPLDun2sAhBZj9Q3CpTPS3RVys16ulu+lo5dSpaUCYjlDsKdZgDqqcwhYzFbbkpqW+kT6oWMNdcx+kkCvxhpeY0lEoUYnll/U9dJokI0uDYDpxILVXUv7T9dViLdKrMd9a0pPs9ramkg/K/YGvGqFHN89NgirH+pndkXGst9xgKfSobZbztAp1BtymXAVqT61srWJjwk0W92qsViW+NCkNWvsakp1C+QQUhMKeIkZttAECxYNvNCbl7W0uss4MczAC7NlK3R6sXfuLV1CxQzX9tPCsqU9Imwv0WuW4TV2ZOgW7oORnaqfODDvlg7iErLVHQwUchn+N3Mglt/V02wJX3uM7hJiM9huTBzH4Z27e5ktZSrNWjuqX3s/8b3rE+kHhWSSlGU9tKMZXtPywloxu2950hHgqRTLKUKd3PpNGqjXtCKdj7sbVMagjU0O9LFRE17jc0mCE+mlb7be/D9XhAxvej36xbrJZVg9vT8+fKAvbutlf4DHMoPeKrc6XZaPCvLErT3Mn0fa7xiA2X83tmBvFVY/1B/rZw+0yoa2BtYZXgp4LXlbfE+b8mocaXnq9itPWj1WqhDqo4a3yg0l2iqkF1aIrXIGxwbCUylHWaUeBZpKyI1LjyrdZIgMcBc7CZTrhIljfaqZAMaCE2+Vm93tYEK81Vg7fQDOZxTXOFu9W7iPQ+vZNxcyGYfX7uguzkrvEVH/mmKVmxwDowLw9+VcDLSokbbK8Doa8LIa3uIKcQEPWycwy+/qiZ9PpNd7lr0l6cGvuisAgHBSEeqjFvsOA3Wv4Q2wUcMLAHHRgXCTcUjN1+BsehFOGwN/RxdIqOkEsNqxGd+H6lqS1VWYr1qsRW6q+l1mnHEBk9bIMuClDK816fdaIeeabCU60jJRwEvMsBpef08l/DwVKNFW4eC1POgNPAI8lQjzUcPPQ4myynIUluvENmXBXipwHIcuYeaBpq36XcCUIevb3t9mBrg6IzoFm018aq04jsN0J2R2pRZO7IK2/tcxe2i02e2WwaGjlwlZbeyZ9GLoDTzcFXKbAUqHEG9xlSNnktY8Vre0MBPqozILeOtcwysJrqVZJy+VG3pH+uHo9QI8/Plh5JZWIirQo85XG+qDBUqOBtmWwn3dcRyFAIDQJszwtnaWJQ00ac2a9Lvm625/H29CACppIBZYDW+Ap1I8qO+9LNQjdg/3MZsRX6CpRE6JUAMZbDwQtvV3N1vlqlc1HQ9u7xmOMV1CMHdUbMO8EGKle7gvlt/Vy+pAalXS4GgNr7GlEzsJGtcttMbSAmeTZnhrWloYMF9NzF0hF0sc7OUml5lKByzKBlgdb1axFnIZhxVT+zhtSVB73NqjDcZ2DcUjw6Jrv7MdpLXDoU2c4W3N/D2UkJ77U1sya14q03eNJvWRuqKAl4j0Bl7sS+rvoRQDowPJQj0iKxOQLvUrtioyBrwyGSdOLAr0VFa7HG67QA/8b9ZAh3vLEueRZni91W5m9b11EeSlgjTh4uyShdr4m9Xw1pbhtV7YpK4eGRaNkZ2C0a2N+VWNobGmz/S80R2cvgBDbcJ81fjvzAEY3tE5V0KkPbJDqunBS+pPLuPE7LxaIYM7Xa63Iq21pwlrpK6opIGIisp14CWzX1mGgS3kwA7sLJNWoKkUM8LBkkudXUK9cfJGIXq1bboleon9pBne+tQNKuTCamK5pZXwdVc0eumJtB1XTQtPAOaLJzga8M6/paPN2/u198fwjkFwV8jxf5L+0i2VNMNra5U14jyBnirkllZSdrcaXmYBL71HpG4o4CUitnoNy/JZXvruHi6UJ7AWToWSDK804L2zbwR+P5eJqQPr1zuWNA5p/9r6zgwP8VYjt7QSCT3DGr19lcMZXidnihRyGTY8Eu/UbTYlyvA2nkAvJZBFwVx1pDW8zlhljbgWCniJqEBjqt+V/j8g1DmyFlMssCiU1PBKMz+DYwNxYun4RhkzqT9pNrS+M8NHdg7GtdwyPDSofe13djJ/O7s0AOafV0czvK5CumgG1fA2LNapgTo02GZWw0vvEakjCniJiJUnsMBBGkB0beMtdlNgtVOF5Tpkl1hneEnLIg0O61sX99KtXbBgXCeH64Drw8+sS0PNP23Sy/QU8NYsxFuFCD93GHi+SfvwugLWqYGCOdvMuzTQ95bUDQW8RGSd4TX9oEj72rIfmgKNTpLhpQNhSyU9iDijdrApgl3APEiorT+n9PNKB86aucll+P3ZEeB5vsn2ravoamzr2MnG6pTE/LeKFuYgdUUBLxGxVdZsZXhZ/a709oKySuTaqOElLYu0pKElZ5aCvJTwVrlBJuNqLWnwVLmJC6tQLWDtans/iXPc078terb1FRedIea8qUsDqQf6FSMiluFl/Q2lwY90ZTP2Q5OSVwadXmjr0BqX+nQVCrkMKjcZtFWGFl07qHKT48d5Q8BxHNzsyESG+KhQklPl9ElrhDhKJuPQtU3LXSWyoUlreCngJXVF16eIKK/UtMoaAIR6qyGXcVArZOKSwYBpBnFJRZVwfw9Fo8/IJ87FMict/TJhhxBvxAbblx1rHyhMwgyjiViEtAhm8w3cW/ZvFWl8lOElIrE8wZit9fVQ4NNp/eClcjNbKcryzJrKGVq+AGP/3FAXaju15PZuGNM1BKO7hAAGfVMPhxBSC7MaXk/K8JK6oYCXiFjAG+RtOnOe0D3M6n6WNY/UjL7le31yD5y6WYg+kX5NPZRGEx3kKbba01HAS0iz5+uhAMcBHCjDS+qOAl4iMmV4aw5g3eQyccIPQBne1mBQTCAt80wIadZ81Aosvb0b3OS09DKpOwp4CQDAYOCRa6zhlWZ4q+PnqaCAlxBCSKOaPTS6qYdAWqhmMdPok08+QVRUFNRqNeLj43Ho0KFq76vT6fD6668jNjYWarUavXv3xo4dO8zus3z5cgwcOBDe3t4ICQnBnXfeiYsXLzb0y2jRCst10BuEjguBnrUHsNLLSdSDlxBCCCHNWZMHvBs3bsSCBQuwbNkyHDt2DL1798aECROQnZ1t8/6LFy/GmjVr8NFHH+HcuXN44oknMGXKFBw/fly8z+7duzFv3jwcOHAAiYmJ0Ol0GD9+PMrKyhrrZbU4rJzBz86OC9KJa5ThJYQQQkhz1uQB74oVKzBnzhzMnj0b3bp1w+rVq+Hh4YF169bZvP+GDRvw8ssvIyEhATExMXjyySeRkJCA9957T7zPjh07MGvWLHTv3h29e/fG559/jtTUVBw9erSxXlaLw1ZMs7efrp+kfVUw9eAlhBBCSDPWpDW8lZWVOHr0KBYtWiTeJpPJMHbsWOzfv9/mY7RaLdRq80lV7u7u2Lt3b7XPU1RUBAAICAiodptarVb8d3FxMQChfEKn09n3YhzAtt2Qz2GvzEINACDQU2HXeHzVpgkD/u7yZvEamkpz2o/EMbQPWz7ahy0f7cOWr7H3YV2ep0kD3tzcXOj1eoSGhprdHhoaigsXLth8zIQJE7BixQqMGDECsbGxSEpKwpYtW6DX224rZDAY8Mwzz2Do0KHo0aOHzfssX74cr732mtXtO3fuhIeHRx1fVd0lJiY2+HPU5u90DoAcuuI8bNu2rdb759yUgV0gOHFgDy7R9MdmsR9J/dA+bPloH7Z8tA9bvsbahxqNxu77trgw5YMPPsCcOXPQpUsXcByH2NhYzJ49u9oSiHnz5uHMmTM1ZoAXLVqEBQsWiP8uLi5GZGQkxo8fDx+fhlvmUafTITExEePGjYNC0bRNtM/uvARcT0HPTlFISOhS6/2z91/H72kXoXST4e5JE8FxXCOMsnlqTvuROIb2YctH+7Dlo33Y8jX2PmRX5O3RpAFvUFAQ5HI5srKyzG7PyspCWJj1ggcAEBwcjJ9++gkVFRXIy8tDeHg4Fi5ciJiYGKv7zp8/H7/++iv27NmDtm3bVjsOlUoFlcq6DlWhUDTKDmus56lJXpnQYizEx92usQQae/UGe6mgVFIDcKB57EdSP7QPWz7ahy0f7cOWrzHjJ3s16aQ1pVKJ/v37IykpSbzNYDAgKSkJgwcPrvGxarUaERERqKqqwg8//IDJkyeLf+N5HvPnz8ePP/6IP//8E9HR1LevNuKiE3Z2XGgfKJR6xAR7NtiYCCGEEEKcoclLGhYsWICZM2diwIABiIuLw8qVK1FWVobZs2cDAGbMmIGIiAgsX74cAHDw4EGkpaWhT58+SEtLw6uvvgqDwYAXX3xR3Oa8efPwzTff4Oeff4a3tzcyMzMBAL6+vnB3d2/8F9kCmFZZsy/g7dfOH188HIcuYd4NOSxCCCGEkHpr8oB36tSpyMnJwdKlS5GZmYk+ffpgx44d4kS21NRUyGSmRHRFRQUWL16M5ORkeHl5ISEhARs2bICfn594n1WrVgEARo0aZfZc69evx6xZsxr6JbVIdW1LxnEcRnYKbsghEUIIIYQ4RZMHvIBQazt//nybf9u1a5fZv0eOHIlz587VuD2e5501NJdgMPDIKxOWFaZFJAghhBDS2jT5whOk6ZktK+xFE9AIIYQQ0rpQwEvEcgY/DwUUcvpIEEIIIaR1oeiG1HnCGiGEEEJIS0IBLxEDXnsnrBFCCCGEtCQU8BJThwaasEYIIYSQVogCXoIcKmkghBBCSCtGAS9BbonQkizImzo0EEIIIaT1aRZ9eEnjMxh4vPjDKcg5DpeySgBQDS8hhBBCWicKeF3U9XwNNh+9aXYbLTpBCCGEkNaIShpcVHmlHgAg40y3RQd6NtFoCCGEEEIaDmV4XZS2Sgh42/i648tH4lCo0SEqiAJeQgghhLQ+FPC6KG2VAQCgUsgQG+zVxKMhhBBCCGk4VNLgoipZwOsmb+KREEIIIYQ0LAp4XZSY4XWjjwAhhBBCWjeKdlwUq+GlgJcQQgghrR1FOy5Kq2M1vFTSQAghhJDWjQJeF0UlDYQQQghxFRTtuCgqaSCEEEKIq6Box0VpqUsDIYQQQlwEBbwuitXwKinDSwghhJBWjqIdF1Wpp5IGQgghhLgGinZclKlLA30ECCGEENK6UbTjoqiGlxBCCCGuggJeF0VdGgghhBDiKijacVHUh5cQQgghroKiHRdFK60RQgghxFU4FPCuX78eGo3G2WMhjYhKGgghhBDiKhyKdhYuXIiwsDA88sgj2Ldvn7PHRBoBlTQQQgghxFU4FO2kpaXhiy++QG5uLkaNGoUuXbrg7bffRmZmprPHRxoIBbyEEEIIcRUORTtubm6YMmUKfv75Z9y4cQNz5szB119/jXbt2uGOO+7Azz//DIPB4OyxEieqpLZkhBBCCHER9U7vhYaGYtiwYRg8eDBkMhlOnz6NmTNnIjY2Frt27XLCEElDoBpeQgghhLgKh6OdrKwsvPvuu+jevTtGjRqF4uJi/Prrr7h27RrS0tJw3333YebMmc4cK3EisaSBVlojhBBCSCvnULQzadIkREZG4vPPP8ecOXOQlpaGb7/9FmPHjgUAeHp64rnnnsONGzecOljiPGJbMippIIQQQkgr51DAGxISgt27d+PMmTN45plnEBAQYHWf4OBgXLt2za7tffLJJ4iKioJarUZ8fDwOHTpU7X11Oh1ef/11xMbGQq1Wo3fv3tixY0e9tumKqKSBEEIIIa7CoWjnf//7HwYPHlzjfTiOQ/v27Wvd1saNG7FgwQIsW7YMx44dQ+/evTFhwgRkZ2fbvP/ixYuxZs0afPTRRzh37hyeeOIJTJkyBcePH3d4m65IS5PWCCGEEOIiHAp4n3rqKXz44YdWt3/88cd45pln6rStFStWYM6cOZg9eza6deuG1atXw8PDA+vWrbN5/w0bNuDll19GQkICYmJi8OSTTyIhIQHvvfeew9t0RVTDSwghhBBX4ebIg3744Qds3brV6vYhQ4bg3//+N1auXGnXdiorK3H06FEsWrRIvE0mk2Hs2LHYv3+/zcdotVqo1Wqz29zd3bF37956bVOr1Yr/Li4uBiCUT+h0OrteiyPYthvyOWyp0hugN/AAABlvaPTnb22aaj8S56F92PLRPmz5aB+2fI29D+vyPA4FvHl5efD19bW63cfHB7m5uXZvJzc3F3q9HqGhoWa3h4aG4sKFCzYfM2HCBKxYsQIjRoxAbGwskpKSsGXLFuj1eoe3uXz5crz22mtWt+/cuRMeHh52vx5HJSYmNvhzSGn1ANv1fyUlQkVVDU7R2PuROB/tw5aP9mHLR/uw5WusfajRaOy+r0MBb4cOHbBjxw7Mnz/f7Pbt27cjJibGkU3a7YMPPsCcOXPQpUsXcByH2NhYzJ49u17lCosWLcKCBQvEfxcXFyMyMhLjx4+Hj4+PM4Ztk06nQ2JiIsaNGweFQtFgz2OpQFMJHNoFAJiUcCvc5FTWUB9NtR+J89A+bPloH7Z8tA9bvsbeh+yKvD0cCngXLFiA+fPnIycnB7fccgsAICkpCe+9957d5QwAEBQUBLlcjqysLLPbs7KyEBYWZvMxwcHB+Omnn1BRUYG8vDyEh4dj4cKFYqDtyDZVKhVUKpXV7QqFolF2WGM9D2OAkA13k3FwV1u/buKYxt6PxPloH7Z8tA9bPtqHLV9jxk/2cii19/DDD+O9997D//73P4wePRqjR4/GV199hVWrVmHOnDl2b0epVKJ///5ISkoSbzMYDEhKSqq1C4RarUZERASqqqrwww8/YPLkyfXepquglmSEEEIIcSUOZXgB4Mknn8STTz6JnJwcuLu7w8vLy6HtLFiwADNnzsSAAQMQFxeHlStXoqysDLNnzwYAzJgxAxEREVi+fDkA4ODBg0hLS0OfPn2QlpaGV199FQaDAS+++KLd23R1pg4NVLxLCCGEkNbP4YCXCQ4Ortfjp06dipycHCxduhSZmZno06cPduzYIU46S01NhUxmykRWVFRg8eLFSE5OhpeXFxISErBhwwb4+fnZvU1XZ1pljTK8hBBCCGn9HA54N2/ejE2bNiE1NRWVlZVmfzt27FidtjV//nyrCXDMrl27zP49cuRInDt3rl7bdHVU0kAIIYQQV+JQxPPhhx9i9uzZCA0NxfHjxxEXF4fAwEAkJydj4sSJzh4jcTJaZY0QQgghrsShgPfTTz/F2rVr8dFHH0GpVOLFF19EYmIinnrqKRQVFTl7jMTJxAwvrbJGCCGEEBfgUMSTmpqKIUOGABBWOSspKQEATJ8+Hd9++63zRkcaRKUxw6uk/ruEEEIIcQEORTxhYWHIz88HALRr1w4HDhwAAFy7dg08zztvdKRBmLo0UMBLCCGEkNbPoYjnlltuwdatWwEAs2fPxrPPPotx48Zh6tSpmDJlilMHSJzP1KWBangJIYQQ0vo51KVh7dq1MBiEoGnevHkIDAzEvn37cMcdd+Dxxx936gCJ81GXBkIIIYS4kjoHvFVVVXjrrbfw8MMPo23btgCA+++/H/fff7/TB0cahqlLAwW8hBBCCGn96hzxuLm54Z133kFVVVVDjIc0AmpLRgghhBBX4lCKb8yYMdi9e7ezx0IaiVZHbckIIYQQ4jocquGdOHEiFi5ciNOnT6N///7w9PQ0+/sdd9zhlMGRhkElDYQQQghxJQ4FvHPnzgUArFixwupvHMdBr9fXb1SkQVFJAyGEEEJciUMBL+vQQFomFvAqKcNLCCGEEBdAEY8LorZkhBBCCHElDmV4X3/99Rr/vnTpUocGQxoH1fASQgghxJU4FPD++OOPZv/W6XS4du0a3NzcEBsbSwFvMyeutKagGl5CCCGEtH4OBbzHjx+3uq24uBizZs2ipYVbACppIIQQQogrcVrE4+Pjg9deew1Llixx1iZJA6EuDYQQQghxJU5N8RUVFaGoqMiZmyQNgGp4CSGEEOJKHCpp+PDDD83+zfM8MjIysGHDBkycONEpAyMNh1ZaI4QQQogrcSjgff/9983+LZPJEBwcjJkzZ2LRokVOGRhpOJV6KmkghBBCiOtwKOC9du2as8dBGhHr0kALTxBCCCHEFTgU8RQVFSE/P9/q9vz8fBQXF9d7UKRhUQ0vIYQQQlyJQxHP/fffj++++87q9k2bNuH++++v96BIw6K2ZIQQQghxJQ5FPAcPHsTo0aOtbh81ahQOHjxY70GRhiVmeGnhCUIIIYS4AIcCXq1Wi6qqKqvbdTodysvL6z0o0nB4nkcllTQQQgghxIU4FPHExcVh7dq1VrevXr0a/fv3r/egSMNh2V2AAl5CCCGEuAaHujS88cYbGDt2LE6ePIkxY8YAAJKSknD48GHs3LnTqQMkzmUe8FJJAyGEEEJaP4dSfEOHDsX+/fsRGRmJTZs24ZdffkGHDh1w6tQpDB8+3NljJE7EJqxxHKCQc008GkIIIYSQhudQhhcA+vTpg6+//tqZYyGNQFq/y3EU8BJCCCGk9XMow7tt2zb8/vvvVrf//vvv2L59e70HRRoOK2lQyql+lxBCCCGuwaGoZ+HChdDr9Va38zyPhQsX1ntQpOGwVdaoJRkhhBBCXIVDAe/ly5fRrVs3q9u7dOmCK1eu1HtQpOHQohOEEEIIcTUORT2+vr5ITk62uv3KlSvw9PSs96BIw6FlhQkhhBDiahyKeiZPnoxnnnkGV69eFW+7cuUKnnvuOdxxxx1OGxxxPlPASyUNhBBCCHENDgW877zzDjw9PdGlSxdER0cjOjoaXbt2RWBgIN599906b++TTz5BVFQU1Go14uPjcejQoRrvv3LlSnTu3Bnu7u6IjIzEs88+i4qKCvHver0eS5YsQXR0NNzd3REbG4t//etf4Hm+zmNrbbQ6Y0mDgjK8hBBCCHENDrUl8/X1xb59+5CYmIiTJ0/C3d0dvXr1wogRI+q8rY0bN2LBggVYvXo14uPjsXLlSkyYMAEXL15ESEiI1f2/+eYbLFy4EOvWrcOQIUNw6dIlzJo1CxzHYcWKFQCAt99+G6tWrcIXX3yB7t2748iRI5g9ezZ8fX3x1FNPOfKSWw0qaSCEEEKIq3G4Dy/HcRg/fjzGjx9frwGsWLECc+bMwezZswEIyxP/9ttvWLdunc2OD/v27cPQoUPx4IMPAgCioqLwwAMP4ODBg2b3mTx5Mm677TbxPt9++221mWOtVgutViv+u7i4GACg0+mg0+nq9fpqwrbdkM9hSaOtBAAo5VyjPm9r1hT7kTgX7cOWj/Zhy0f7sOVr7H1Yl+dxOOAtKyvD7t27kZqaisrKSrO/2ZtFraysxNGjR7Fo0SLxNplMhrFjx2L//v02HzNkyBB89dVXOHToEOLi4pCcnIxt27Zh+vTpZvdZu3YtLl26hE6dOuHkyZPYu3evmAG2tHz5crz22mtWt+/cuRMeHh52vZb6SExMbPDnYI5mcQDkKMjNwbZt2xrteV1BY+5H0jBoH7Z8tA9bPtqHLV9j7UONRmP3fR0KeI8fP46EhARoNBqUlZUhICAAubm58PDwQEhIiN0Bb25uLvR6PUJDQ81uDw0NxYULF2w+5sEHH0Rubi6GDRsGnudRVVWFJ554Ai+//LJ4n4ULF6K4uBhdunSBXC6HXq/Hm2++iWnTptnc5qJFi7BgwQLx38XFxYiMjMT48ePh4+Nj12txhE6nQ2JiIsaNGweFQtFgzyOVtvcakHwZHdpHICGhZ6M8Z2vXFPuROBftw5aP9mHLR/uw5WvsfciuyNvDoYD32WefxaRJk7B69Wr4+vriwIEDUCgUeOihh/D00087skm77dq1C2+99RY+/fRTxMfH48qVK3j66afxr3/9C0uWLAEAbNq0CV9//TW++eYbdO/eHSdOnMAzzzyD8PBwzJw502qbKpUKKpXK6naFQtEoO6yxngcASrVCDa+vh4p+UJysMfcjaRi0D1s+2octH+3Dlq8x4yd7ORTwnjhxAmvWrIFMJoNcLodWq0VMTAzeeecdzJw5E3fddZdd2wkKCoJcLkdWVpbZ7VlZWQgLC7P5mCVLlmD69Ol49NFHAQA9e/ZEWVkZHnvsMbzyyiuQyWR44YUXsHDhQtx///3ifa5fv47ly5fbDHhdSXGFUO/i404/JoQQQghxDQ5N1VcoFJDJhIeGhIQgNTUVgNC94caNG3ZvR6lUon///khKShJvMxgMSEpKwuDBg20+RqPRiM/NyOVCT1nWdqy6+xgMBrvH1loVl1cBAHzUDpdvE0IIIYS0KA5FPX379sXhw4fRsWNHjBw5EkuXLkVubi42bNiAHj161GlbCxYswMyZMzFgwADExcVh5cqVKCsrE7s2zJgxAxEREVi+fDkAYNKkSVixYgX69u0rljQsWbIEkyZNEgPfSZMm4c0330S7du3QvXt3HD9+HCtWrMDDDz/syMttVViG15cyvIQQQghxEQ4FvG+99RZKSkoAAG+++SZmzJiBJ598Eh07dsS6devqtK2pU6ciJycHS5cuRWZmJvr06YMdO3aIE9lSU1PNsrWLFy8Gx3FYvHgx0tLSEBwcLAa4zEcffYQlS5Zg7ty5yM7ORnh4OB5//HEsXbrUkZfbqhSVU0kDIYQQQlyLQwHvgAEDxP8OCQnBjh07bN7vn3/+wYABA2xOCJOaP38+5s+fb/Nvu3btMvu3m5sbli1bhmXLllW7PW9vb6xcuRIrV66s8XldUTELeNUU8BJCCCHENTToclsTJ05EWlpaQz4FqaPiCmMNrzvV8BJCCCHENTRowMsmkZHmg2V4qYaXEEIIIa6iQQNe0rxU6PTQVgmdKqiGlxBCCCGuggJeF8I6NHAc4KWkkgZCCCGEuAYKeF2IqQevAjIZ18SjIYQQQghpHA0a8HIcBVXNiWmVNcruEkIIIcR10KQ1F1JELckIIYQQ4oIcSvWVl5eD53l4eHgAAK5fv44ff/wR3bp1w/jx48X7scUpSPNAPXgJIYQQ4oocyvBOnjwZX375JQCgsLAQ8fHxeO+99zB58mSsWrXKqQMkzsN68FJLMkIIIYS4EocC3mPHjmH48OEAgM2bNyM0NBTXr1/Hl19+iQ8//NCpAyTOI2Z4qYaXEEIIIS7EoYBXo9HA29sbALBz507cddddkMlkGDRoEK5fv+7UARLnoZIGQgghhLgihwLeDh064KeffsKNGzfw+++/i3W72dnZ8PHxceoAifOYujRQwEsIIYQQ1+FQwLt06VI8//zziIqKQnx8PAYPHgxAyPb27dvXqQMkzsP68FINLyGEEEJciUPFnPfccw+GDRuGjIwM9O7dW7x9zJgxmDJlitMGR5yL+vASQgghxBU5HPmEhYUhLCzM7La4uLh6D4g0HOrDSwghhBBXZHfAe9ddd9m90S1btjg0GNKwTF0aKOAlhBBCiOuwO+D19fVtyHGQRkB9eAkhhBDiiuwOeNevX9+Q4yANjOd5aktGCCGEEJfkUJeGa9eu4fLly1a3X758GSkpKfUdE2kAmko9qgw8AJq0RgghhBDX4lDAO2vWLOzbt8/q9oMHD2LWrFn1HRNpAKxDg0LOwV0hb+LREEIIIYQ0HocC3uPHj2Po0KFWtw8aNAgnTpyo75hIA2A9eH3UCnAc18SjIYQQQghpPA4FvBzHoaSkxOr2oqIi6PX6eg+KOB+tskYIIYQQV+VQwDtixAgsX77cLLjV6/VYvnw5hg0b5rTBEecp0rAJa1S/SwghhBDX4lD08/bbb2PEiBHo3Lkzhg8fDgD4+++/UVxcjD///NOpAyTOQRleQgghhLgqhzK83bp1w6lTp3DfffchOzsbJSUlmDFjBi5cuIAePXo4e4zECWjRCUIIIYS4qjpleNetW4c77rgDQUFBCA8Px1tvvdVQ4yJOxhadoB68hBBCCHE1dcrwfvXVV2jbti2GDBmCt99+GxcuXGiocREnKxIzvFTDSwghhBDXUqeA988//0RGRgbmzp2Lo0ePIi4uDh07dsRzzz2HPXv2wGAwNNQ4ST2xkgZaVpgQQgghrqbONbz+/v546KGHsGnTJuTm5uKjjz5CeXk5pk2bhpCQEMyYMQObN29GWVlZQ4yXOIhNWvOmkgZCCCGEuBiHJq0xSqUSt956Kz799FPcuHEDO3bsQFRUFP71r39hxYoVzhojcYIyrdBCzltFJQ2EEEIIcS1OjX4GDBiAAQMG4PXXX4dOp3Pmpkk9lWiFSWueFPASQgghxMU4FP3o9Xp8/vnnSEpKQnZ2tlntLsdxSEpKgkJBl86bkzJjwOtFAS8hhBBCXIxD0c/TTz+Nzz//HLfddht69OgBjuOcPS7iZBTwEkIIIcRVORT9fPfdd9i0aRMSEhKcMohPPvkE//nPf5CZmYnevXvjo48+QlxcXLX3X7lyJVatWoXU1FQEBQXhnnvuwfLly6FWq8X7pKWl4aWXXsL27duh0WjQoUMHrF+/HgMGDHDKmFua0gpW0iBv4pEQQgghhDQuhwJepVKJDh06OGUAGzduxIIFC7B69WrEx8dj5cqVmDBhAi5evIiQkBCr+3/zzTdYuHAh1q1bhyFDhuDSpUuYNWsWOI4TJ8oVFBRg6NChGD16NLZv347g4GBcvnwZ/v7+ThlzS8PzPEorjRleNWV4CSGEEOJaHOrS8Nxzz+GDDz4Az/P1HsCKFSswZ84czJ49G926dcPq1avh4eGBdevW2bz/vn37MHToUDz44IOIiorC+PHj8cADD+DQoUPifd5++21ERkZi/fr1iIuLQ3R0NMaPH4/Y2Nh6j7cl0lTqwXYVlTQQQgghxNU4FP3s3bsXf/31F7Zv347u3btbTVDbsmWLXduprKzE0aNHsWjRIvE2mUyGsWPHYv/+/TYfM2TIEHz11Vc4dOgQ4uLikJycjG3btmH69OnifbZu3YoJEybg3nvvxe7duxEREYG5c+dizpw5Nrep1Wqh1WrFfxcXFwMAdDpdg3abYNtu6I4WhWXCa5NxgBsM1EHDyRprP5KGQ/uw5aN92PLRPmz5Gnsf1uV5HAp4/fz8MGXKFEceaiY3Nxd6vR6hoaFmt4eGhla7bPGDDz6I3NxcDBs2DDzPo6qqCk888QRefvll8T7JyclYtWoVFixYgJdffhmHDx/GU089BaVSiZkzZ1ptc/ny5Xjttdesbt+5cyc8PDzq+Sprl5iY2KDbzy4HADeoZDy2b9/eoM/lyhp6P5KGR/uw5aN92PLRPmz5GmsfajQau+/L8c6oS3BQeno6IiIisG/fPgwePFi8/cUXX8Tu3btx8OBBq8fs2rUL999/P9544w3Ex8fjypUrePrppzFnzhwsWbIEgFBjPGDAAOzbt0983FNPPYXDhw/bzBzbyvBGRkYiNzcXPj4+znzJZnQ6HRITEzFu3LgGbeN2Oq0Id60+iDa+aux5fkSDPY+raqz9SBoO7cOWj/Zhy0f7sOVr7H1YXFyMoKAgFBUV1RqvNWlBZ1BQEORyObKyssxuz8rKQlhYmM3HLFmyBNOnT8ejjz4KAOjZsyfKysrw2GOP4ZVXXoFMJkObNm3QrVs3s8d17doVP/zwg81tqlQqqFQqq9sVCkWj7LCGfp4KYZE1eKnc6EekATXW54U0HNqHLR/tw5aP9mHL15jxk70cmrSWlZWF6dOnIzw8HG5ubpDL5Wb/s5dSqUT//v2RlJQk3mYwGJCUlGSW8ZXSaDSQycyHzZ6TJauHDh2Kixcvmt3n0qVLaN++vd1ja01MLclowhohhBBCXI9DEdCsWbOQmpqKJUuWoE2bNvVaeGLBggWYOXMmBgwYgLi4OKxcuRJlZWWYPXs2AGDGjBmIiIjA8uXLAQCTJk3CihUr0LdvX7GkYcmSJZg0aZIY+D777LMYMmQI3nrrLdx33304dOgQ1q5di7Vr1zo8zpaszNiSzJtakhFCCCHEBTncpeHvv/9Gnz596j2AqVOnIicnB0uXLkVmZib69OmDHTt2iBPZUlNTzTK6ixcvBsdxWLx4MdLS0hAcHIxJkybhzTffFO8zcOBA/Pjjj1i0aBFef/11REdHY+XKlZg2bVq9x9sSiRleJQW8hBBCCHE9DkVAkZGRTunBy8yfPx/z58+3+bddu3aZ/dvNzQ3Lli3DsmXLatzm7bffjttvv91ZQ2zRSrVCES8tOkEIIYQQV+RQDe/KlSuxcOFCpKSkOHk4pCGUaY2rrFENLyGEEEJckEMR0NSpU6HRaBAbGwsPDw+rWXL5+flOGRxxjlItm7Rm/4RCQgghhJDWwqGAd+XKlU4eBmlIpWKGl9q8EEIIIcT1OBTw2lqtjDRfbNKaF2V4CSGEEOKCHKrh3bZtG37//Xer23fu3ElL1zZDrC0ZTVojhBBCiCtyKOBduHAh9Hq91e0GgwELFy6s96CIc4k1vNSWjBBCCCEuyKGA9/Lly1ZL9wJAly5dcOXKlXoPijiXqaSBAl5CCCGEuB6HAl5fX18kJydb3X7lyhV4enrWe1DEucS2ZFTSQAghhBAX5FDAO3nyZDzzzDO4evWqeNuVK1fw3HPP4Y477nDa4IhzlIhtySjgJYQQQojrcSjgfeedd+Dp6YkuXbogOjoa0dHR6Nq1KwIDA/Huu+86e4ykHnieFzO83hTwEkIIIcQFORQB+fr6Yt++fUhMTMTJkyfh7u6OXr16YcSIEc4eH6mncp0eBuMq0JThJYQQQogrqnMEpNPp4O7ujhMnTmD8+PEYP358Q4yLOAnr0MBxgIeS+vASQgghxPXUuaRBoVCgXbt2NtuSkeanTCvsJy+lGziOa+LREEIIIYQ0PodqeF955RW8/PLLyM/Pd/Z4iJOxlmRUzkAIIYQQV+VQFPTxxx/jypUrCA8PR/v27a1akR07dswpgyP1V0otyQghhBDi4hyKgu68804nD4M0lFJqSUYIIYQQF+dQFLRs2TK77vftt9/ijjvuoMUompC46ISKJqwRQgghxDU5VMNrr8cffxxZWVkN+RSkFmJJA2V4CSGEEOKiGjTg5Xm+ITdP7EAlDYQQQghxdQ0a8JKmR6usEUIIIcTVUcDbypVQWzJCCCGEuDgKeFu5MippIIQQQoiLo4C3lWM1vN7Uh5cQQgghLqpBA9727dtDoVA05FOQWoiT1pQU8BJCCCHENTkU8MbExCAvL8/q9sLCQsTExIj/PnPmDCIjIx0fHam3MlppjRBCCCEuzqGANyUlBXq93up2rVaLtLS0eg+KOA/14SWEEEKIq6tTFLR161bxv3///Xf4+vqK/9br9UhKSkJUVJTTBkfqr0wrnJjQpDVCCCGEuKo6RUF33nknAIDjOMycOdPsbwqFAlFRUXjvvfecNjhSf6W0tDAhhBBCXFydAl6DwQAAiI6OxuHDhxEUFNQggyLOo60SMrxqBQW8hBBCCHFNDl3nvnbtmtVthYWF8PPzq+94iBPxPA9tlXCSonKjgJcQQgghrsmhSWtvv/02Nm7cKP773nvvRUBAACIiInDy5EmnDY7Uj07Pg+eF/1YpqOUyIYQQQlyTQ1HQ6tWrxXZjiYmJ+OOPP7Bjxw5MnDgRL7zwglMHSBzHyhkAQOVGAS8hhBBCXJNDJQ2ZmZliwPvrr7/ivvvuw/jx4xEVFYX4+HinDpA4jpUzAIBSTgEvIYQQQlyTQ1GQv78/bty4AQDYsWMHxo4dC0CoGbXVn7c2n3zyCaKioqBWqxEfH49Dhw7VeP+VK1eic+fOcHd3R2RkJJ599llUVFTYvO+///1vcByHZ555ps7jaulM9bsycBzXxKMhhBBCCGkaDmV477rrLjz44IPo2LEj8vLyMHHiRADA8ePH0aFDhzpta+PGjViwYAFWr16N+Ph4rFy5EhMmTMDFixcREhJidf9vvvkGCxcuxLp16zBkyBBcunQJs2bNAsdxWLFihdl9Dx8+jDVr1qBXr16OvMwWT6sTTj6onIEQQgghrsyhSOj999/H/Pnz0a1bNyQmJsLLywsAkJGRgblz59ZpWytWrMCcOXMwe/ZsdOvWDatXr4aHhwfWrVtn8/779u3D0KFD8eCDDyIqKgrjx4/HAw88YJUVLi0txbRp0/DZZ5/B39/fkZfZ4okZXmpJRgghhBAX5lCGV6FQ4Pnnn7e6/dlnn63TdiorK3H06FEsWrRIvE0mk2Hs2LHYv3+/zccMGTIEX331FQ4dOoS4uDgkJydj27ZtmD59utn95s2bh9tuuw1jx47FG2+8UeM4tFottFqt+O/i4mIAgE6ng06nq9Nrqgu27YZ6jrKKSgCASs416OtwdQ29H0nDo33Y8tE+bPloH7Z8jb0P6/I89Vpv9ty5c0hNTUVlZaXZ7XfccYddj8/NzYVer0doaKjZ7aGhobhw4YLNxzz44IPIzc3FsGHDwPM8qqqq8MQTT+Dll18W7/Pdd9/h2LFjOHz4sF3jWL58OV577TWr23fu3AkPDw+7tlEfiYmJDbLdK8UA4Aadthzbtm1rkOcgJg21H0njoX3Y8tE+bPloH7Z8jbUPNRqN3fd1KOBNTk7GlClTcPr0aXAcB97Y7JVNjHJk4pq9du3ahbfeeguffvop4uPjceXKFTz99NP417/+hSVLluDGjRt4+umnkZiYCLVabdc2Fy1ahAULFoj/Li4uRmRkJMaPHw8fH5+GeinQ6XRITEzEuHHjoFAonL79v6/kAmePIcDXBwkJg52+fSJo6P1IGh7tw5aP9mHLR/uw5WvsfciuyNvDoYD36aefRnR0NJKSkhAdHY1Dhw4hLy8Pzz33HN599127txMUFAS5XI6srCyz27OyshAWFmbzMUuWLMH06dPx6KOPAgB69uyJsrIyPPbYY3jllVdw9OhRZGdno1+/fuJj9Ho99uzZg48//hharRZyuXlNq0qlgkqlsnouhULRKDusoZ5Hzwsl2mqlnH48GkFjfV5Iw6F92PLRPmz5aB+2fI0ZP9nLoUlr+/fvx+uvv46goCDIZDLIZDIMGzYMy5cvx1NPPWX3dpRKJfr374+kpCTxNoPBgKSkJAwebDsjqdFoIJOZD5sFsDzPY8yYMTh9+jROnDgh/m/AgAGYNm0aTpw4YRXstmZs4Qnq0kAIIYQQV+ZQhlev18Pb2xuAkKVNT09H586d0b59e1y8eLFO21qwYAFmzpyJAQMGIC4uDitXrkRZWRlmz54NAJgxYwYiIiKwfPlyAMCkSZOwYsUK9O3bVyxpWLJkCSZNmgS5XA5vb2/06NHD7Dk8PT0RGBhodXtrp9WxPryuE+QTQgghhFhyKODt0aMHTp48iejoaMTHx+Odd96BUqnE2rVrERMTU6dtTZ06FTk5OVi6dCkyMzPRp08f7NixQ5zIlpqaapbRXbx4MTiOw+LFi5GWlobg4GBMmjQJb775piMvpVWTLjxBCCGEEOKq7A54T506hR49ekAmk2Hx4sXizLjXX38dt99+O4YPH47AwEBs3LixzoOYP38+5s+fb/Nvu3btMh+wmxuWLVuGZcuW2b19y224CrGkgfrwEkIIIcSF2R3w9u3bFxkZGQgJCcGTTz4ptvzq0KEDLly4gPz8fPj7+9MSts0IZXgJIYQQQuowac3Pzw/Xrl0DAKSkpMBgMJj9PSAggILdZsZUw0sBLyGEEEJcl90Z3rvvvhsjR45EmzZtwHEcBgwYUG3Hg+TkZKcNkDjO1KWBShoIIYQQ4rrsDnjXrl2Lu+66C1euXMFTTz2FOXPmiJ0aSPMkljQoKMNLCCGEENdVpy4Nt956KwDg6NGjePrppyngbeaoDy8hhBBCiINtydavX+/scZAGQH14CSGEEEIcXGmNtAzUpYEQQgghhALeVs3Uh5d2MyGEEEJcF0VCrZgpw0slDYQQQghxXRTwtmLUh5cQQgghhALeVo26NBBCCCGEUMDbLBRqdPj2UCpS8zRO3a6pDy+VNBBCCCHEdVHA2wy8tOUMFm05jR+O3XTqdqlLAyGEEEIIBbzNQkKPUADALyfTwfO807ZLJQ2EEEIIIRTwNgtjuoZA5SZDcm4ZzqQVO227tPAEIYQQQggFvM2Cl8oNY7sJWd6tJ9Octl1TDS/tZkIIIYS4LoqEmok7eocDAH45mYH0wnJM+fQfTPvvgXqVOFBJAyGEEEII4NbUAyCCUZ2D4a12Q2ZxBRI+/BuFGh0AoLi8Cr4eijpvj+d5WniCEEIIIQSU4W02VG5yTOwRBgBisAsAGl2VQ9vT6Xmw5DCVNBBCCCHElVEk1IxMHRgJAOgQ4gUPpZCVLdPqHdpWRZXpcVTSQAghhBBXRpFQM9K/fQD+en4Ufv2/YfBzF8oYNJWOZXhZhwYAUMppNxNCCCHEdVENbzMTHeQJAPBQCbvG0QyvdMIax3HOGRwhhBBCSAtEqb9mytNY0uBwhpdWWSOEEEIIAUABb7PloTRmeCsdzPCyRScU1KGBEEIIIa6NAt5mylNlzPBqHc3wUg9eQgghhBCAAt5mq94ZXippIIQQQggBQAFvs1X/DC8tOkEIIYQQAlDA22y5K+pbw2ssaaBFJwghhBDi4igaaqbEDC91aSCEEEIIqReKhpopsYbX4T68VNJACCGEEAJQwNts1T/DS10aCCGEEEIACnibLZbh1VAfXkIIIYSQeqGAt5mildYIIYQQQpyjWURDn3zyCaKioqBWqxEfH49Dhw7VeP+VK1eic+fOcHd3R2RkJJ599llUVFSIf1++fDkGDhwIb29vhISE4M4778TFixcb+mU4lYeqvjW8VNJACCGEEAI0g4B348aNWLBgAZYtW4Zjx46hd+/emDBhArKzs23e/5tvvsHChQuxbNkynD9/Hv/73/+wceNGvPzyy+J9du/ejXnz5uHAgQNITEyETqfD+PHj/7+9Ow+L6rr7AP6dGZgBhGGQdVQUEhUXcFeK+xtR1EBikiZULVETtVZJtdT4aoNimtdg08qjsS7NojSmLTFNjalRIgXRBA0aIsYtxgWDMSwSZRNkBua8f+BcHUHZmbn4/TwPT5k759577vwk/XI491zcvHmzvS6rxVpvhJdTGoiIiOjhZmftDiQkJGDevHmYM2cOAGDr1q349NNPsW3bNixfvrxO+8OHD2PUqFGYMWMGAMDPzw/Tp09HZmam1CY5Odlin8TERHh5eSErKwtjx45tw6tpPS1+0po0h9fqv9MQERERWZVVA6/BYEBWVhZWrFghbVMqlQgNDcWRI0fq3WfkyJF4//33cfToUYwYMQKXLl3C3r17ERUVdd/zlJSUAAA6d+5c7/tVVVWoqqqSXpeWlgIAjEYjjEZjk6+rsczHru8capUAUPukteb0odJQu4+9ov7jU+t5UB1JHlhD+WMN5Y81lL/2rmFTzmPVwFtUVISamhp4e3tbbPf29sa3335b7z4zZsxAUVERRo8eDSEEqqursWDBAospDXczmUxYsmQJRo0ahcDAwHrbxMfH49VXX62zff/+/XBycmriVTVdSkpKnW2lBgCwQ4WhGns+3QulomnHvHhZCUCJnIvfYe8tec1flqv66kjywhrKH2sof6yh/LVXDSsqKhrd1upTGpoqPT0dr7/+OjZv3ozg4GBcuHABixcvxmuvvYaVK1fWab9o0SKcOnUKX3zxxX2PuWLFCsTExEivS0tL4evri0mTJkGr1bbJdQC1v5mkpKRg4sSJsLe3t3ivwlCNlVlpEFDgsYmTpCkOjbV/5zfAtXwMCOyHqSE9WrPbdI8H1ZHkgTWUP9ZQ/lhD+WvvGpr/It8YVg28Hh4eUKlUKCgosNheUFAAHx+fevdZuXIloqKiMHfuXABAUFAQbt68ifnz5+OVV16BUnlnzmp0dDT27NmDQ4cOoVu3bvfth0ajgUajqbPd3t6+XQpW33lcVHZQKAAhAINJCdcm9sNYUzslwknTPtdA7ffvhdoOayh/rKH8sYby1575qbGsekeTWq3G0KFDkZqaKm0zmUxITU1FSEhIvftUVFRYhFoAUKlqVyIQQkj/Gx0djV27diEtLQ3+/v5tdAVtR6lUwMm++Ss1cJUGIiIiolpWn9IQExODWbNmYdiwYRgxYgTWr1+PmzdvSqs2PP/88+jatSvi4+MBABEREUhISMDgwYOlKQ0rV65ERESEFHwXLVqEf/zjH9i9ezdcXFyQn58PAHB1dYWjo6N1LrQZnDR2uGmoadZavFyHl4iIiKiW1QNvZGQkrl27hlWrViE/Px+DBg1CcnKydCNbbm6uxYhubGwsFAoFYmNjcfXqVXh6eiIiIgJr1qyR2mzZsgUAMH78eItzbd++HbNnz27za2otndQqXENLR3gZeImIiOjhZvXAC9TOtY2Ojq73vfT0dIvXdnZ2iIuLQ1xc3H2PZ57aIHeOLViL9846vJzSQERERA83Dv/ZMPPT1iqbNcLLKQ1EREREAAOvTXPS3B7hbdYcXk5pICIiIgIYeG2aeYSXqzQQERERNR8Drw1zatEc3ttTGuxZYiIiInq4MQ3ZsE6a2yO8VVylgYiIiKi5mIZsWHNHeIUQnNJAREREdBsDrw1r7hxeQ41J+p5TGoiIiOhhxzRkw5q7SoN5dBfglAYiIiIipiEb1twRXvNDJwBArWKJiYiI6OHGNGTDmj/Ce+ehEwqFotX7RURERCQnDLw27N4R3lvGxgVfrtBAREREdAcTkQ27e5WGPd/8iH6rkrHz2JUG9zNPadDYc4UGIiIiIgZeG3b3Orx7TuTBJIDDF4sa3O9GhQEA4Opo36b9IyIiIpIDBl4bdvcI71ff3wAA5JXcanC//NttfLQObdc5IiIiIplg4LVhTrfn8JZUGlFUXgUAyC+tG3g/OfEjnt16GAW33zO38WbgJSIiImLgtWWdbo/w3i2v5BaEEBbbko7m4tjlG9h/Oh8ApODr46pp+04SERER2TgGXhvmqK5705mh2oTrNw0W28pu1a7i8ENxJYC7Ai9HeImIiIgYeG2Z2k5Z74Mj7p3HW15VG3iv3qgNvPmltdMfvBh4iYiIiBh4bZ2T5s4or29nRwB3bkozk0Z4bwfeAt60RkRERCRh4LVx5nm8Pb2c0ddHCwDIK713hNcIALhaXIkak8C12ze4+bgy8BIRERHVvSuKbIp5pYZhPdykJ6fll1RK7xtrTLh1+0ET18qqcPVGbehVKRXwcOZNa0REREQMvDZOe/vhEcP8OuNaWe3IbV7xnRHe8tvTGcy+zq1dr9fTWQOVUtFOvSQiIiKyXZzSYOOWhPZC1M96IHyAHvrbUxTuvmnNfMOaWdbtB1R4azm6S0RERARwhNfmjenliTG9PAFACrx3P3yi7Nb9Ai/n7xIREREBHOGVFb1r7SoNeSWV0sMn7h3h/Ta/FABvWCMiIiIyY+CVEa/b0xRuGU0oqaxdmcG8QoOZ6fZD2DjCS0RERFSLgVdGHOxVcO+kBgD8ePvGtXunNJhxDV4iIiKiWgy8MqPXmefx1i5NZp7ScO9NahzhJSIiIqrFwCszPlrzPN7aEV7zsmR9bj+UQmrnylUaiIiIiAAGXtmRVmoosZzS0MPdCQ72d8rJEV4iIiKiWgy8MuNzz1q85ikNLg526KqrHf3tpFbBxcHeOh0kIiIisjEMvDJz5+ETtXN4zSO8zhp7dHVzAgB4c0kyIiIiIgkDr8zUHeGtXZbMxcEO3dxqR3i9XRh4iYiIiMwYeGXG63aYvVZWBcBySoO/eycAgG9nR+t0joiIiMgG2UTg3bRpE/z8/ODg4IDg4GAcPXr0ge3Xr1+PgIAAODo6wtfXF7/97W9x69YtizZNPaZcmB8+UXarGreMNdIqDc4aOzw33BdLJ/XGS4/1smYXiYiIiGyK1QPvBx98gJiYGMTFxeHrr7/GwIEDERYWhsLCwnrb/+Mf/8Dy5csRFxeHs2fP4t1338UHH3yA3//+980+ppy4aOygsast27WyKpRV3Qm8ro72iH6sF3w7O1mzi0REREQ2xc7aHUhISMC8efMwZ84cAMDWrVvx6aefYtu2bVi+fHmd9ocPH8aoUaMwY8YMAICfnx+mT5+OzMzMZh+zqqoKVVVV0uvS0lIAgNFohNForNO+tZiP3dRzeDqr8UPxLeTduCmN8DrYNf041DqaW0eyHayh/LGG8scayl9717Ap57Fq4DUYDMjKysKKFSukbUqlEqGhoThy5Ei9+4wcORLvv/8+jh49ihEjRuDSpUvYu3cvoqKimn3M+Ph4vPrqq3W279+/H05ObT9ampKS0qT2dtUqAAokHzyC4ptKAAp8deQLXOa9albV1DqS7WEN5Y81lD/WUP7aq4YVFRWNbmvVwFtUVISamhp4e3tbbPf29sa3335b7z4zZsxAUVERRo8eDSEEqqursWDBAmlKQ3OOuWLFCsTExEivS0tL4evri0mTJkGr1da7T2swGo1ISUnBxIkTYW/f+HVz9xRn4/LZQnTp2Q+G784BAMLDQtG5k7qtukoP0Nw6ku1gDeWPNZQ/1lD+2ruG5r/IN4bVpzQ0VXp6Ol5//XVs3rwZwcHBuHDhAhYvXozXXnsNK1eubNYxNRoNNJq6j+K1t7dvl4I19TzmdXZzb9y5Uc/N2RH2dlafkv1Qa69/L9R2WEP5Yw3ljzWUv/bMT41l1cDr4eEBlUqFgoICi+0FBQXw8fGpd5+VK1ciKioKc+fOBQAEBQXh5s2bmD9/Pl555ZVmHVNuzEuT5RTdBABo7JRQM+wSERER1cuqgVetVmPo0KFITU3FtGnTAAAmkwmpqamIjo6ud5+KigoolZbhTqVSAQCEEM06ptx4utSORl+6Vht4XRxkN1BPRPRQMk/Fq6mpsXZXbI7RaISdnR1u3brFz0em2qKG9vb2Us5rCasnpZiYGMyaNQvDhg3DiBEjsH79ety8eVNaYeH5559H165dER8fDwCIiIhAQkICBg8eLE1pWLlyJSIiIqQPpKFjyp3X7cD74+3HCztrrF5GIiJqgMFgQF5eXpNutHmYCCHg4+ODK1euQKFQWLs71AxtUUOFQoFu3brB2dm5RcexelKKjIzEtWvXsGrVKuTn52PQoEFITk6WbjrLzc21GNGNjY2FQqFAbGwsrl69Ck9PT0RERGDNmjWNPqbcmUd4hah97cwRXiIim2YymZCTkwOVSoUuXbpArVYz1N3DZDKhvLwczs7Odf6SS/LQ2jUUQuDatWv44Ycf0KtXrxaN9NpEUoqOjr7vdIP09HSL13Z2doiLi0NcXFyzjyl35sBrxhFeIiLbZjAYYDKZ4Ovr2y7LXcqRyWSCwWCAg4MDA69MtUUNPT09cfnyZRiNxhYFXv6LkiEP53sDL+9mJSKSAwY5oqZprb+E8CdPhuxVSos1d3nTGhEREdH9MfDKlOddo7yc0kBERER0fwy8MuWlvRN4OcJLRERtRQiB+fPno3PnzlAoFMjOzrZ2l2zW7NmzpSVRybYw8MqUxQgvAy8REbWR5ORkJCYmYs+ePcjLy0NgYGCLjqdQKPDxxx+3Tufayf/8z//gnXfesXY3qAWYlGTq7pUaXDilgYiI2sjFixeh1+sxcuRIa3cFQO2KF2q1uuGGreT69evIyMhAUlJSu53TFhmNRlk/8pkjvDJ1d+DlCC8RkfwIIVBhqLbKlzAv5N6A2bNn46WXXkJubi4UCgX8/PyQnJyM0aNHQ6fTwd3dHeHh4bh48aK0j8FgQHR0NPR6PRwcHNCjRw/p4VF+fn4AgKeeeko6XkNWr16NQYMG4Z133oG/vz8cHBwAAMXFxfjVr34Fb29vODg4IDAwEHv27AEAJCYmQqfT4bPPPkPfvn3h7OyMyZMnIy8vz+Lapk2bhj//+c/Q6/Vwd3fHokWLYDQaLc7/6aefYsiQIdJa/qdPn0Z4eDi0Wi1cXFwwZswYi+u/27/+9S8EBQXB0dER7u7uCA0Nxc2bNxu85mPHjmHixInw8PCAq6srxo0bh6+//tqizYOuHwAyMjIwfvx4ODk5wc3NDWFhYbhx4waA2jqsX7/e4niDBg3C6tWrpdcKhQJbtmzBE088gU6dOmHNmjWoqanBiy++CH9/fzg6OiIgIAAbNmyo0/9t27ahf//+0Gg00Ov10jKxL7zwAsLDwy3aGo1GeHl54d13323wc2kJJiWZsgi8XJaMiEh2Ko016LfqM6uc+8wfwuCkbjgCbNiwAY8++ijeeustHDt2DCqVCocOHUJMTAwGDBiA8vJyrFq1Ck899RSys7OhVCrx5ptv4pNPPsHOnTvRvXt3XLlyBVeuXAFQG+S8vLywfft2TJ48udHrql64cAEfffQR/v3vf0OlUsFkMmHKlCkoKyvD+++/j0cffRRnzpyxOF5FRQX+/Oc/Y8eOHVAqlfjlL3+JpUuX4u9//7vU5sCBA9Dr9Thw4AAuXLiAyMhIDBo0CPPmzZPafPLJJ3jyyScBAFevXsXYsWMxfvx4pKWlQavVIiMjA9XV1XX6nJeXh+nTp+ONN97AU089hbKyMnz++eeN+mWjrKwMs2bNwsaNGyGEwLp16zB16lScP38eLi4uDV5/dnY2JkyYgBdeeAEbNmyAnZ0dDhw40OTH/a5evRpr167F+vXrYWdnB5PJhG7duuHDDz+Eu7s7Dh8+jPnz50Ov1+O5554DALz77ruIjY3F2rVrMWXKFJSUlCAjIwMAMHfuXIwdOxZ5eXnQ6/UAgD179qCiogKRkZFN6ltTMfDKlJeLg/Q9V2kgIqK24OrqChcXF6hUKvj4+AAAnnnmGYs227Ztg6enJ86cOYPAwEDk5uaiV69eGD16NBQKBXr06CG19fT0BADodDrpeI1hMBjw3nvvSfvv378fR48exdmzZ9G7d28AwCOPPGKxj9FoxNatW/Hoo48CqH0g1R/+8AeLNm5ubvjLX/4ClUqFPn364PHHH0dqaqoUeKuqqpCcnCyNfG7atAmurq5ISkqS/rxvPv+98vLyUF1djaefflr6DIKCghp1vY899pjF67feegs6nQ4HDx5EeHg4/vvf/z7w+t944w0MGzYMmzdvlrb179+/Uee+24wZMzBnzhyLba+++qr0vb+/P44cOYKdO3dKgXfdunWIiYnB4sWLpXbDhw8HAIwcORIBAQHYsWMHli1bBgDYvn07nn322RY/OrghTEoyZTGHl1MaiIhkx9FehTN/CLPauZvr/PnzWLVqFTIzM1FUVASTyQQAyM3NRWBgIGbPno2JEyciICAAkydPRnh4OCZNmtSi/vbo0UMKu0DtCGa3bt3uGzYBwMnJSQq7AKDX61FYWGjRpn///hajwnq9HidPnpRep6WlwcvLSwqL2dnZGDNmTKPmsg4cOBATJkxAUFAQwsLCMGnSJPz85z+Hm5tbg/sWFBQgNjYW6enpKCwsRE1NDSoqKpCbm9uo68/Ozsazzz7b4HkaMmzYsDrbNm3ahG3btiE3NxeVlZUwGAwYNGgQAKCwsBB5eXl1Avvd5s6di7feegvLli1DQUEB9u3bh7S0tBb3tSGcwytTllMaGHiJiORGoVDASW1nla+WPL0qIiIC169fx9tvv43MzExkZmYCqB2FBYAhQ4YgJycHr732GiorK/Hcc8/h5z//eYs+q06dOlm8dnR0bHCfe0OpQqGoM52gvjbmAA/UTmd44oknmnReM5VKhZSUFOzbtw/9+vXDxo0bERAQgJycnAb3nTVrFrKzs7FhwwYcPnwY2dnZcHd3lz7jhvrR0PtKpbLOZ3Hv3GWg7ueelJSEpUuX4sUXX8T+/fuRnZ2NOXPmNLpfAPD888/j0qVLOHLkCN5//334+/tjzJgxDe7XUgy8MqV1sEM3N0e4aOws1uQlIiJqKz/99BPOnTuH2NhYTJgwAX379pVuhLqbVqtFZGQk3n77bXzwwQf46KOPcP36dQC1IbOpc0nvNWDAAPzwww/47rvvWnScBxFC4D//+Y80f9d83s8//7zecFgfhUKBUaNG4dVXX8Xx48ehVquxa9euBvfLyMjAb37zG0ydOlW6+auoqMiiHw+6/gEDBiA1NfW+x/f09LS4ga+0tLRRQTwjIwMjR47EwoULMXjwYPTs2dPihj0XFxd07979gSO27u7umDZtGrZv347ExMQ6UybaCocGZUqhUOCT6NEwVJsadeMBERFRS7m5ucHd3R1vvfUW9Ho9cnNzsXz5cos2CQkJ0Ov1GDx4MJRKJT788EP4+PhAp9MBqF0hIDU1FaNGjYJGo2nUn/jvNW7cOIwdOxbPPPMMEhIS0LNnT3z77bdQKBSYPHlya1wqsrKyUFFRgdGjR0vboqOjsXHjRvziF7/AihUr4Orqii+//BIjRoxAQECAxf6ZmZlITU3FpEmT4OXlhczMTFy7dg19+/Zt8Ny9evXCjh07MGzYMJSWluLll1+2GD1t6PpXrFiBoKAgLFy4EAsWLIBarcaBAwfw7LPPwsPDA4899hgSExMREREBnU6HVatWNeoGwl69euG9997DZ599Bn9/f+zYsQPHjh2Dv7+/1Gb58uWIiYmBt7e3dGNdRkYGXnrpJanN3LlzER4ejpqaGsyaNavB87YGjvDKWOdOavi4OjTckIiIqBUolUokJSUhKysLgYGB+O1vf4s//elPFm1cXFykm6aGDx+Oy5cvY+/evVAqayPHunXrkJKSAl9fXwwePLjZffnoo48wfPhwTJ8+Hf369cOyZctaPHJ8t927d2Pq1Kmws7szqOTu7o60tDSUl5dj3LhxGDp0KN5+++165/RqtVocOnQIU6dORe/evREbG4t169ZhypQpDZ773XffxY0bNzBkyBBERUXhN7/5Dby8vCzaPOj6e/fujf379+PEiRMYMWIEQkJCsHv3bulaVqxYgXHjxiE8PByPP/44pk2bZjHf+X5+9atf4emnn0ZkZCSCg4Px008/YeHChRZtpk+fjoSEBGzevBn9+/dHeHg4zp8/b9EmNDQUer0eYWFh6NKlS4PnbQ0K0djF+B4ipaWlcHV1RUlJCbRabZudx2g0Yu/evZg6daqsF3N+2LGO8scayp+t1/DWrVvIycmxWEeWLJlMJpSWlkKr1Urh2JoGDBiA2NhYafUBalhja1heXo6uXbti+/btePrppx94zAf97DQlr/Fv4URERER3MRgMeOaZZxo1GkuNZzKZUFRUhHXr1kGn01ncENjWGHiJiIjIavr374/vv/++3vf++te/YubMme3cI0CtViMuLq7Njv+gNWf37dvXLqsWWENubi78/f3RrVs3JCYmWkwXaWsMvERERGQ1e/fuhdFohMlkQnl5OZydnaU/h5sf59vRZGdn3/e9rl27tl9H2pmfn1+jH2vd2hh4iYiIyGrMTyGztTm8balnz57W7sJDp2P/iyIiIrIhvE+cqGla62eGgZeIiKiNmVeOqKiosHJPiOTF/BS3xqwT/CCc0kBERNTGVCoVdDodCgsLAQBOTk4terxvR2QymWAwGHDr1q0OP6Who2rtGppMJly7dg1OTk4tvsGNgZeIiKgd+Pj4AIAUesmSEAKVlZVwdHTkLwMy1RY1VCqV6N69e4uPx8BLRETUDhQKBfR6Pby8vGA0Gq3dHZtjNBpx6NAhjB071iYfHkINa4saqtXqVhktZuAlIiJqRyqVqsXzETsilUqF6upqODg4MPDKlC3XkJNkiIiIiKhDY+AlIiIiog6NgZeIiIiIOjTO4a2HeZHj0tLSNj2P0WhERUUFSktLbW6uCzUe6yh/rKH8sYbyxxrKX3vX0JzTGvNwCgbeepSVlQEAfH19rdwTIiIiInqQsrIyuLq6PrCNQvA5h3WYTCb8+OOPcHFxadO1AEtLS+Hr64srV65Aq9W22XmobbGO8scayh9rKH+sofy1dw2FECgrK0OXLl0aXLqMI7z1UCqV6NatW7udT6vV8oe7A2Ad5Y81lD/WUP5YQ/lrzxo2NLJrxpvWiIiIiKhDY+AlIiIiog6NgdeKNBoN4uLioNForN0VagHWUf5YQ/ljDeWPNZQ/W64hb1ojIiIiog6NI7xERERE1KEx8BIRERFRh8bAS0REREQdGgMvEREREXVoDLxWtGnTJvj5+cHBwQHBwcE4evSotbv00Dp06BAiIiLQpUsXKBQKfPzxxxbvCyGwatUq6PV6ODo6IjQ0FOfPn7doc/36dcycORNarRY6nQ4vvvgiysvLLdp88803GDNmDBwcHODr64s33nijrS/toRAfH4/hw4fDxcUFXl5emDZtGs6dO2fR5tatW1i0aBHc3d3h7OyMZ555BgUFBRZtcnNz8fjjj8PJyQleXl54+eWXUV1dbdEmPT0dQ4YMgUajQc+ePZGYmNjWl/dQ2LJlCwYMGCAtWB8SEoJ9+/ZJ77N+8rN27VooFAosWbJE2sY62r7Vq1dDoVBYfPXp00d6X7Y1FGQVSUlJQq1Wi23btonTp0+LefPmCZ1OJwoKCqzdtYfS3r17xSuvvCL+/e9/CwBi165dFu+vXbtWuLq6io8//licOHFCPPHEE8Lf319UVlZKbSZPniwGDhwovvzyS/H555+Lnj17iunTp0vvl5SUCG9vbzFz5kxx6tQp8c9//lM4OjqKv/71r+11mR1WWFiY2L59uzh16pTIzs4WU6dOFd27dxfl5eVSmwULFghfX1+RmpoqvvrqK/Gzn/1MjBw5Unq/urpaBAYGitDQUHH8+HGxd+9e4eHhIVasWCG1uXTpknBychIxMTHizJkzYuPGjUKlUonk5OR2vd6O6JNPPhGffvqp+O6778S5c+fE73//e2Fvby9OnTolhGD95Obo0aPCz89PDBgwQCxevFjazjravri4ONG/f3+Rl5cnfV27dk16X641ZOC1khEjRohFixZJr2tqakSXLl1EfHy8FXtFQog6gddkMgkfHx/xpz/9SdpWXFwsNBqN+Oc//ymEEOLMmTMCgDh27JjUZt++fUKhUIirV68KIYTYvHmzcHNzE1VVVVKb//3f/xUBAQFtfEUPn8LCQgFAHDx4UAhRWy97e3vx4YcfSm3Onj0rAIgjR44IIWp/6VEqlSI/P19qs2XLFqHVaqWaLVu2TPTv39/iXJGRkSIsLKytL+mh5ObmJt555x3WT2bKyspEr169REpKihg3bpwUeFlHeYiLixMDBw6s9z0515BTGqzAYDAgKysLoaGh0jalUonQ0FAcOXLEij2j+uTk5CA/P9+iXq6urggODpbqdeTIEeh0OgwbNkxqExoaCqVSiczMTKnN2LFjoVarpTZhYWE4d+4cbty40U5X83AoKSkBAHTu3BkAkJWVBaPRaFHDPn36oHv37hY1DAoKgre3t9QmLCwMpaWlOH36tNTm7mOY2/DntnXV1NQgKSkJN2/eREhICOsnM4sWLcLjjz9e57NmHeXj/Pnz6NKlCx555BHMnDkTubm5AORdQwZeKygqKkJNTY3FPwYA8Pb2Rn5+vpV6RfdjrsmD6pWfnw8vLy+L9+3s7NC5c2eLNvUd4+5zUMuZTCYsWbIEo0aNQmBgIIDaz1etVkOn01m0vbeGDdXnfm1KS0tRWVnZFpfzUDl58iScnZ2h0WiwYMEC7Nq1C/369WP9ZCQpKQlff/014uPj67zHOspDcHAwEhMTkZycjC1btiAnJwdjxoxBWVmZrGto1yZHJSKykkWLFuHUqVP44osvrN0VaqKAgABkZ2ejpKQE//rXvzBr1iwcPHjQ2t2iRrpy5QoWL16MlJQUODg4WLs71ExTpkyRvh8wYACCg4PRo0cP7Ny5E46OjlbsWctwhNcKPDw8oFKp6tzVWFBQAB8fHyv1iu7HXJMH1cvHxweFhYUW71dXV+P69esWbeo7xt3noJaJjo7Gnj17cODAAXTr1k3a7uPjA4PBgOLiYov299awofrcr41Wq5X1/xHYCrVajZ49e2Lo0KGIj4/HwIEDsWHDBtZPJrKyslBYWIghQ4bAzs4OdnZ2OHjwIN58803Y2dnB29ubdZQhnU6H3r1748KFC7L+WWTgtQK1Wo2hQ4ciNTVV2mYymZCamoqQkBAr9ozq4+/vDx8fH4t6lZaWIjMzU6pXSEgIiouLkZWVJbVJS0uDyWRCcHCw1ObQoUMwGo1Sm5SUFAQEBMDNza2drqZjEkIgOjoau3btQlpaGvz9/S3eHzp0KOzt7S1qeO7cOeTm5lrU8OTJkxa/uKSkpECr1aJfv35Sm7uPYW7Dn9u2YTKZUFVVxfrJxIQJE3Dy5ElkZ2dLX8OGDcPMmTOl71lH+SkvL8fFixeh1+vl/bPYZrfD0QMlJSUJjUYjEhMTxZkzZ8T8+fOFTqezuKuR2k9ZWZk4fvy4OH78uAAgEhISxPHjx8X3338vhKhdlkyn04ndu3eLb775Rjz55JP1Lks2ePBgkZmZKb744gvRq1cvi2XJiouLhbe3t4iKihKnTp0SSUlJwsnJicuStYJf//rXwtXVVaSnp1sspVNRUSG1WbBggejevbtIS0sTX331lQgJCREhISHS++aldCZNmiSys7NFcnKy8PT0rHcpnZdfflmcPXtWbNq0icshtZLly5eLgwcPipycHPHNN9+I5cuXC4VCIfbv3y+EYP3k6u5VGoRgHeXgd7/7nUhPTxc5OTkiIyNDhIaGCg8PD1FYWCiEkG8NGXitaOPGjaJ79+5CrVaLESNGiC+//NLaXXpoHThwQACo8zVr1iwhRO3SZCtXrhTe3t5Co9GICRMmiHPnzlkc46effhLTp08Xzs7OQqvVijlz5oiysjKLNidOnBCjR48WGo1GdO3aVaxdu7a9LrFDq692AMT27dulNpWVlWLhwoXCzc1NODk5iaeeekrk5eVZHOfy5ctiypQpwtHRUXh4eIjf/e53wmg0WrQ5cOCAGDRokFCr1eKRRx6xOAc13wsvvCB69Ogh1Gq18PT0FBMmTJDCrhCsn1zdG3hZR9sXGRkp9Hq9UKvVomvXriIyMlJcuHBBel+uNVQIIUTbjR8TEREREVkX5/ASERERUYfGwEtEREREHRoDLxERERF1aAy8RERERNShMfASERERUYfGwEtEREREHRoDLxERERF1aAy8RERERNShMfASEZHEz88P69evt3Y3iIhaFQMvEZGVzJ49G9OmTQMAjB8/HkuWLGm3cycmJkKn09XZfuzYMcyfP7/d+kFE1B7srN0BIiJqPQaDAWq1utn7e3p6tmJviIhsA0d4iYisbPbs2Th48CA2bNgAhUIBhUKBy5cvAwBOnTqFKVOmwNnZGd7e3oiKikJRUZG07/jx4xEdHY0lS5bAw8MDYWFhAICEhAQEBQWhU6dO8PX1xcKFC1FeXg4ASE9Px5w5c1BSUiKdb/Xq1QDqTmnIzc3Fk08+CWdnZ2i1Wjz33HMoKCiQ3l+9ejUGDRqEHTt2wM/PD66urvjFL36BsrKytv3QiIiagIGXiMjKNmzYgJCQEMybNw95eXnIy8uDr68viouL8dhjj2Hw4MH46quvkJycjIKCAjz33HMW+//tb3+DWq1GRkYGtm7dCgBQKpV48803cfr0afztb39DWloali1bBgAYOXIk1q9fD61WK51v6dKldfplMpnw5JNP4vr16zh48CBSUlJw6dIlREZGWrS7ePEiPv74Y+zZswd79uzBwYMHsXbt2jb6tIiImo5TGoiIrMzV1RVqtRpOTk7w8fGRtv/lL3/B4MGD8frrr0vbtm3bBl9fX3z33Xfo3bs3AKBXr1544403LI5593xgPz8//N///R8WLFiAzZs3Q61Ww9XVFQqFwuJ890pNTcXJkyeRk5MDX19fAMB7772H/v3749ixYxg+fDiA2mCcmJgIFxcXAEBUVBRSU1OxZs2aln0wRESthCO8REQ26sSJEzhw4ACcnZ2lrz59+gCoHVU1Gzp0aJ19//vf/2LChAno2rUrXFxcEBUVhZ9++gkVFRWNPv/Zs2fh6+srhV0A6NevH3Q6Hc6ePStt8/Pzk8IuAOj1ehQWFjbpWomI2hJHeImIbFR5eTkiIiLwxz/+sc57er1e+r5Tp04W712+fBnh4eH49a9/jTVr1qBz58744osv8OKLL8JgMMDJyalV+2lvb2/xWqFQwGQyteo5iIhagoGXiMgGqNVq1NTUWGwbMmQIPvroI/j5+cHOrvH/uc7KyoLJZMK6deugVNb+IW/nzp0Nnu9effv2xZUrV3DlyhVplPfMmTMoLi5Gv379Gt0fIiJr45QGIiIb4Ofnh8zMTFy+fBlFRUUwmUxYtGgRrl+/junTp+PYsWO4ePEiPvvsM8yZM+eBYbVnz54wGo3YuHEjLl26hB07dkg3s919vvLycqSmpqKoqKjeqQ6hoaEICgrCzJkz8fXXX+Po0aN4/vnnMW7cOAwbNqzVPwMiorbCwEtEZAOWLl0KlUqFfv36wdPTE7m5uejSpQsyMjJQU1ODSZMmISgoCEuWLIFOp5NGbuszcOBAJCQk4I9//CMCAwPx97//HfHx8RZtRo4ciQULFiAyMhKenp51bnoDaqcm7N69G25ubhg7dixCQ0PxyCOP4IMPPmj16yciaksKIYSwdieIiIiIiNoKR3iJiIiIqENj4CUiIiKiDo2Bl4iIiIg6NAZeIiIiIurQGHiJiIiIqENj4CUiIiKiDo2Bl4iIiIg6NAZeIiIiIurQGHiJiIiIqENj4CUiIiKiDo2Bl4iIiIg6tP8HJl3x/y7pfk8AAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: graphs/fast_rcnn_cls_accuracy_over_iterations.png\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAArMAAAHWCAYAAABkNgFvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/OQEPoAAAACXBIWXMAAA9hAAAPYQGoP6dpAACgIklEQVR4nOzdd3hTZRsG8DtJ06R70F1KW8reew+RvRREQEBBVAQVB4gDURBQ+EQZiigKKg5UQFFEEClLpoDsDaUto9BN90qT8/2RnNOkg6YhbUlz/67ruz45PUne5GQ85znP+7wyQRAEEBERERHZIHl1D4CIiIiIyFIMZomIiIjIZjGYJSIiIiKbxWCWiIiIiGwWg1kiIiIislkMZomIiIjIZjGYJSIiIiKbxWCWiIiIiGwWg1kiIiIislkMZonu0dGjR9GlSxe4uLhAJpPh5MmT1T0km7Rt2za0atUKarUaMpkMaWlpZt/23XffhUwmq7zB1XBPPvkkwsLCqnsYZEWxsbGQyWRYs2ZNdQ+FqNIxmCW6BxqNBiNHjkRqaiqWLl2K77//HqGhoVZ9jB9//BHLli2z6n3eD0aMGIFBgwYBAFJSUjBq1Cg4OTlhxYoV+P777+Hi4lLNI6xZbt26hXfffZcnWxbYs2cPZDIZfvnlF2nbwYMH8e6771bopKsy1NTvB6KKcKjuARDZsqtXr+LatWtYtWoVnnnmmUp5jB9//BFnz57FK6+8Uin3Xx00Gg0iIyOxcOFCAPrsdmZmJubPn48+ffpU8+hqplu3bmHu3LkICwtDq1atTP62atUq6HS66hmYjTp48CDmzp2LJ598Ep6entU2jrK+H0JDQ5GbmwulUlk9AyOqQszMEt2DxMREAKjWH7OKEAQBubm51T0M7Nu3D5mZmRg8eDAA23sdaxqlUgmVSlXdw6hW2dnZ1T0EAEBOTo5V7kcmk0GtVkOhUFjl/ojuZwxmiSz05JNPomfPngCAkSNHQiaT4YEHHgAAnD59Gk8++STq1q0LtVqNgIAAPPXUU0hJSTG5j8zMTLzyyisICwuDSqWCn58f+vbti+PHjwMAHnjgAWzZsgXXrl2DTCaDTCarUG1jWFgYhgwZgr///hvt2rWDk5MTvvjiCwBAWloapk2bJj127dq1MX78eCQnJwMourS6fv16vP/++6hduzbUajV69+6NqKgok8d54IEH0KxZM5w/fx69evWCs7MzgoODsWjRolLHtWXLFjRp0gRhYWF44IEHMGHCBABA+/btIZPJ8OSTTwLQB70jR45EnTp1oFKpEBISgmnTppkVkEdGRqJbt27w9PSEq6srGjZsiLfeestkn/z8fMyZMwf16tWT7v/1119Hfn6+2a8xoH8vuLq6Ii4uDsOGDYOrqyt8fX0xY8YMaLVak311Oh2WLVuGpk2bQq1Ww9/fH5MnT8adO3dK7Pfuu+8iKCgIzs7O6NWrF86fP4+wsDDp9QGA1NRUzJgxA82bN4erqyvc3d0xcOBAnDp1Stpnz549aN++PQBg4sSJ0ntJrKc0rpnVaDTw9vbGxIkTSzzPjIwMqNVqzJgxw2qv4YYNG9C2bVs4OTnBx8cHjz/+OOLi4qS/f/TRR5DJZLh27VqJ286cOROOjo4mr93hw4cxYMAAeHh4wNnZGT179sSBAwdMbifWWJ8/fx5jx46Fl5cXunXrZtZ4xdu/9tprAIDw8HDp9YyNjZX2+eGHH6Tn5e3tjcceeww3btwwuR/xc3Ps2DH06NEDzs7O0nt006ZNGDx4MIKCgqBSqRAREYH58+ebvJ/u9v1QVs3srl270L17d7i4uMDT0xMPP/wwLly4UOrrExUVJWWePTw8MHHixBLBtjmfM6LKxjIDIgtNnjwZwcHBWLBgAV566SW0b98e/v7+APRf8NHR0Zg4cSICAgJw7tw5fPnllzh37hz+/fdfabLSlClT8Msvv2Dq1Klo0qQJUlJSsH//fly4cAFt2rTBrFmzkJ6ejps3b2Lp0qUAAFdX1wqN89KlSxgzZgwmT56MSZMmoWHDhsjKykL37t1x4cIFPPXUU2jTpg2Sk5Pxxx9/4ObNm/Dx8ZFu/7///Q9yuRwzZsxAeno6Fi1ahHHjxuHw4cMmj3Pnzh0MGDAAjzzyCEaNGoVffvkFb7zxBpo3b46BAwea7Lt161YMGTIEADBr1iw0bNgQX375JebNm4fw8HBEREQA0Ac6OTk5eO6551CrVi0cOXIEy5cvx82bN7Fhw4Yyn/O5c+cwZMgQtGjRAvPmzYNKpUJUVJRJUKPT6fDQQw9h//79ePbZZ9G4cWOcOXMGS5cuxeXLl/H7779X6HXWarXo378/OnbsiI8++gg7duzA4sWLERERgeeee07ab/LkyVizZg0mTpyIl156CTExMfj0009x4sQJHDhwQLosPHPmTCxatAhDhw5F//79cerUKfTv3x95eXkmjxsdHY3ff/8dI0eORHh4OBISEvDFF1+gZ8+eOH/+PIKCgtC4cWPMmzcPs2fPxrPPPovu3bsDALp06VLieSiVSgwfPhwbN27EF198AUdHR+lvv//+O/Lz8/HYY49Z5TUUX4f27dtj4cKFSEhIwMcff4wDBw7gxIkT8PT0xKhRo/D6669j/fr1UgApWr9+Pfr16wcvLy8A+kBt4MCBaNu2LebMmQO5XI5vvvkGDz74IPbt24cOHTqY3H7kyJGoX78+FixYAEEQ7jpWY4888gguX76Mn376CUuXLpU+L76+vgCA999/H++88w5GjRqFZ555BklJSVi+fDl69OghPS9RSkoKBg4ciMceewyPP/649B2yZs0auLq6Yvr06XB1dcWuXbswe/ZsZGRk4MMPPwSACn8/7NixAwMHDkTdunXx7rvvIjc3F8uXL0fXrl1x/PjxEifKo0aNQnh4OBYuXIjjx49j9erV8PPzwwcffADAvM8ZUZUQiMhiu3fvFgAIGzZsMNmek5NTYt+ffvpJACDs3btX2ubh4SG88MILd32MwYMHC6GhoRaNLzQ0VAAgbNu2zWT77NmzBQDCxo0bS9xGp9MJglD03Bo3bizk5+dLf//4448FAMKZM2ekbT179hQACN999520LT8/XwgICBBGjBhhcv/R0dECAGH37t3Stm+++UYAIBw9etRk39Jex4ULFwoymUy4du2atG3OnDmC8dfZ0qVLBQBCUlJSqa+LIAjC999/L8jlcmHfvn0m21euXCkAEA4cOFDmbYubMGGCAECYN2+eyfbWrVsLbdu2lf69b98+AYCwdu1ak/22bdtmsj0+Pl5wcHAQhg0bZrLfu+++KwAQJkyYIG3Ly8sTtFqtyX4xMTGCSqUyGc/Ro0cFAMI333xT6viN32N///23AEDYvHmzyX6DBg0S6tatK/37Xl7DgoICwc/PT2jWrJmQm5srbf/zzz8FAMLs2bOlbZ07dzZ5HQVBEI4cOWLyntPpdEL9+vWF/v37S+9hQdC/h8LDw4W+fftK28T3y5gxY8ocn7HSPucffvihAECIiYkx2Tc2NlZQKBTC+++/b7L9zJkzgoODg8l28XOzcuXKEo9Z2nt/8uTJgrOzs5CXlydtK+v7ISYmpsTxbtWqleDn5yekpKRI206dOiXI5XJh/Pjx0jbx9XnqqadM7nP48OFCrVq1pH+b8zkjqgosMyCqBE5OTtJ/5+XlITk5GZ06dQIAqYQA0NeIHj58GLdu3aq0sYSHh6N///4m23799Ve0bNkSw4cPL7F/8RZXEydONMnOiVm96Ohok/1cXV3x+OOPS/92dHREhw4dSuy3ZcsWeHh4mHVZ1/h1zM7ORnJyMrp06QJBEHDixIkybydmvjZt2lTmxKYNGzagcePGaNSoEZKTk6X/PfjggwCA3bt3lzu+4qZMmWLy7+7du5s8/w0bNsDDwwN9+/Y1ecy2bdvC1dVVesydO3eisLAQzz//vMn9vfjiiyUeU6VSQS7Xf5VrtVqkpKRIl3uN32sV8eCDD8LHxwfr1q2Ttt25cweRkZEYPXq0yfOx9DX877//kJiYiOeffx5qtVraPnjwYDRq1AhbtmyRto0ePRrHjh3D1atXpW3r1q2DSqXCww8/DAA4efIkrly5grFjxyIlJUUaS3Z2Nnr37o29e/eWeC8UP17WsHHjRuh0OowaNcrkNQkICED9+vVLvCYqlarUkg7j935mZiaSk5PRvXt35OTk4OLFixUe1+3bt3Hy5Ek8+eST8Pb2lra3aNECffv2xdatW0vcprT3c0pKCjIyMgCY9zkjqgoMZokqQWpqKl5++WX4+/vDyckJvr6+CA8PBwCkp6dL+y1atAhnz55FSEgIOnTogHfffbdE8HevxMc1dvXqVTRr1sys29epU8fk3+Il3eI1nrVr1y4RCHt5eZXYb8uWLejXrx8cHMqvcrp+/br04yvWoYp1ysavY3GjR49G165d8cwzz8Df3x+PPfYY1q9fb/KDe+XKFZw7dw6+vr4m/2vQoAGAoklp5lKr1dJlZlHx53/lyhWkp6fDz8+vxONmZWVJjynWh9arV8/k/ry9vaXXX6TT6bB06VLUr18fKpUKPj4+8PX1xenTp+/6Gt2Ng4MDRowYgU2bNkm1rxs3boRGozEJZu/lNRSfY8OGDUv8rVGjRiY1siNHjoRcLpeCa0EQsGHDBgwcOBDu7u7SWABgwoQJJcazevVq5Ofnl3g9Svts3KsrV65AEATUr1+/xDguXLhQ4jUJDg42OVkUnTt3DsOHD4eHhwfc3d3h6+srnSxaclzv9no3btxYCvyNlffZN+dzRlQVWDNLVAlGjRqFgwcP4rXXXkOrVq3g6uoKnU6HAQMGmHzRjxo1Ct27d8dvv/2G7du348MPP8QHH3yAjRs3lqgztZRxhscSZc2GForVGJqzX05ODvbs2YPPP/+83MfVarXo27cvUlNT8cYbb6BRo0ZwcXFBXFwcnnzyybv+YDo5OWHv3r3YvXs3tmzZgm3btmHdunV48MEHsX37digUCuh0OjRv3hxLliwp9T5CQkLKHaMxc2aN63Q6+Pn5Ye3ataX+vXgwbI4FCxbgnXfewVNPPYX58+fD29sbcrkcr7zyyj0FFY899hi++OIL/PXXXxg2bBjWr1+PRo0aoWXLlibPx5qvYVmCgoLQvXt3rF+/Hm+99Rb+/fdfXL9+XardFMcCAB9++GGJ1mOi4vWk9/rZKI1Op4NMJsNff/1V6nvCnDGkpaWhZ8+ecHd3x7x58xAREQG1Wo3jx4/jjTfeqLJgsbzPtDmfM6KqwGCWyMru3LmDnTt3Yu7cuZg9e7a0XcwcFRcYGIjnn38ezz//PBITE9GmTRu8//77UjBbGStbRURE4OzZs1a/3/Ls2rUL+fn5ZgXqZ86cweXLl/Htt99i/Pjx0vbIyEizHksul6N3797o3bs3lixZggULFmDWrFnYvXs3+vTpg4iICJw6dQq9e/eustXDIiIisGPHDnTt2vWugZS48EZUVJRJ9jAlJaVEpvuXX35Br1698NVXX5lsT0tLM5nIV9Hn2KNHDwQGBmLdunXo1q0bdu3ahVmzZpV4Ppa+huJzvHTpklSWILp06VKJxUdGjx6N559/HpcuXcK6devg7OyMoUOHmowFANzd3aukV3FZzzciIgKCICA8PFzKUFfUnj17kJKSgo0bN6JHjx7S9piYGLPHUZzx613cxYsX4ePjY9FCJeV9zoiqAssMiKxMzEYUz1wWX6VHq9WWuFzo5+eHoKAgk7ZGLi4uFl8uLsuIESNw6tQp/PbbbyX+Vnzc1rR161a0a9dOmrF9N6W9joIg4OOPPy73tqmpqSW2idk68bUdNWoU4uLisGrVqhL75ubmVkrf0VGjRkGr1WL+/Pkl/lZYWCitJtW7d284ODiUyGB/+umnJW6nUChKHLMNGzaYtLcCIAUq5q5YJZfL8eijj2Lz5s34/vvvUVhYaFJiID4fS1/Ddu3awc/PDytXrjR5v//111+4cOGC1INYNGLECCgUCvz000/YsGEDhgwZYhJ8tW3bFhEREfjoo4+QlZVV4vGSkpLMet7mKuv1fOSRR6BQKDB37twSx0UQhBLt+UpT2nu/oKAAn332WanjMOf7ITAwEK1atcK3335rMuazZ89i+/bt0mp8FWHO54yoKjAzS2Rl7u7u6NGjBxYtWgSNRoPg4GBs3769RFYlMzMTtWvXxqOPPoqWLVvC1dUVO3bswNGjR7F48WJpv7Zt22LdunWYPn062rdvD1dXV5OMlCVee+01/PLLLxg5ciSeeuoptG3bFqmpqfjjjz+wcuVKk0vJ1rR169ZSJ7uUplGjRoiIiMCMGTMQFxcHd3d3/PrrryUyk6WZN28e9u7di8GDByM0NBSJiYn47LPPULt2bWni2RNPPIH169djypQp2L17N7p27QqtVouLFy9i/fr1Um9ea+rZsycmT56MhQsX4uTJk+jXrx+USiWuXLmCDRs24OOPP8ajjz4Kf39/vPzyy1i8eDEeeughDBgwAKdOncJff/0FHx8fk2zckCFDMG/ePEycOBFdunTBmTNnsHbtWtStW9fksSMiIuDp6YmVK1fCzc0NLi4u6Nix413rRkePHo3ly5djzpw5aN68ORo3bmzy93t5DZVKJT744ANMnDgRPXv2xJgxY6TWXGFhYZg2bZrJ/n5+fujVqxeWLFmCzMzMEoG1XC7H6tWrMXDgQDRt2hQTJ05EcHAw4uLisHv3bri7u2Pz5s1mHSdztG3bFoC+PdZjjz0GpVKJoUOHIiIiAu+99x5mzpyJ2NhYDBs2DG5uboiJicFvv/2GZ5991qRPb2m6dOkCLy8vTJgwAS+99BJkMhm+//77Uk80K/L98OGHH2LgwIHo3Lkznn76aak1l4eHB959990KvwbmfM6IqkTVN1AgqjnKas118+ZNYfjw4YKnp6fg4eEhjBw5Urh165YAQJgzZ44gCPrWVa+99prQsmVLwc3NTXBxcRFatmwpfPbZZyb3lZWVJYwdO1bw9PQUAFSoTVdoaKgwePDgUv+WkpIiTJ06VQgODhYcHR2F2rVrCxMmTBCSk5Pv+txKa/nTs2dPoWnTpiUew7jl09mzZwUAwpEjR0rsV1ZrrvPnzwt9+vQRXF1dBR8fH2HSpEnCqVOnSjx+8dZcO3fuFB5++GEhKChIcHR0FIKCgoQxY8YIly9fNrn/goIC4YMPPhCaNm0qqFQqwcvLS2jbtq0wd+5cIT09vdTXrTQTJkwQXFxcSmwvPi7Rl19+KbRt21ZwcnIS3NzchObNmwuvv/66cOvWLWmfwsJC4Z133hECAgIEJycn4cEHHxQuXLgg1KpVS5gyZYq0X15envDqq68KgYGBgpOTk9C1a1fh0KFDQs+ePYWePXuaPO6mTZuEJk2aCA4ODiavYfHWXCKdTieEhIQIAIT33nuv1Od+r6/hunXrhNatWwsqlUrw9vYWxo0bJ9y8ebPUfVetWiUAENzc3EzaeRk7ceKE8Mgjjwi1atUSVCqVEBoaKowaNUrYuXOntI94XMxtKVXWZ2H+/PlCcHCwIJfLS7Tp+vXXX4Vu3boJLi4ugouLi9CoUSPhhRdeEC5duiTtU9bnRhAE4cCBA0KnTp0EJycnISgoSHj99dellmnGbe3K+n4o7XMqCIKwY8cOoWvXroKTk5Pg7u4uDB06VDh//rzJPmW9PuLnVHye5n7OiCqbTBAq8ZoiEZHBokWLsGTJEty+fbvKalRrmrS0NHh5eeG9994rUb9KRGSvWDNLRFUiLCwMS5cuZSBrptKW7BXrrsVlk4mICGBmlsgGJSUlmazRXpyjo6NJY3SyTHp6eqlBpbGAgIBKeew1a9ZgzZo1GDRoEFxdXbF//3789NNP6NevH/7+++9KeUwiIlvEYJbIBoWFhZk0lS+uZ8+e2LNnT9UNqIZ68skn8e233951n8r6Cj1+/Dhef/11nDx5EhkZGfD398eIESPw3nvvlehVSkRkzxjMEtmgAwcO3DVj6OXlJc22JsudP3++3KWG2UuTiKh6MZglIiIiIpvFCWBEREREZLPsbtEEnU6HW7duwc3NjbOqiYiIiO5DgiAgMzMTQUFBkMvvnnu1u2D21q1bCAkJqe5hEBEREVE5bty4gdq1a991H7sLZt3c3ADoXxx3d/dKexyNRoPt27dLy1WSbeJxtH08hraPx9D28Rjavqo+hhkZGQgJCZHitruxu2BWLC1wd3ev9GDW2dkZ7u7u/ODaMB5H28djaPt4DG0fj6Htq65jaE5JKCeAEREREZHNYjBLRERERDaLwSwRERER2Sy7q5klIiIyhyAIKCwshFarre6h2DyNRgMHBwfk5eXx9bRRlXEMlUolFArFPd8Pg1kiIqJiCgoKcPv2beTk5FT3UGoEQRAQEBCAGzdusMe7jaqMYyiTyVC7dm24urre0/0wmCUiIjKi0+kQExMDhUKBoKAgODo6MgC7RzqdDllZWXB1dS23AT7dn6x9DAVBQFJSEm7evIn69evfU4aWwSwREZGRgoIC6HQ6hISEwNnZubqHUyPodDoUFBRArVYzmLVRlXEMfX19ERsbC41Gc0/BLN9RREREpWDQRVS5rHXFg59UIiIiIrJZDGaJiIiIyGZVazC7d+9eDB06FEFBQZDJZPj999/Lvc2ePXvQpk0bqFQq1KtXD2vWrKn0cRIREdkCQRDw7LPPwtvbGzKZDCdPnqzuIVWrAwcOoHnz5lAqlRg2bJhZt3nyySfN3remCwsLw7Jly6p7GOWq1mA2OzsbLVu2xIoVK8zaPyYmBoMHD0avXr1w8uRJvPLKK3jmmWfw999/V/JIiYiI7n/btm3DmjVr8Oeff+L27dto1qzZPd2fuYmm+0mvXr2wevVqAMD06dPRqlUrxMTEMPl1F2vWrIGnp2eJ7UePHsWzzz5b9QOqoGrtZjBw4EAMHDjQ7P1XrlyJ8PBwLF68GADQuHFj7N+/H0uXLkX//v0ra5hEREQ24erVqwgMDESXLl2qeygA9J0hHB0dq+zxUlNTceDAAfz8888A9K/HlClTULt27SobQ03i6+tb3UMwi0215jp06BD69Oljsq1///545ZVXyrxNfn4+8vPzpX9nZGQA0K9kodFoKmWc4v0b/396rgZz/riAqKQss+8jyFONWQMbIbQWW8NUl+LHkWwPj6Htq+pjqNFoIAgCdDoddDodAP3l+1xN1a9c5aRUmD3je+LEifjuu+8A6DOqoaGh+Oyzz7BgwQKcPXsWCoUCnTp1wrJlyxAREQFAH2y++uqr2LhxI+7cuQN/f39MnjwZb775JurWrQsAGD58OAAgNDQU0dHRdx3D3LlzsWnTJjz//PNYuHAhrl27hsLCQty5cwevvvoq/vrrL6Snp6NevXpYsGABhgwZgjVr1mD69On46aefMH36dNy4cQNdu3bF119/jcDAQOm5paWloVu3bliyZAkKCgowevRoLF26FEqlUnr8zZs3o02bNsjOzpZet6eeegpPPfUUvvrqKzzxxBOYPHkydu/ejfj4eNSpUwfPPfccXnrpJek+BEGQjj8A/PLLL5g/fz6ioqLg7OyM1q1b47fffoOLiwsAYPXq1Vi6dCliYmIQFhaGF198Ec8991y5xys2NhYRERHYsGEDVqxYgcOHD6N+/fr47LPP0LlzZ2m//fv3Y9asWfjvv//g4+ODYcOGYcGCBdLj3759G5MmTcLu3bsREBCA+fPn4+2338bLL7+Ml19+GQCwdOlSrFmzBtHR0fD29saQIUPwwQcfwNXVFXv27MHEiROl9w0AzJ49G3PmzEHdunWl+xk3bhzy8/OxYcMG6bXRaDQIDg7GRx99hPHjx0On02HRokVYtWoV4uPj0aBBA8yaNQuPPvpoqa+BTqeDIAiltuaqyOfdpoLZ+Ph4+Pv7m2zz9/dHRkYGcnNz4eTkVOI2CxcuxNy5c0ts3759e5X0D4yMjES+Fvj8ggIxmRVrQXEpIQv/Ru3DuHo6NPcWKmmEZI7IyMjqHgLdIx5D21dVx9DBwQEBAQHIyspCQUEBACC3QIvOS/6tksc3dmh6Jzg5mtd/c968eahduzbWrFmDXbt2QaFQ4ODBg5g8eTKaNm2K7OxsLFiwAMOGDcO+ffsgl8uxfPlybNq0CV999RVq166NuLg4xMXFISMjAzt27ED9+vWxYsUK9O7dGwqFQkoIlSU/Px9RUVFYv349vv32W8jlcqSlpWHgwIHIzMyUrrBevHgR+fn5yMjIQF5eHnJycrBo0SJ89tlnkMvlmDx5Ml555RWsWrUKgD6w2b17N2rVqoVNmzYhOjoaTz/9NBo2bIgJEyZIj//bb7+hX79+8PDwwMWLF9G+fXu89dZbGD58ONzd3ZGWlgZfX198/fXX8Pb2xuHDhzFt2jR4eHhIQbtGo0FhYSEyMjIQHx+PcePGYe7cuRgyZAgyMzNx6NAhpKenQ6vVYv369ZgzZw4WLVqEFi1a4PTp03j55Zchl8sxZsyYu75WWVn65NasWbMwb948fPjhh3jvvfcwZswYHD9+HA4ODoiJicGgQYMwa9YsLFu2DMnJyXj99dcxZcoUqUTz8ccfR0pKCjZv3gylUolZs2YhMTEReXl50vEqKCjAggULEBoaitjYWMyYMQPTpk3D4sWL0axZMyxcuBALFizA0aNHAQAuLi7IyMiATqeT7mfYsGGYOHEibt++La3YtW3bNuTk5KB3797IyMjARx99hA0bNuCjjz5CREQEDh48iPHjx8PFxQVdu3Yt8RoUFBQgNzcXe/fuRWFhocnfKrL6nk0Fs5aYOXMmpk+fLv07IyMDISEh6NevH9zd3SvtcTUaDSIjI/HAg73x0vpziMlMhrvaAe8Pawo3dfkvu04n4NM90Th+PQ2rLymw6onWeKCBbaT7axLxOPbt29fk7J9sB4+h7avqY5iXl4cbN27A1dUVarUaAOBQUFjOrSqHm7sbnB3N+6l2d3eHj48PlEol6tevDwBSdlX07bffwt/fHzdv3kSzZs2QmJiIhg0bon///pDJZCY1tuJvZEBAgHR/5VGpVCgoKMDatWulS9Tbt2/HsWPHcPjwYbRu3RoymQwtWrSQbqNWq6HRaPDll19KGeMXX3wR8+fPl8agVCrh7e2NL774AgqFAu3atcOvv/6KgwcP4sUXXwSgD6R37tyJ+fPnw8vLC15eXpDL5fDz8zMZ/8KFC6X/bt68OU6dOoU///xTCoqVSiUcHBzg7u6OqKgoFBYWYsyYMQgNDQUAk6zpokWL8NFHH0mBa/PmzREbG4vvv/8ekydPvutrJQaEM2bMwMiRIwEA7733Hpo3b47ExEQ0atQIn376KcaOHYs33nhDut3y5cvRq1cvrFq1CrGxsdizZw8OHz6Mdu3aAQC+/vprNGzYEGq1Wnr9jG/frFkz5OXl4fnnn5dOFvz8/CCXy0scZ7lcLt3PsGHD8MILL2Dnzp144oknAACbNm3C0KFDERwcjPz8fCxduhTbt2+XXqMWLVrg2LFj+OGHH0otK83Ly4OTkxN69OghfdZE5Z04GbOpYDYgIAAJCQkm2xISEuDu7l5qVhbQf7BUKlWJ7Uqlskq+FBdFRuOfK8lQK+X4ZmJ7tA31Nvu23Rr4Y/r6k/jz9G38eSYBfZsGVeJI6W6q6v1ClYfH0PZV1THUarWQyWSQy+XSwgkuKiXOz6v6uRkVKTMAii4Ti+O+cuUKZs+ejcOHDyM5OVm6PHzz5k20aNECEydORN++fdG4cWMMGDAAQ4YMQb9+/Uzu0/h1MOfxQ0NDTa6inj59GrVr10a9evWk17X4/Ts7O5sEUkFBQUhMTJT2lclkaNq0qcnxDwoKwpkzZ6R99uzZAz8/PzRv3vyu41+xYgW+/vprXL9+Hbm5uSgoKECrVq1MHkscZ+vWrdG7d2+0bNkS/fv3R79+/fDoo4/Cy8sL2dnZuHr1KiZNmmQSuBYWFsLDw6Pc10z8u/FjBwcHAwCSk5Mhl8tx+vRpnD59Gj/++KN0O7EE4tq1a7hy5QocHBzQrl076T4aNGgALy8vk9d6x44dWLhwIS5evIiMjAwUFhYiLy8PeXl5cHZ2lvYrbczi/YgdIX788UdMmDAB2dnZ+OOPP/Dzzz9DLpcjOjoaOTk5JeYwFRQUoHXr1qXet1wuh0wmK/WzXZHPuk0Fs507d8bWrVtNtkVGRpqcJd1vxnUMwT9XkjF/WLMKBbIA4Oggx9gOdfDn6ds4HJ0KQRC4PjgRUTWQyWRmZ0jvJ0OHDkVoaChWrVqFoKAg6HQ6NGvWTCqfaNOmDWJiYvDXX39hx44dGDVqFPr06YNffvnF4scUazlFZSWbjBUPXGQyGQRBKHcfMTgHgD/++AMPPfTQXR/n559/xowZM7B48WJ07twZbm5u+PDDD3H48OFS91coFIiMjMTBgwexfft2LF++HLNmzcLhw4elUsVVq1ahY8eOJW5nLuPnJf7Gi88rKysLkydPNqnpFdWpUweXL18u9/5jY2MxZMgQPPfcc3j//ffh7e2N/fv34+mnn0ZBQUGFSi5HjhyJIUOGIDExEZGRkXBycsKAAQOksQLAli1bpKBcVFpS0Zqq9ZOZlZWFqKgo6d8xMTE4efIkvL29UadOHcycORNxcXFSQfuUKVPw6aef4vXXX8dTTz2FXbt2Yf369diyZUt1PYVy1fdzxc5Xe0LlYNmaw63reEGpkCE+Iw83UnNRh5PBiIjIDCkpKbh06RJWrVqF7t27A9BPJirO3d0do0ePxujRo/Hoo49iwIABSE1Nhbe3N5RKJbTae5v41qJFC9y8eRNRUVFo06bNPd1XWQRBwObNm/HDDz/cdb8DBw6gS5cueP7556VtV69evettZDIZunbtiq5du2L27NkIDQ3Fb7/9hunTpyMoKAjR0dEYN26cVZ5HcW3atMH58+dRr169Uv/esGFDFBYW4sSJE2jbti0AICoqCnfu3JH2OXbsGHQ6HRYvXixlR9evX29yP46OjmYd544dOyIkJATr1q3DX3/9hZEjR0rBeJMmTaBSqXD9+nX07NnToudrqWoNZv/77z/06tVL+rdY2zphwgSsWbMGt2/fxvXr16W/h4eHY8uWLZg2bRo+/vhj1K5dG6tXr77v23JZGsgCgJOjAi1qe+LYtTs4HJPCYJaIiMzi5eWFWrVq4csvv0RgYCCuX7+ON99802SfJUuWIDAwULoMvGHDBgQEBEg9R8PCwrBz50507doVKpUKXl5eFR5Hz5490aNHD4wfPx5Lly5FgwYNcPHiRchkMimrd6+OHTuGnJwcdOvW7a771a9fH9999x3+/vtvhIeH4/vvv8fRo0cRHh5e6v6HDx/Gzp070a9fP/j5+eHw4cNISkpC48aNAei7N7z00kvw8PDAgAEDkJ+fj//++w937twxma9jqTfeeAOdOnXC1KlT8cwzz8DFxQXnz59HZGQkPv30UzRq1Ah9+vTBs88+i88//xxKpRKvvvoqnJycpCxvvXr1oNFosHz5cgwdOhQHDhzAypUrTR4nLCwMWVlZ2LlzJ1q2bAlnZ+cyM7ZjxozBypUrcfnyZezevVva7ubmJk0s0+l06NatG9LT03HgwAG4u7ubTNSztmpdNOGBBx6QWmAY/09sbLxmzRrs2bOnxG1OnDiB/Px8XL16FU8++WSVj7uqdQjXlyccjkmt5pEQEZGtkMvl+Pnnn3Hs2DE0a9YM06ZNw4cffmiyj5ubGxYtWoR27dqhffv2iI2NxdatW6UM3uLFixEZGYmQkBC0bt3a4rFs2LABbdq0wbhx49CkSRO8/vrr95zxNbZp0yYMGjQIDg53z9FNnjwZjzzyCEaPHo2OHTsiJSXFJEtbnLu7O/bu3YtBgwahQYMGePvtt7F48WJpMtMzzzyD1atX45tvvkHz5s3Rs2dPrFmzpszguKJatGiBf/75B5cvX0b37t3RunVrzJ49G0FBRXNovvvuO/j7+6NHjx4YPnw4Jk2aBDc3N2lCVcuWLbFkyRJ88MEHaNasGdauXWsyCQ4AunTpgilTpmD06NHw9fXFokWLyhzT2LFjcf78eQQHB5foUDB//ny88847WLhwoVSHvWXLFqu9HmWRCcWLUmq4jIwMeHh4ID09vdK7GWzduhWDBg265wkLey4l4slvjqKOtzP2vt6r/BuQ1VjzOFL14DG0fVV9DPPy8hATE4Pw8PASM6zJMjqdDhkZGXB3dzd7MllFtGjRAm+//TZGjRpl9fu2NTdv3kRISAh27NiB3r17W+1+K+MY3u2zVpF4zfaq2e1Q21AvyGXA9dQc3E7PRaBH+cX0RERE9qCgoAAjRoyo0IqiNcmuXbuQlZWF5s2b4/bt23j99dcRFhaGHj16VPfQqky1lhmQedzUSjQN8gAAHGGpARERVZOmTZvC1dW11P+tXbu2Wsbk6OiIOXPmwM3NrVoevywLFiwo87WyZuCt0Wjw1ltvoWnTphg+fDh8fX2xZ88eu7oSxcysjegY7o0zcen4NzoVD7cKLv8GREREVrZ169YylxktvkKnvZsyZUqZZQ/mtCszV//+/e/7ifCVjcGsjWhdxwtADC7Gm78iBhERkTWJq2BR+by9veHtXbH+8mQZlhnYiAAPfcPh5Kz8ah4JEZF9sLP50URVzlqfMQazNsLXVT/LLzmzgF+wRESVSKw1zMnJqeaRENVs4kp0FVkxrTQsM7ARPm6OAIBcjRbZBVq4qnjoiIgqg0KhgKenJxITEwEAzs7OXEr8Hul0OhQUFCAvL69SWnNR5bP2MdTpdEhKSoKzs3O5/YHLw4jIRjg7OsDFUYHsAi2SM/MZzBIRVaKAgAAAkAJaujeCICA3N9dkZSqyLZVxDOVyOerUqXPP98eIyIb4uKmQnZKDpKx8hPm4VPdwiIhqLJlMhsDAQPj5+ZU5e5/Mp9FosHfvXvTo0cOuWkbVJJVxDB0dHa2S5WUwa0N8XFW4lpKD5ExOAiMiqgoKheKe6/lI/zoWFhZCrVYzmLVR9/MxZOGKDfF11Xc0SGJHAyIiIiIADGZtijgJjJlZIiIiIj0GszZEbM/FzCwRERGRHoNZGyJmZpMyC6p5JERERET3BwazNkSsmeUqYERERER6DGZtiI+bYQIYa2aJiIiIADCYtSnGmVkuaUtERETEYNam+Boys/mFOmTmF1bzaIiIiIiqH4NZG6JWKuBmWMaW7bmIiIiIGMzaHNbNEhERERVhMGtjfFwNCydksT0XEREREYNZG+MrZWbzqnkkRERERNWPwayN8ZE6GjAzS0RERMRg1sZw4QQiIiKiIgxmbQwngBEREREVYTBrY5iZJSIiIirCYNbGMDNLREREVITBrI0RuxkkZxVwSVsiIiKyewxmbYy7Wr8CWIFWh/xCXTWPhoiIiKh6MZi1MWqlQvpvBrNERERk7xjM2hgHuQxymf6/8zXa6h0MERERUTVjMGtjZDKZlJ1lZpaIiIjsHYNZG6Ry0B+2PGZmiYiIyM4xmLVBKgdmZomIiIgABrM2Sa3UH7b8QmZmiYiIyL4xmLVBYmY2T8PMLBEREdk3BrM2iJlZIiIiIj0GszaImVkiIiIiPQazNkjFzCwRERERAAazNomZWSIiIiI9BrM2SKqZZZ9ZIiIisnMMZm2QlJlln1kiIiKycwxmbZBUM8syAyIiIrJzDGZtkFrKzLLMgIiIiOwbg1kbxMwsERERkR6DWRskZmbZmouIiIjsHYNZGyRmZtmai4iIiOwdg1kbpHbgoglEREREAINZm6RSctEEIiIiIoDBrE1SczlbIiIiIgAMZm2SuGgCuxkQERGRvWMwa4NUrJklIiIiAsBg1iapWTNLREREBIDBrE1iZpaIiIhIj8GsDRIzs/mFzMwSERGRfWMwa4PEzGyehplZIiIism8MZm0QM7NEREREegxmbZBxZlYQhGoeDREREVH1YTBrg8QVwHQCUKhjMEtERET2i8GsDRIzswDrZomIiMi+MZi1QcbBLOtmiYiIyJ4xmLVBMpmMHQ2IiIiIwGDWZhUtnMDMLBEREdkvBrM2qmhJW2ZmiYiIyH4xmLVRKiUzs0RERET3RTC7YsUKhIWFQa1Wo2PHjjhy5Mhd91+2bBkaNmwIJycnhISEYNq0acjLy6ui0d4f1A6GhRM0DGaJiIjIflV7MLtu3TpMnz4dc+bMwfHjx9GyZUv0798fiYmJpe7/448/4s0338ScOXNw4cIFfPXVV1i3bh3eeuutKh559RIzs3mFLDMgIiIi+1XtweySJUswadIkTJw4EU2aNMHKlSvh7OyMr7/+utT9Dx48iK5du2Ls2LEICwtDv379MGbMmHKzuTUNM7NEREREgEN1PnhBQQGOHTuGmTNnStvkcjn69OmDQ4cOlXqbLl264IcffsCRI0fQoUMHREdHY+vWrXjiiSdK3T8/Px/5+fnSvzMyMgAAGo0GGo3Gis/GlHjflfUYjgoZACA7r6BSn4e9q+zjSJWPx9D28RjaPh5D21fVx7Aij1OtwWxycjK0Wi38/f1Ntvv7++PixYul3mbs2LFITk5Gt27dIAgCCgsLMWXKlDLLDBYuXIi5c+eW2L59+3Y4Ozvf+5MoR2RkZKXcb1qqHIAc/504CYe4E5XyGFSkso4jVR0eQ9vHY2j7eAxtX1Udw5ycHLP3rdZg1hJ79uzBggUL8Nlnn6Fjx46IiorCyy+/jPnz5+Odd94psf/MmTMxffp06d8ZGRkICQlBv3794O7uXmnj1Gg0iIyMRN++faFUKq1+/39lnML5tAQ0aNwUgzrWsfr9k15lH0eqfDyGto/H0PbxGNq+qj6G4pV0c1RrMOvj4wOFQoGEhAST7QkJCQgICCj1Nu+88w6eeOIJPPPMMwCA5s2bIzs7G88++yxmzZoFudy0DFilUkGlUpW4H6VSWSUHo7Iex8lRf+gKdTJ+MVSBqnq/UOXhMbR9PIa2j8fQ9lVl/GSuap0A5ujoiLZt22Lnzp3SNp1Oh507d6Jz586l3iYnJ6dEwKpQ6CdDCYJQeYO9z6iVXM6WiIiIqNrLDKZPn44JEyagXbt26NChA5YtW4bs7GxMnDgRADB+/HgEBwdj4cKFAIChQ4diyZIlaN26tVRm8M4772Do0KFSUGsPVGI3Ay6aQERERHas2oPZ0aNHIykpCbNnz0Z8fDxatWqFbdu2SZPCrl+/bpKJffvttyGTyfD2228jLi4Ovr6+GDp0KN5///3qegrVQsXMLBEREVH1B7MAMHXqVEydOrXUv+3Zs8fk3w4ODpgzZw7mzJlTBSO7fzEzS0RERHQfLJpAlhFrZvO5AhgRERHZMQazNkrMzOZxBTAiIiKyYwxmbZTKgZlZIiIiIgazNkqtZGaWiIiIiMGsjWJmloiIiIjBrM1iZpaIiIiIwazNKsrMMpglIiIi+8Vg1kaJmdl8LppAREREdozBrI1iZpaIiIiIwazNKqqZZWaWiIiI7JfFwey+ffvw+OOPo3PnzoiLiwMAfP/999i/f7/VBkdlY2aWiIiIyMJg9tdff0X//v3h5OSEEydOID8/HwCQnp6OBQsWWHWAVDoVl7MlIiIisiyYfe+997By5UqsWrUKSqVS2t61a1ccP37caoOjsqkNy9lqtAK0OqGaR0NERERUPSwKZi9duoQePXqU2O7h4YG0tLR7HROZQczMAszOEhERkf2yKJgNCAhAVFRUie379+9H3bp173lQVD6VITMLcOEEIiIisl8WBbOTJk3Cyy+/jMOHD0Mmk+HWrVtYu3YtZsyYgeeee87aY6RSKOQyKBUyAMzMEhERkf1ysORGb775JnQ6HXr37o2cnBz06NEDKpUKM2bMwIsvvmjtMVIZ1A4KaLSFzMwSERGR3bIomJXJZJg1axZee+01REVFISsrC02aNIGrq6u1x0d3oVLKkZnPzCwRERHZL4vKDH744Qfk5OTA0dERTZo0QYcOHRjIVgOxbpaZWSIiIrJXFgWz06ZNg5+fH8aOHYutW7dCq2VmsDpIvWa5ChgRERHZKYuC2du3b+Pnn3+GTCbDqFGjEBgYiBdeeAEHDx609vjoLqTMLFcBIyIiIjtlUTDr4OCAIUOGYO3atUhMTMTSpUsRGxuLXr16ISIiwtpjpDKoDZnZPGZmiYiIyE5ZNAHMmLOzM/r37487d+7g2rVruHDhgjXGRWZQSzWzDGaJiIjIPlmUmQWAnJwcrF27FoMGDUJwcDCWLVuG4cOH49y5c9YcH92FWqqZZZkBERER2SeLMrOPPfYY/vzzTzg7O2PUqFF455130LlzZ2uPjcqhVoo1s8zMEhERkX2yKJhVKBRYv349+vfvD4VCUf4NqFJIwSzLDIiIiMhOWRTMrl271trjIAsUBbMsMyAiIiL7ZHYw+8knn+DZZ5+FWq3GJ598ctd9X3rppXseGJWP3QyIiIjI3pkdzC5duhTjxo2DWq3G0qVLy9xPJpMxmK0izMwSERGRvTM7mI2JiSn1v6n6SK25OAGMiIiI7JRFrbnmzZuHnJycEttzc3Mxb968ex4UmYdlBkRERGTvLApm586di6ysrBLbc3JyMHfu3HseFJlHLDNgn1kiIiKyVxYFs4IgQCaTldh+6tQpeHt73/OgyDzMzBIREZG9q1BrLi8vL8hkMshkMjRo0MAkoNVqtcjKysKUKVOsPkgqHRdNICIiIntXoWB22bJlEAQBTz31FObOnQsPDw/pb46OjggLC+NKYFVI5cBuBkRERGTfKhTMTpgwAQAQHh6OLl26QKlUVsqgyDximUFuATOzREREZJ8sWgGsZ8+e0n/n5eWhoKDA5O/u7u73NioyC8sMiIiIyN5ZNAEsJycHU6dOhZ+fH1xcXODl5WXyP6oa7GZARERE9s6iYPa1117Drl278Pnnn0OlUmH16tWYO3cugoKC8N1331l7jFQGdjMgIiIie2dRmcHmzZvx3Xff4YEHHsDEiRPRvXt31KtXD6GhoVi7di3GjRtn7XFSKaQVwBjMEhERkZ2yKDObmpqKunXrAtDXx6ampgIAunXrhr1791pvdHRXRTWzLDMgIiIi+2RRMFu3bl3ExMQAABo1aoT169cD0GdsPT09rTY4ujuxzECrE6DRMqAlIiIi+2NRMDtx4kScOnUKAPDmm29ixYoVUKvVmDZtGl577TWrDpDKJmZmAZYaEBERkX2yqGZ22rRp0n/36dMHFy9exLFjx1CvXj20aNHCaoOju1M5FJ2L5Gl0cFNX42CIiIiIqoFFwWxxoaGhCA0NtcZdUQXIZDKolXLkaXTMzBIREZFdsiiY/eSTT0rdLpPJoFarUa9ePfTo0QMKhaLU/ch61EoF8jQ65HPhBCIiIrJDFgWzS5cuRVJSEnJycqRFEu7cuQNnZ2e4uroiMTERdevWxe7duxESEmLVAZMpfXsuDfK4cAIRERHZIYsmgC1YsADt27fHlStXkJKSgpSUFFy+fBkdO3bExx9/jOvXryMgIMCktpYqBxdOICIiIntmUWb27bffxq+//oqIiAhpW7169fDRRx9hxIgRiI6OxqJFizBixAirDZRKJ/WaZWaWiIiI7JBFmdnbt2+jsLCwxPbCwkLEx8cDAIKCgpCZmXlvo6NyqZRcBYyIiIjsl0XBbK9evTB58mScOHFC2nbixAk899xzePDBBwEAZ86cQXh4uHVGSWVSG9pz5XECGBEREdkhi4LZr776Ct7e3mjbti1UKhVUKhXatWsHb29vfPXVVwAAV1dXLF682KqDpZJYZkBERET2zKKa2YCAAERGRuLixYu4fPkyAKBhw4Zo2LChtE+vXr2sM0K6K04AIyIiInt2T4sm1K1bFzKZDBEREXBwsMr6C1RBatbMEhERkR2zqMwgJycHTz/9NJydndG0aVNcv34dAPDiiy/if//7n1UHSHen7zML5BeyzICIiIjsj0XB7MyZM3Hq1Cns2bMHarVa2t6nTx+sW7fOaoOj8ollBrkFzMwSERGR/bGoNuD333/HunXr0KlTJ8hkMml706ZNcfXqVasNjsrHMgMiIiKyZxZlZpOSkuDn51die3Z2tklwS5VP6jPL1lxERERkhywKZtu1a4ctW7ZI/xYD2NWrV6Nz587WGRmZpaibAWtmiYiIyP5YVGawYMECDBw4EOfPn0dhYSE+/vhjnD9/HgcPHsQ///xj7THSXYgTwFhmQERERPbIosxst27dcPLkSRQWFqJ58+bYvn07/Pz8cOjQIbRt29baY6S7cHLkoglERERkvyxuDhsREYFVq1ZZcyxkAbHMIJ81s0RERGSHLA5mdTodoqKikJiYCJ3ONCvYo0ePex4YmYdlBkRERGTPLApm//33X4wdOxbXrl2DIAgmf5PJZNBqGVhVlaLWXCwzICIiIvtjUTA7ZcoUqaNBYGAg23FVI5XUzYAnEERERGR/LJoAduXKFSxYsACNGzeGp6cnPDw8TP5XUStWrEBYWBjUajU6duyII0eO3HX/tLQ0vPDCCwgMDIRKpUKDBg2wdetWS56KzVOzzywRERHZMYuC2Y4dOyIqKsoqA1i3bh2mT5+OOXPm4Pjx42jZsiX69++PxMTEUvcvKChA3759ERsbi19++QWXLl3CqlWrEBwcbJXx2JqimlmWGRAREZH9sajM4MUXX8Srr76K+Ph4NG/eHEql0uTvLVq0MPu+lixZgkmTJmHixIkAgJUrV2LLli34+uuv8eabb5bY/+uvv0ZqaioOHjwoPW5YWJglT6NGULPMgIiIiOyYRcHsiBEjAABPPfWUtE0mk0EQhApNACsoKMCxY8cwc+ZMaZtcLkefPn1w6NChUm/zxx9/oHPnznjhhRewadMm+Pr6YuzYsXjjjTegUChK7J+fn4/8/Hzp3xkZGQAAjUYDjUZj1jgtId53ZT4GADjI9BPw8jTaSn8se1RVx5EqD4+h7eMxtH08hravqo9hRR7HomA2JibGkpuVkJycDK1WC39/f5Pt/v7+uHjxYqm3iY6Oxq5duzBu3Dhs3boVUVFReP7556HRaDBnzpwS+y9cuBBz584tsX379u1wdna2yvO4m8jIyEq9/ywNADhAoxXw55atkHMuXqWo7ONIlY/H0PbxGNo+HkPbV1XHMCcnx+x9LQpmQ0NDzdpv8ODBWL16NQIDAy15mFLpdDr4+fnhyy+/hEKhQNu2bREXF4cPP/yw1GB25syZmD59uvTvjIwMhISEoF+/fnB3d7fauIrTaDSIjIxE3759S5RhWFNOQSFm/bcLAPBg335wdrS4dTCVoqqOI1UeHkPbx2No+3gMbV9VH0PxSro5KjXy2bt3L3Jzc8v8u4+PDxQKBRISEky2JyQkICAgoNTbBAYGQqlUmpQUNG7cGPHx8SgoKICjo6PJ/iqVCiqVqsT9KJXKKjkYlf04boqiQ6iFgl8SlaSq3i9UeXgMbR+Poe3jMbR9VRk/mcuibgbW4ujoiLZt22Lnzp3SNp1Oh507d6Jz586l3qZr166IiooyWXXs8uXLCAwMLBHI2gO5XAZHBSeBERERkX2q1mAWAKZPn45Vq1bh22+/xYULF/Dcc88hOztb6m4wfvx4kwlizz33HFJTU/Hyyy/j8uXL2LJlCxYsWIAXXnihup5CtePCCURERGSvqr3AcvTo0UhKSsLs2bMRHx+PVq1aYdu2bdKksOvXr0MuL4q5Q0JC8Pfff2PatGlo0aIFgoOD8fLLL+ONN96orqdQ7dRKBTLzCtlrloiIiOxOtQezADB16lRMnTq11L/t2bOnxLbOnTvj33//reRR2Q6p1yxXASMiIiI7U+1lBnTvpFXAChjMEhERkX2p1GD2rbfegre3d2U+BEFfZgAwM0tERET2x+Jg9vvvv0fXrl0RFBSEa9euAQCWLVuGTZs2SfvMnDkTnp6e9zxIujsnMZhlzSwRERHZGYuC2c8//xzTp0/HoEGDkJaWJi1f6+npiWXLlllzfGQGdjMgIiIie2VRMLt8+XKsWrUKs2bNMlm8oF27djhz5ozVBkfmUTMzS0RERHbKomA2JiYGrVu3LrFdpVIhOzv7ngdFFVMUzDIzS0RERPbFomA2PDwcJ0+eLLF927ZtaNy48b2OiSpI7cDWXERERGSfLOozO336dLzwwgvIy8uDIAg4cuQIfvrpJyxcuBCrV6+29hipHCwzICIiIntlUTD7zDPPwMnJCW+//TZycnIwduxYBAUF4eOPP8Zjjz1m7TFSOZwc9cFsbkFhNY+EiIiIqGpZvALYuHHjMG7cOOTk5CArKwt+fn7WHBdVgKezEgBwJ0dTzSMhIiIiqloW1czm5uYiJycHAODs7Izc3FwsW7YM27dvt+rgyDy1XBwBAKnZBdU8EiIiIqKqZVEw+/DDD+O7774DAKSlpaFDhw5YvHgxHn74YXz++edWHSCVz9tFBQBIYTBLREREdsaiYPb48ePo3r07AOCXX35BQEAArl27hu+++w6ffPKJVQdI5fM2ZGZTsvKreSREREREVcuiYDYnJwdubm4AgO3bt+ORRx6BXC5Hp06dpKVtqeqwzICIiIjslUXBbL169fD777/jxo0b+Pvvv9GvXz8AQGJiItzd3a06QCpfLVd9MJtToOXCCURERGRXLApmZ8+ejRkzZiAsLAwdO3ZE586dAeiztKWtDEaVy1XlAEeF/lCybpaIiIjsiUWtuR599FF069YNt2/fRsuWLaXtvXv3xvDhw602ODKPTCaDt4sj4jPykJKVj2BPp+oeEhEREVGVsLjPbEBAAAICAky2dejQ4Z4HRJaRgllmZomIiMiOmB3MPvLII2bf6caNGy0aDFlOrJtNzWIwS0RERPbD7GDWw8OjMsdB98ibHQ2IiIjIDpkdzH7zzTeVOQ66R7W4cAIRERHZIYu6GdD9Rywz4MIJREREZE8sngD2yy+/YP369bh+/ToKCkyzgcePH7/ngVHFsMyAiIiI7JFFmdlPPvkEEydOhL+/P06cOIEOHTqgVq1aiI6OxsCBA609RjKDtKQtg1kiIiKyIxYFs5999hm+/PJLLF++HI6Ojnj99dcRGRmJl156Cenp6dYeI5mBS9oSERGRPbIomL1+/Tq6dOkCAHByckJmZiYA4IknnsBPP/1kvdGR2Wq5GiaAsWaWiIiI7IhFwWxAQABSU1MBAHXq1MG///4LAIiJiYEgCNYbHZlNLDPILtAiT6Ot5tEQERERVQ2LgtkHH3wQf/zxBwBg4sSJmDZtGvr27YvRo0dzOdtq4q52gFIhA8BSAyIiIrIfFnUz+PLLL6HT6QAAL7zwAmrVqoWDBw/ioYcewuTJk606QDKPTCaDl7MjEjPzkZpdgCBPp+oeEhEREVGlMzsz+8gjjyAjIwMA8MMPP0CrLbqU/dhjj+GTTz7Biy++CEdHR+uPkszCjgZERERkb8wOZv/8809kZ2cD0JcWsGvB/YcLJxAREZG9MbvMoFGjRpg5cyZ69eoFQRCwfv16uLu7l7rv+PHjrTZAMp+4pC1rZomIiMhemB3Mrly5EtOnT8eWLVsgk8nw9ttvQyaTldhPJpMxmK0mLDMgIiIie2N2MNulSxepBZdcLsfly5fh5+dXaQOjipMWTshiMEtERET2oUKtub7++mskJycjJiYGvr6+lTUmspC3KzOzREREZF8qFMz+8MMPqF27NsaMGYNFixbhwoULlTUuskAtqcyAE8CIiIjIPlQomN21axdu376N559/HseOHUPHjh1Rv359vPrqq9i7d6/Ue5aqh5taCQDIyecKYERERGQfKrwCmJeXFx5//HGsX78eycnJWL58OXJzczFu3Dj4+flh/Pjx+OWXX6Q2XlR1VA76w5lfyGCWiIiI7INFy9mKHB0dMWDAAHz22We4ceMGtm3bhrCwMMyfPx9Lliyx1hjJTCoHBQAgT8MMOREREdkHi5azLUu7du3Qrl07zJs3DxqNxpp3TWZQK5mZJSIiIvtiUTCr1WqxZs0a7Ny5E4mJiSa1sjKZDDt37oRSqbTaIMk8YmY2v5CZWSIiIrIPFgWzL7/8MtasWYPBgwejWbNmpS6eQFVPZcjM5mm0EASBx4WIiIhqPIuC2Z9//hnr16/HoEGDrD0eugdqQ2ZWJwCFOgFKBYNZIiIiqtksmgDm6OiIevXqWXssdI/EzCzAUgMiIiKyDxYFs6+++io+/vhjCIJg7fHQPXBUGAWzGk4CIyIioprPojKD/fv3Y/fu3fjrr7/QtGnTEpO9Nm7caJXBUcXI5TI4KuQo0OqQx8wsERER2QGLgllPT08MHz7c2mMhK1Ap9cEsM7NERERkDywKZr/55htrj4OsROWgQCYKWTNLREREduGeVgCj+4+4pG0eM7NERERkBywKZhMSEvDEE08gKCgIDg4OUCgUJv+j6qOSVgFjZpaIiIhqPovKDJ588klcv34d77zzDgIDA9mc/z6i5ipgREREZEcs7mawb98+tGrVysrDoXslZWZZZkBERER2wKIyg5CQEPaYvU9JNbPMzBIREZEdsCiYXbZsGd58803ExsZaeTh0r9RKQ5kBM7NERERkBywqMxg9ejRycnIQEREBZ2fnEosmpKamWmVwVHFiZpY1s0RERGQPLApmly1bZuVhkLWoDBPA2JqLiIiI7IFFweyECROsPQ6yEmZmiYiIyJ5YVDO7detW/P333yW2b9++HX/99dc9D4osJ9XMMpglIiIiO2BRMPvmm29Cqy15GVun0+HNN9+850GR5YoysywzICIioprPomD2ypUraNKkSYntjRo1QlRU1D0PiixX1GeWmVkiIiKq+SwKZj08PBAdHV1ie1RUFFxcXO55UGS5ohXAmJklIiKims+iYPbhhx/GK6+8gqtXr0rboqKi8Oqrr+Khhx6y2uCo4piZJSIiIntiUTC7aNEiuLi4oFGjRggPD0d4eDgaN26MWrVq4aOPPrL2GKkCVA6cAEZERET2w6LWXB4eHjh48CAiIyNx6tQpODk5oUWLFujRo4e1x0cVJC1nyz6zREREZAcqHMxqNBo4OTnh5MmT6NevH/r161cZ4yILsTUXERER2ZMKlxkolUrUqVOn1NZcVP3YmouIiIjsiUU1s7NmzcJbb72F1NRUqwxixYoVCAsLg1qtRseOHXHkyBGzbvfzzz9DJpNh2LBhVhlHTSBOAMvjBDAiIiKyAxbVzH766aeIiopCUFAQQkNDS7TjOn78uNn3tW7dOkyfPh0rV65Ex44dsWzZMvTv3x+XLl2Cn59fmbeLjY3FjBkz0L17d0ueQo3F1lxERERkTywKZq2ZCV2yZAkmTZqEiRMnAgBWrlyJLVu24Ouvvy5zNTGtVotx48Zh7ty52LdvH9LS0qw2HlsnteZizSwRERHZAYuC2Tlz5pi1308//YSHHnqozIUUCgoKcOzYMcycOVPaJpfL0adPHxw6dKjM+503bx78/Pzw9NNPY9++fXcdQ35+PvLz86V/Z2RkANBPZNNoNGY9D0uI912Zj1EaBQQA+m4GVf3YNVF1HUeyHh5D28djaPt4DG1fVR/DijyORcGsuSZPnoyOHTuibt26pf49OTkZWq0W/v7+Jtv9/f1x8eLFUm+zf/9+fPXVVzh58qRZY1i4cCHmzp1bYvv27dvh7Oxs1n3ci8jIyEp/DGPxOQDggMycPGzdurVKH7smq+rjSNbHY2j7eAxtH4+h7auqY5iTk2P2vpUazAqCYNX7y8zMxBNPPIFVq1bBx8fHrNvMnDkT06dPl/6dkZGBkJAQ9OvXD+7u7lYdnzGNRoPIyEj07dsXSqWy0h6nuBt3crDw1H4IMgUGDepfZY9bU1XXcSTr4TG0fTyGto/H0PZV9TEUr6Sbo1KD2fL4+PhAoVAgISHBZHtCQgICAgJK7H/16lXExsZi6NCh0jadTl8b6uDggEuXLiEiIsLkNiqVCiqVqsR9KZXKKjkYVfU4Ile1/rnmF+rg4OAAmUxWZY9dk1X1cSTr4zG0fTyGto/H0PZVZfxkLotac1mLo6Mj2rZti507d0rbdDoddu7cic6dO5fYv1GjRjhz5gxOnjwp/e+hhx5Cr169cPLkSYSEhFTl8O9L4nK2OgHQaK2bGSciIiK631RrZhYApk+fjgkTJqBdu3bo0KEDli1bhuzsbKm7wfjx4xEcHIyFCxdCrVajWbNmJrf39PQEgBLb7ZXYzQDQt+dydKjW8xUiIiKiSlXtwezo0aORlJSE2bNnIz4+Hq1atcK2bdukSWHXr1+HXM6AzFwqB+NgVge3ahwLERERUWWr1GA2NDTUrJqHqVOnYurUqaX+bc+ePXe97Zo1aywYWc0lk8ng6CBHQaGOvWaJiIioxrMo5Vm3bl2kpKSU2J6WlmbShuvs2bOsY60GYnY2T8NVwIiIiKhmsyiYjY2NhVZbMlDKz89HXFzcPQ+K7o1aaVjSVsPMLBEREdVsFSoz+OOPP6T//vvvv+Hh4SH9W6vVYufOnQgLC7Pa4MgyYmY2v5CZWSIiIqrZKhTMDhs2DIC+LnPChAkmf1MqlQgLC8PixYutNjiyTFGZATOzREREVLNVKJgVFygIDw/H0aNHzV6Fi6qWVGbAzCwRERHVcBZ1M4iJiSmxLS0tTer5StWrqMyAmVkiIiKq2SyaAPbBBx9g3bp10r9HjhwJb29vBAcH49SpU1YbHFlGXAWMwSwRERHVdBYFsytXrpRabkVGRmLHjh3Ytm0bBg4ciNdee82qA6SKE1cBY2suIiIiquksKjOIj4+Xgtk///wTo0aNQr9+/RAWFoaOHTtadYBUcWpmZomIiMhOWJSZ9fLywo0bNwAA27ZtQ58+fQAAgiCU2n+WqpaYmc1nZpaIiIhqOIsys4888gjGjh2L+vXrIyUlBQMHDgQAnDhxAvXq1bPqAKniOAGMiIiI7IVFwezSpUsRFhaGGzduYNGiRXB1dQUA3L59G88//7xVB0gVJ00AY2aWiIiIajiLglmlUokZM2aU2D5t2rR7HhDdO7WSmVkiIiKyDxYFs6Lz58/j+vXrKCgoMNn+0EMP3dOg6N6wNRcRERHZC4uC2ejoaAwfPhxnzpyBTCaDIAgA9MvcAuAksGpWtJwtjwMRERHVbBZ1M3j55ZcRHh6OxMREODs749y5c9i7dy/atWuHPXv2WHmIVFFFy9kyM0tEREQ1m0WZ2UOHDmHXrl3w8fGBXC6HXC5Ht27dsHDhQrz00ks4ceKEtcdJFSC15ipkZpaIiIhqNosys1qtFm5ubgAAHx8f3Lp1CwAQGhqKS5cuWW90ZJGiMgNmZomIiKhmsygz26xZM5w6dQrh4eHo2LEjFi1aBEdHR3z55ZeoW7eutcdIFVQ0AYyZWSIiIqrZzM7Mnj59GjqdPtP39ttvS5O+5s2bh5iYGHTv3h1bt27FJ598UjkjJbNJrbmYmSUiIqIazuzMbOvWrXH79m34+fnhueeew9GjRwEA9erVw8WLF5GamgovLy+powFVH7bmIiIiInthdmbW09MTMTExAIDY2FgpSyvy9vZmIHufYGsuIiIishdmZ2ZHjBiBnj17IjAwEDKZDO3atYNCoSh13+joaKsNkCpOxdZcREREZCfMDma//PJLPPLII4iKisJLL72ESZMmSR0N6P4iZmY5AYyIiIhqugp1MxgwYAAA4NixY3j55ZcZzN6nxAlgbM1FRERENZ1Frbm++eYba4+DrIituYiIiMheWLRoAt3filYA00kt1IiIiIhqIgazNZCYmRUEQKNlMEtEREQ1F4PZGkicAAYAuWzPRURERDUYg9kaSOUgh5ezEgBwIzWnmkdDREREVHkYzNZAMpkM9f30nSauJmVV82iIiIiIKg+D2Roqws8VAHAlgcEsERER1VwMZmuo+mIwm5hZzSMhIiIiqjwMZmuo+v76YDYqkZlZIiIiqrkYzNZQ9QyZ2diUHBQUciUwIiIiqpkYzNZQAe5quKocoNUJiE3Jru7hEBEREVUKBrM1lEwmk7KzLDUgIiKimorBbA1Wjx0NiIiIqIZjMFuDiR0NothrloiIiGooBrM1mNjR4EoC23MRERFRzcRgtgar56tfBSw6ORtanVDNoyEiIiKyPgazNViwlxPUSjkKCnW4kZpT3cMhIiIisjoGszWYQi5DXR92NCAiIqKai8FsDVfbywkAcDsjr5pHQkRERGR9DGZrOF83FQAgicEsERER1UAMZms4Pzc1ACAxM7+aR0JERERkfQxmazg/d31mlsEsERER1UQMZms4PzcxmGWZAREREdU8DGZrOKnMIIOZWSIiIqp5GMzWcGKZQXJWPhdOICIiohqHwWwNV8vFETIZoBOA1OyC6h4OERERkVUxmK3hHBRy1HJh3SwRERHVTAxm7UDRJDDWzRIREVHNwmDWDoh1s0mcBEZEREQ1DINZO8D2XERERFRTMZi1A1wFjIiIiGoqBrN2wFfMzLLMgIiIiGoYBrN2gGUGREREVFMxmLUD4gQwlhkQERFRTcNg1g4Y18wKAlcBIyIiopqDwawdEGtmCwp1yMgtrObREBEREVkPg1k7oFYq4K52AAAkZbFuloiIiGoOBrN2ws/dUGrAjgZERERUgzCYtRNc0paIiIhqIgazdoLtuYiIiKgmYjBrJ1hmQERERDXRfRHMrlixAmFhYVCr1ejYsSOOHDlS5r6rVq1C9+7d4eXlBS8vL/Tp0+eu+5OemJmNz2BmloiIiGqOag9m161bh+nTp2POnDk4fvw4WrZsif79+yMxMbHU/ffs2YMxY8Zg9+7dOHToEEJCQtCvXz/ExcVV8chtS6CHEwAgPp3BLBEREdUcDtU9gCVLlmDSpEmYOHEiAGDlypXYsmULvv76a7z55psl9l+7dq3Jv1evXo1ff/0VO3fuxPjx40vsn5+fj/z8okvrGRkZAACNRgONRmPNp2JCvO/KfIyK8HXVH+rb6bn3zZhswf12HKnieAxtH4+h7eMxtH1VfQwr8jgyoRqXhCooKICzszN++eUXDBs2TNo+YcIEpKWlYdOmTeXeR2ZmJvz8/LBhwwYMGTKkxN/fffddzJ07t8T2H3/8Ec7Ozvc0fluSlg/MOe4AuUzA4o5ayGXVPSIiIiKi0uXk5GDs2LFIT0+Hu7v7Xfet1sxscnIytFot/P39Tbb7+/vj4sWLZt3HG2+8gaCgIPTp06fUv8+cORPTp0+X/p2RkSGVJpT34twLjUaDyMhI9O3bF0qlstIex1yFWh3mntgBnSBDxx69pVXB6O7ut+NIFcdjaPt4DG0fj6Htq+pjKF5JN0e1lxnci//973/4+eefsWfPHqjV6lL3UalUUKlKBm5KpbJKDkZVPU754wD83NSIz8hDck4hgrxdq3tINuV+OY5kOR5D28djaPt4DG1fVcZP5qrWCWA+Pj5QKBRISEgw2Z6QkICAgIC73vajjz7C//73P2zfvh0tWrSozGHWGAEe+oD/VhongREREVHNUK3BrKOjI9q2bYudO3dK23Q6HXbu3InOnTuXebtFixZh/vz52LZtG9q1a1cVQ60RAg3BbHx6bjWPhIiIiMg6qr3MYPr06ZgwYQLatWuHDh06YNmyZcjOzpa6G4wfPx7BwcFYuHAhAOCDDz7A7Nmz8eOPPyIsLAzx8fEAAFdXV7i68tL53YiZ2dvsNUtEREQ1RLUHs6NHj0ZSUhJmz56N+Ph4tGrVCtu2bZMmhV2/fh1yeVEC+fPPP0dBQQEeffRRk/uZM2cO3n333aocus0pyswymCUiIqKaodqDWQCYOnUqpk6dWurf9uzZY/Lv2NjYyh9QDRVgWDjh9j0EswkZeZix4RSe6BSKfk3vXtdMREREVNnui2CWqoY1MrOR5xOw70oyBAEMZomIiKjaVftytlR1jINZnU7A8et3sPti6csGlyUlqwAAkJyVX86eRERERJWPwawd8XNTQyYDCrQ6xKXl4onVh/H0t0dxIzXH7PtIzdYHscmGoJaIiIioOjGYtSOODnL4uOoXkPj56HVkF2ihE4D/rqWafR8p2fogNjU7Hzpdta2ETERERASAwazdEUsNfjpyQ9p2/Fqa2bdPNQSzOgFIy9VYdWxkO26k5iBPo63uYRARETGYtTcB7vpgVgxKAeD49Ttm3z7FqLyAdbP26d/oFPT4cDfe+f1sdQ+FiIiIway9ETOzAOCm1jezuBifiZyCQrNun5LNYNbe/XHqFgQB2HrmNgoKddU9HLpHSyMvY9J3/zHTTkQ2i8GsnRF7zQLAsFbBCPRQQ6sTcPpmerm31ekE3MkpCmZTqmkSmCDUnFrd9BwN/jh1C/mFthFICIKAfy4lAQCyC7QVqreuSpfiMzF9/UkkZt6/C4TEp+fh2DXzr4pUBp1OwOf/XEXk+QQciEqu1rEQEVmKwaydMc7MDmgWgDZ1vACYV2qQkaeB1mjSV3VkZr89GIv27+/A+VsZVf7YFZVTUIjj1++UGXznFBTisVX/4qWfTmDDfzereHSWuZqUhbi0XOnf/1xOqsbRlG3ZjsvYeDwOX+2Pqe6hlOnZ7//DiM8PIioxq9rGkJCZJ2XXD11NqfTH02h1SLGxKzo3UnMw6bv/cKIC5VhEVLUYzNqZEG9nAICnsxIdwr3Ruo4nAPMmgRmXGADVk5ndePwmkrMKsOdyxfrjVof3tlzAI58dxN/n4kv8TRAEvLbhNC7c1gfll+Izzb5fQRDw9u9nMOu3M2Zlqa8kZOLLvVetkv3dY8jKqpX6rw4xS1sZcgu0FmfhxSsNp26kWXFE1qPVCdKxP3fL9KpIoVaHF348jrd+O1Pp47iWUtSW71B05Qezb/92Fh0W7MTZuPKvBFWGHecTsPtSxb47vj4Qg8jzCXj797M16qqQPbialIVXfj5RofaTZJsYzNqZNnU8MXNgI6wY2wZKhRytDZnZE9fvQKsTcO5Wepl1kMWD15Tsqs2waHUCLiXogz5b+HI6fTMNAHA01jSjIwgCPtp+CVvO3Ja2Xa/A84nPyMMP/17H2sPXEZOcXe7+8/48jwVbL2Lb2ZJBdUWJmdhnutWFTKavt76XFeXKsv9KMhrP3mZRZjU1u0DKHp+Ny7gvW8jdSsuFRqsfV2yy6bHffSkJW07fxo+HryPdSh1Ddl5IwMRvjiAxw/RYGb/vzt/OQFqOdU5Qk7PySwR+2fmF+O1kHLQ6AXuvVH1G/0ZqDiZ9/x8mf3cMWfnmzREAgOOGUpBztzJKfJbp/vbBXxfx+8lbWL0vurqHQpWMwaydkclkmNwzAl3r+QAAmgW7w1EhR0p2Abp9sAuDP9mPkSsPllprmFoseE3KrNrM7LWUbORp9IF2RYK/6iAIgpT1Ms663skuwDPf/ocVu68CAB5uFQQAuHHH/OdjHPz8V8qP643UHJMfazEDeDnB/OxvaXILtDgco6+RHdY6CC1rewIAfj8ZhynfH0PX/+3C5lO3rJK9ErNnP/x7rcL3d8Yo65eVX4hoMwL+qmb8/r2WYjq+n49cl/77llFJx734dHcUdl9Kwvf/XjMdh1FmVhAgHd978dOR62j33g6s/++GyfY9l5KkE+WLt+/tvWiJP0/fhiDoF40x97OQW6DFOaOSpm8O3L9lK2QqK78Qewwn3xeq4f1GVYvBrJ1TOSjQNNgdAHDbkGE7dTMdwz49gDPFJoWJZQYOcpnh3+ZlZq11ae6iUVBofHn0fqHVCdJzTcvRIDNPH1BejNf/GOYUFGL4Zwew82IiHB3kmD+sGWb0awgAuJmaa3YGMdYo+DkSaxp8nLmZjgcX78HUH48D0GcpxdXaimcAK+rf6BQUFOoQ7OmECF9XPNDQFwDwv78uYtu5eMSl5eLFn07g+bXHK5T5Ko2YcY5NyalwMHrGkBGX/h1XOfXVG4/fxMiVBy3KTBu/f2OMjuft9FyTy+DWCGa1OkE6odpVbPnqa4ag2tFB/1NgjbrZbw/GAtB3vTBmXG4jfiZK88epW3jo0/24WYETPHNsNhrPZTPLek7dTEOhToCLowKA/jlYe1y25mpSFqatO3nfvw67LyZKJ08X4jNYIlLDMZglzOjXEAOaBmDRiBbY9kp31PVxwa30PAz9dD+eXnMUxwwz1lMNQVG4jwuA8mtmNVodhi7fj4c+PYDcgnuv17x4u+gHUH+Z9v5pC5WUmY8O7+/AC4Yg8ppR5i05qwBJmfn4NzoFsSk58HF1xG/Pd8ETnUIR6KGGQi5DgVaHxEzzTg6Mg9n/igWz3x6KhUYr4GCUPvA0nlx0rxlK8dJwz4a+kMlk6NnAV/pbWC1nTOoeDge5DH+djcfne6Lu6bFijca680JChW4rZmbFAOR0JdRnxiRn482NZ3A09g42nYyr8O2vpRY9P+PAdv3RmzA+p4mzQjB7LSUbOYbP37lbGSbB93XDe2lA0wAA+hOWexGVmCmddB67dkcKJgoKddhtFEhfTcous4Z71d5onL6Zjk0nb5X694oQT6qiErNw3uj745KZmVmx28QDDf3QtV4t6ASUyG7bmyWRl/HbiTh8tudqdQ/lrozLqjLzCnHzjnWuctD9icEsoWs9H6x8oi1GtQ9BowB3/PZ8VwxuEQiZDNh5MRGPrjyEm3dypMxsA383AOV3Mzgbl44zhv99ZkZwcyleP1FJW0aG8oJRNkUnAHH30ZfTvitJSMkuQOT5BBQU6kpcOr4Un4kjMfofxt6N/NE0yAMA4KCQI8hT32HC3NIJ40AvNiVHKgnJyNPgz9P6AKBAq8OVxEyTy6mxydn3lJ0QO0i0C9XXWbes7YnR7UIwpkMdbH6xG2YNboL3hzcDAOy9bHmbp0KtzuS12HGhYhN2xCsKD7cOBmD9YFYQBLy18YwUqJ21oLOGcc13anYB0nP1nULWHdWXGIhdR6wRzBa/xGqc+RVf51HtQgDor34s/OsCHl5xAFtO30ZFbT5VdJs8jQ5n4tIAAAevJiMzvxC+biq4qx2g1Qm4mljy5Cq/UCtlbY2DT0t8ufcqWrz7N97787yUlXVU6H/yzJ1wKZ4stg31wsQu4QCAX4/dvC/rsKuCTifgoKGFW/ETaWs4GJVs0fuuuNwCrXQVwlVV1E/9frT9XDxmbzp7XyVnbBGDWSrBw1mJFWPbYNerDyCsljMEQT87XFw1rL6/KwAgp0B718UWjOs5v/gnGleT7t6C6M2Np7Fg60X8dbb0LzPxB0imr3KotLrZtJyCCnUXAIoyOBqtgKjELJNaREB/WfVIjD7r1T7c2+RvdQwdJsyd1CaWC4ivg/g6bzp5S6opBvQnE8aZ2VyNFgkZlk/au2K4L/FkRi6X4YNHW2DhI83hplYC0GewAODsrXSk51g2eenmnVwU6gSpnOXYtTtmT0xKzsrHLUPmcWyHOgD0wdzd1nYQBAE/Hbl+10vfxjb8d9Nk5n/xbgTmKF4mcy0lGwevJuNWeh48nZV4vFMogKITtvQcDb47FGt2+UZKVr60CIJYM61U6F9P8Uc+I0+DO4Zj1KqOJxoajusX/0Tj1I00fF3B+lBBEKSTKTGA+DdaH/CIJQb9mvijUaC+rKm01/tSfKY0Me7iPQSziZl5WBp5BToBWL0/Bit260+mx3bUvyfMqZnV6QQcv54GQB/M9mzoCzeVA5KzCnCqWCmLrblwOwObT+knRlWk5di5WxnSe+ZyQhbuZFtn3kRmngavbTiFsasP44UfjyO6nN+K8vxzORG5Gi2CPZ3Qr4k/gKLPQXFZ+YVYvS/a4laTeRotTly/Y/FrMeePc/ju0DXsrOBJO5liMEtlCvdxQQdD4HUxPlOqkQ2t5SzV2N2t1ECs51Qr5SjQ6jBn07kyM4PZ+YVSO6XS+m5m5RdKwWvrEE8AppfyRYIg4GxcOv48fQuFZp7pHr9+B+dvZUCrE7Dx+E30WLQbAz7eW6EWPsbN7y/czpDG5my41H3yRpp0+btDmGkwG+KlD2bLCs4FQZAuDet0gnSJupthEt/R2FR9QHZYn9Wr5eIIQH+5/Uqi6Y92dPLdfyTyNFqs2B1VoktCclY+UrMLIJMBEb6uZd7e312NCF8XCILlrZ7Ex67n54qG/m7Q6gSz+9mKr3FdXxc0DXKHh5MSGq2A23c5T9h9KREzN57Bm7/evRVWfHoe3vz1NGYaWmZN7lFXGm9FaoQFQZBOdrwNxyomOVtqczagaYBUyiPWzH72TxRmbzqHOZvOAdDXwU767j+M//pIiYzOwahkdP7fLjy15iiAogznI61rA9B3isjTaKUx+Lg6wlXlgFHt9dnZFrX1Vw2uJGRWKJN/MT4TV5Oy4eggx5Se+tfm3+gU5Gm0iDyvLxUZ0CwAjQPcpP2LM168JSY5G3kaLfI0Wjy55hg2xpr/c/XZ7qvI1WgR6KGGTAYU6gQ4OsjxfK8IyGT60p/ygperSVlIz9XASalAkyB3KBVy9DDUiRevPbaWzDwNpq8/WW7nEeOTlYradvY2Bn68Dy/+dALvbbmAEZ8fxK/HzOtzvS/K9HP4n+F77+DVZIvrrXMKCvHwigPYYDSGez1Z2HpG//oNah6AxoaTp7KC2fe3XMB7Wy5gSeTlCj/O6ZtpGLBsL4Z/dhCt50eiz5J/KvS7cSstV5qrYu7JNJWOwSzdVcMAQxbldoYUuNZyUcHXVQVAH+Qcjk7BF/+Y9jHV6QTpMtQHI1pA5SDH/qhk/FnGJaQT19Ok8oLiWU2gKCvr765CS0Mwa5zJTMzMwwfbLqLr/3ZhyPL9mPrjCaw2o63T7kuJeOSzgxj0yT40nbMN09efQkZeIQQBWLL9slk/5pl5GpNMz4XbGdJzECdJbT+XAI1WQIC7GiHeTia3F3v/ltXR4OsDsei0cCd+PXYTCZl5yNPooJDL8Egb/WX0o7GpOBp7B+dvZ8DRQY5X+tQHoG9LdSVBH7z6uBYFTQDKvKS1el80Pvz7EuZuPmeyXZwwU8fbGU6GAL0sYqeMQ1ctKzUQxxju44LejfWZ3tJ69ZZGLDFoEewBmUwmBWbXsmRl3kbssXwxvuw2XnFpuei39B/8fPQGtDoBQ1oE4rX+DRHooYYglP1DKdLpBGw7ext3sgtwJ0eDTEPwK56QXEvJwQFDMNClng+CPPXvkVtp+h+60zf0z+uPU3FIyMjD5lO3EHk+AXsvJ+G340U1u7cMk/AKCnU4eDUFscnZ0thGtK0Nf3cVcjX6rhRidlh8/z3dLRzRCwZh/eTOkMuAjLxCJBnquItfrcjI02DF7ii8+etpTP7+P0z5/pjUF/eBBr7o3VifDTt27Y4h61WAQA81OtWtJX2nXLitn5Tz24mbUt/Z00ZBjE7QZ1D3Xk7Cgasp+Oe2XPrhv5sbqTlYe1hf17p4ZEssG90Kjg5yjGhTG35uaulKSHmTwMQT1JYhHlAayhMeNFx5sCSLlpGnwdzN5+666tu6ozew8XgcZm48XWawGp2Uha4f7MKr609VeAwA8PsJffY8wtcFHcK8oROAVzecMqsWeP8V/WdarEc/GpuKG6k5eOKrI3jymyPItmDi5+HoVEQnZcPLWSl9Hs7ew6TNPI1WqrMf2DxQCmZLO3lKyynAbyf0QXRFg/H1R2/gkc8OIjYlB05K/esRlZiFZRUIio3fCxW9GliW7PxCLN95Bf/76yI++vuSNOelPIejU7DmQEyZZX73OwazdFdiFuVSQqZUZuDt4ohahuAoKTMfL/50Agv/uojHVx+W9rmalIU7ORqolXIMbBaI5x+oBwCY/+d5ZOaVvPxsPCu/tIyreNbaKMAdoYYfo2sp+hrQD7ZdRLcPduPzPVdxKz0PCsPl6Z+PXC83GF37rz6bKZPpa/yUChmm9qoHJ6UCZ+LS8c+V0gMysb43v1CLUzfSTSbtXIjPkLKn/Q0TawoMwWP7cG/IZKaBVUg5ZQY/GVo1bTxxUyoxCPFyQqe6tQDoL/2NXfUvAGBQswB0q+9r2J4uTSrr3UgfXMQmZyMxIw9d/rcLvRfvKXGJUTzZOBKTahLwisG6WGJwN10i9OM6YGGmxjiY7Wd4/baeicf09SfLzYCKmdlmwfogVmwfdrdgVqypzdPocCtdnwmNTsrC+v9uSMHtxmM3kZFXiLq+LvhlSmd8OrYNHBRyqfa5vEUANp++hSk/HMdrv5yW6qn93VVoaPh8Hb9+Rwo6O9ethWBDMCuu0CVOWNJoBXy1PwYf77wi3fenu6Og0eqQX6jF82uPmyxu8v2/16QAsHGgGx5spA/Gtp+Ll64EiJ8nQF86olYqEFpLnxkWS0smrjmK/sv24plv/8Omk3EYuGwfPvz7En4+egN/n0vAtnPxOGG4JD+0ZRAa+rvB01mJnAItlu7Qj3VGv4ZQKuRoFGj4TonPxG8n4jBt3Sk8+c1RFBTqpMysWNt64XaGyTK728+XPxlw2Y4r0GgFdK1XC13q+eDhVsE4NbsfFhjqucVyiuKTwM7GpZusTiZmHdsaasQB/cmpTKbPdt9Or1g984rdUfjmQCze+f1smfuIGdk7ORpsPVP6iX/k+QTkaXTYdyWpwjXweRqtdJXj48da4+dnO+HJLmEAgHd+P3vXk7LcAq1U0jSxq75++EhMKr47FAutTkB+oc6iLjPilYMeDXwxzFDnfuYe6tz3XUlGdoE+K9+qticaG95vsSnZJcrifj56QyrNijF8NwL61+luJXRanYB5f55HoU7AwGYB+Hdmb+yZ8QAA/fdJqpklB8Yrb94tmM3OLyy3TE+0fFcUFkdexsp/ruLT3VF4+tv/yuwdL7qVlouJa47i3c3nsWDrBbMe537DYJbuSvyxvZ5aNAGslqujdCn7n8tJUsB0NPYOHvnsAG6k5kjBaesQLzg6yDG5Z12E1XJGYmZ+qZdzxHpSoPS2W2JfykaBbqhTS7wsn4sDUSn4fM9VFBTq0KaOJ1Y+3gZHZ/WBi6MCsSk5OBp7R6qJnPrjcYz58l9M/v4/pOdqkJiZJ10S2vZyD2x7pTsOvPkgZvRviCc662sWl+++itJ+L2Ya6ntX7omWzq7r+uoDgNM306Xa1G71fKSzdgBS2YaxoprZkj+OUYmZUtnF0ZiigCe0lgsCPZxQ28sJgqC/jPpgIz+8PaQJQr2d4apykGoPgzzUaGbIUMYkZ+OPU7eQlJmPq0nZGPH5QSzboc9AG89EzynQmlzyvSzVy5ZdYiDqVLcWZDJ9lqJ4k35ziMFsmI8LWoV44rX+DSGXARuPx+GhT/eX+UNRqNVJx6KFIYjtWFf/ep9IkZWa1RPLUkRXk/SP/ebGM3j9l9NYaziR2GyoBZ3SMwLtjMpEmhna2pWXSTpi6N+651Ki9LqGersgzBA07jUEGA393eDrpkItF0c4OsghCPofduPnvGpfNGKSs/UnlS6OuJ6ag7X/XsOk747h5I00eDgp8dKD+pNHMdtWx9sZbmolhrTQ9zXeeDxOyoLWMYzBWD0//XG+kpCJjDwNThpWUttxIQEv/3wScWm5qOPtjGl9GmD+w00x7+GmmNanAWYPaYLBzQMhl8vQ3vA6aXUCmgS6Y7ghUBFPiBIz8/H+Fv0PZ3JWPjafuiUFz/2ainWOmdhnFMxuO3f3YPbMzXRsNGTaxLZ3AODkqJBOIsXvNOOrKVvP3MaQ5fvx/Nrj0rajhu+wdqFFx7uWq0oqczIuNRAEAQeikks9UQf0ZVI/GsqAzt/OKPXENSEjTwqggbK7JogdJzLyCpFWRl16Vn4hBn+yD23mR6LXR/pWffmFWuy/kizVkjYNcodcLsOcoU3Qx3AF5G7lBkdiU1Gg1SHQQ43RhpKUs3HpWHe0qJ+wJfMYxGC2caC79Hk6f8vyxU7+MpwEDGgWALlchlquKvi6qSAI+uzs94di8eXeq8jM0+A7Qxs5MQFyOCYVhVodhq04gAc+3GPyuYtJzpZO8K8benmrHORYPqY1PJyVCPNxQaMANwiCfkKwOY4bHe+YlOwyu/688etp9Fnyj8mJXWnyNFppEumwVkHwdFYiLUdTbsnXu3+ckzqefLU/Rkqg2BIGs3RXtVxV8HHVfxGIlx+8XRzhYygzEHtJtqnjiWBPJ8Sm5GDqTyekSzbtw/RZDbVSgXkP6zMj3x6MNZk0U1Cok7I6gP6HTczAnbmZjs2nbknBQOMAd9Tx1v/43kjNkdoijWpXG78+1wUDmgXC28VR+tFe/98N/HTkBmZuPIM/T9/GoegU/H0uAfP/PI/fjutXI2pTxxMNA9zQKMAdfm76WeSTuteFWinH6ZsZ+P2aHCdvpElfrum5RT/uq/dH4x/D0rqPtQ+BXAapv6yb2gHeLo4mAWDxelmgKJiNz8grcWnRuHauQKuTfjjEmsq3BzfB4BaB+HFSR3z9ZHv4uKogl8vQNMhdul09fzeEGwKW6ORs/GW4zwhfF+gEfSZr65l4bDlteinfuE3TlQpkZj2dHaXH//V4HD7ZeQVf7r1abnZAJAazdQ3P8YVe9fDzs50R4K5GdFI2Xv75RKmXwv6NTkVqdgG8nJXSMs3d6vmgXagnNDoZFkdeKXGbW+l5Jj9YUYlZKNTqpGVwv94fg4vxGbickAWlQob+TQJMbt/MkJktbxKYGDAX6gRpVbM6tZwR5qM/9uLT6VJPn9WWy2VSdnbXRX0AF+7jgnAfF+nkanKPuphkqNt9d/N57L2cBCelAp+ObY2nu9eFUiGTXnMxO9UlohaaBrkjV6OV3gd1jDKzovpiMJuYhVM30iAI+kyymJkc06EO/nq5O17uUx9PdA7D+M5heLlPfTzVLRxyQ2DQ0ejE7e3BjaXtrioH6TGNs8j/23YRWp0AXzeVNJHwn8tJiE7KhuGmOHY9rcwTJEEQ8O7mcxAE/Q+5uLphceJ7WDxxS80ukLKlR2JTkZZTgPj0PFxLyYFcBrQNM70fMbtt3Grs56M3MG714TKzWuuP3pC+F4DSy2bEbfX8XKFUyHDielqJjH+hVmcysTY2pfR2eweiknHuVgZSswsQk5yNP0/fxlf7Y7D9vP4x+jbxl4J7mUyG0e31E+M2nSp7roEYSHWr54PaXk4I9FCjUCcgw+h5XU81HY8gCPj9RBzm/3keL/x0Et9dkWPdfzdNgvkLhm4gTQLdUc/XFSoHObLyC0u9Qlf89er10R6T7GZBoQ6RhhKDQc0Dpe1iqcHMX8/gnU3nsGCrviTtVnoevF0cpeD8SEwq9l1JxsX4TCRm5ks9k78/FIteH+3Bx4arDGJSoWGAGxwURWGU2K6weDeXvZeT8NCn+016t+dpihbkEE9ci89xAPTHfNfFRAgC8N2h2BJ/z8zTSN9hm0/dwp0cDYI9nbB4VCvpNdhWxqRq8XXcfj4BDnIZRrXT19W/8/tZqc7dVjCYpXI1CigKYFxVDlA5KFDLEMyKX9CPta+D9VM6w13tgFM30qTL1cYz93s08MWg5gHQCTBZpvRMXDryC3XwclbC01k/K/56Sg6iErPw0Ir9ePGnE9IlwUaBbqjt5QSZTJ99EDNmI9rUNrl8P9Lwofzz9C3M+1Nf/zm2Yx3MHtIEMhnwy7GbUp9EsS2RMV83FSYYLr/tuS3HyC+P4KWfTwDQ1xaJwUdmXqE047lLhA/qGk2OCqvlAplMhkaGGkEPJ6UUJBjzclZKNWjFWzFtM/zAiROFxNchzJCdHtAsACvGtkGXCB+T2zU3XGYH9IFJuCFrfC0lR8pe/jipE17oFQEAmPfnOenEoKUhiysGs4JQ1HS/vl/5waz4WgDAB9suYknkZSzYehEPfbq/1MvxN1JzMHfzOSz86wJyC7TSpf4wn6KMYYdwb3z7VAc4KRXYdyUZy3aUzO6LM+kHNAuUahxlMhlmDWwEGQRsOnW7RFlF8QUWriZl4UpiFvILiy49vmGYGNazgS88DO9PkVjOcCUxq8waR41WZ9JWzvjyfmixrGhXo+MotmwT6zObBLrjqW76y7s+ro54onMonugUCi/DmHzdVFg3uRO61/eFh5MSPRv4Sfcl/pjLZDI8awiARaG1Sglm/YuCWbGmuGN4LayZ2AEX5g3Awkeaw8XQsaAsA5oFwF3tgEdaB6NLPdP3p/F3yluDGkEmg1Sf27K2hxR8iyc2zYM9EOYqQBCKPhPF/XHqFo5duwNnRwXeHNi4zHGJj305Xj/Bbe7mc1JQLQj6k6LDhitFTYLc4a42PeYPGkp29kclS5k0cbUzcRKfMa1OkDpDiCd5pZVL/GWYtDSqXW0MaKYPQn4olp09fztDqrcGyg5mxZOxQc0D8NagRgCA5Tuj8Lchsy1mvkU9G/jCy1mJpMz8MsuDxKsH3er7QCYryrwDRTX5xTOz+64k45V1Jw2BdCKOJcvx9qbz6PnhbhyJSUVOQaG0aEjjQHc4KOTSe/VMXDoEQcD+K6VnvMXlvJdsL/ouOHA1GZl5hfBzU6Gt0cmM+H4Svz993VRSED62Qx30qK9/fx6JScWGY0WZ5m8PxeLmnRws+vsSAP2VCaAomG0cUJQ0APS/cfrnbVoCsuZgLE7fTMf8P89L207fTEehToCfm0pqd1haXe+5WxlS1nTnhUSTUphbabnovfgfdFq4E9vOxuNbQ7D7eKdQKOQyDDK8j/4+l4BCrQ5xabn45kCMVEKRnV+Id//Q/z4+26MuPhjRAsNaBaFQJ2Dy9//Z1Ip3DGapXMY/PGJQJX55iR5o6ItgTyd8MKKFtE0hl5XIjozvHAZAf4lOzABIl/PCvKUf9+up+lZFggD4uKrQvb4PnnsgAg393aBWKhDgrv+hz9PoL3u1L5bxbBvqhbo+LsjT6JCn0aF7fR+893AzPNUtHE8b6r3EmcqDWwSiNK/3b4T/DW+Klt76cW45cxu303OlDIVxEODsqECjADfpixiAVA7RIkQf8HSuW0vKThmTyWRS3azxj8GN1BycjcuAXAa82q+ByW1CfUpeGjbWvHZRMNvA3xWB7mqoHORSRrNtqBf83dV48cH6CK3ljISMfEQnZ0OpkOHtIU0A6Ft+FRTqF3PIyCuEXFZUSlGevk2Kfiy7RNSCt4sjLsZnYsjy/ej10R5MX3cSb/xyGpO++w8PfLQH3xyIxRf/RBtKHvRZbbGURdQwwA3/G9EcgL4u7IjR0qsFhTop0zi02PFsFuyO9r765/3uH+dMMsRibZ6bWh+YXU3MKlGvJwYGYrbfmL+7viTAeJWt4i4nZKKgUCd1ABHVqaUvB/F1058YymVAh7pF7+MgD31mVvyBaxjghjHtQ/DWoEb4cnw7ODs6wEXlgE/HtsGYDnXw+wtdpfIKABjasuh1aGL0vhzcPFDK+gKmNbMi8aQlKjELJ27oTwDaGLLdaqOymbup7eWMk7P7YfGoliX+Jp5stQ/zwqTuddGjvq/R3zxRz89Vas0GAF0jaqFVLf1xK62W9HJCplSy8EKveggw9OktTZiPC5QKGbILtBj9xb/YdPKW/rU3nHgfuposvbc6htcqcfvGgW4I8XZCnkaHX4/fxPWUHOnK0q30PJNV2wRBwI+Hr+HmnVx4OSvx8WOtAeh7tBoHJSlZ+VIAPbBZIMYbypx+Pxlnkok+HG06mUesoc/M05icKIrdALrX98Wk7nXRNtQLuRot0nM18HBSlrhC5Oggl97fvx0vWWpwLSUbF+MzoZDLpGMlXnVzdlRgSs8Iw36mwexGw311quuNdwY3woDaOtT10V8R+v1kHC7GZ0IQ9MGl+DkQSw3OxaXj6wOxePyrw3htw2mT+xUEAecMz3d/VLJUilW8xEBk/P6f+1BT7H2tF6b3bYCHWgbhme7h0u/HpYRMKSPp7eKItBwNRn/xr5S0uZSQicw8jdS7Waz/FrUL84KTUoHEzHyTwFT8TjkSmyq9t44Z1WSLpS+lfYccNZpPUqgT8NsJfdIht0CLSd/9h8TMfBQU6vDc2mM4G6efBCxmmjvW9YansxKp2QXYdTERj68+jLmbz0tdUVbsjsLt9DyEeDvhxQfrQyaT4cORLTGmQx3oBGDu5vNYtTdaevz/rt1BbCassgiStTGYpXI1NApmxYlfYpkBoP/y8TMElwObB2KMob9ns2APqd+kqF2oF7wMdTxHDZfLjko/HN5Gk7typHqixzvVwfdPd8QbAxpJ2dcQox/gIS0CSwSJMpkMIw0ZVx9XRywe1VLaZ0b/htIl7EHNA6UeqcUp5DKMaBOMpxrq0C7UE4IA/HYiDvsNwewbAxpJk0la1vY0ZBWKXivxuYxqF4L5DzfFuw81LfVxjJ/PTaNgVrzs2CHcG0NbBkl1XQCksoGyiBOTAKCenxvkcplUmgAAA5vpL5erlQrMNRpX9/q+aBfqBW8XR+RqtDh9M02qLQzzcTE7kGkf5o0/X+yGfa/3wo+TOmH7tB7SQhwxydnYeCIO6/67gcjzCdDqBGlsX+7Tf3HW9XEpMVEOAB5uFSx1cTCu7zsQlYz0XA18XFXoWLdkADKkjg6uKgecupmON389LWVNxKVuxQzG1aRsKSgY0iJQCqhUDnL0aeJf4n5lMhmaGgKzg1dTSs3OnjM8Rts6XtIPNQDpxE08li1qe5pkAYO9TLteNDJc0ny2RwTaGJ0kdq3ng4WPNDcJUAGgT2N/uKoc4CCXmZzcOCjkmNRdf0KnVsqlIMJYhK8rZDL9JXgxQ98m1KvEfuWRy2WlHscnu4Zh5sBGWDGuDWQymdT/FdC3BlM5KExawHWJ8EbLWvpj9m90KgYs24vp605i9b5orDkQg4c/PYDEzHzU9XXB04bsdVmUCrn0nSbW9k99sD6eMpzkHriagsOG76TSatxlMpm0gMLqfdFScCES614vxWdi9Jf/4h1D4PBE5zDU83NFs2B36ATTjgh/nr4NnaD/Lg3xdka7UC+0ruOJPI0On+wqKo8Rj4Wf4ZiJkwnf/v0shizfj90XE6HTCVL3i5a1PSGTyTD3oaZSX+rejf1MLo2Lhhs+V3+fSyjRlUD8LupU1xtehpPMoS2D0K2eD94a1Fi6QmF8Mp6dXyhlgt8Y0AjjO9XBwBAd3h6kr2XecT5BusxuHGyKpTuHY1Kl/sB/n483KU2Iz8gzKVH54d9rSM0ukDLeA5qZlgP1beKPYa2C8OGjLTChSxicHBV4qXd9fDKmNTydHVHLVSXViWu0ApoHe2B6X30CQbxa5uKogCDo2yxeMKrzNaZyUKCT4YRUnGiXkJEnXXUAIC0gJAazbep4SQmj0oJZMfgVEwm/HLuJ3AItXt1wEuduZaCWiyOGtQqSyo8eahkkJZ2UCjn6GjqLvPTzCelKx4ZjN/HdoVis3qfPvM4e0lTqUqNUyLFgeDO81Lu+Yd+iTPWH269g6VkHRN6HPXEZzFK5GhldShGzZbWMMrNiuxrRnKFN8NagRlg4vHmJ+3JQyKW2PdvPxyOnoFA682wf5i1lO6+l5uCEISPWppTaN+Ns0sOtgksd98SuYXilT318+1QHqRYW0AdwX45vi7Ed62BG/wal3ra4R1rrsxbfHbyGq0nZkMn0l4TnDG0CXzcVHuugD5yNv9zE56JUyPFE57C7ZovqFMvM6ozOwAc0DYC7WillxhRyWYlAp7i6hvpKH1dH6YsyzCgANv6yf6ChHx5upX9+o9qFQCaTSV/I/0an4LKhvVcDM0sMRM2CPaQg3cdVhRVj2+DkO/3w9ZPt8GrfBnitf0O8Pbgx/pjaFX+93F2azAaYlhgUJ/ZL3Wn44QaKJmgNbh5gEvSLPByBT0a3gEIuw8YTcVgaqZ/0JpYZPGw4vslZ+VLmvW8TfwwxZHkfbORX4sRMep6GS8cfbLuIRu9sQ6+P9uC1Daeky7JFHRbcMbh5UXZXfA83CND/iIqXOkVBnsWDWdMfzvK4qBzw46SO+PapDgj0ML2v0e3rYEiLQLzcu0GpwaaTowK1De+xPI0OaqW8xA/3vXBTKzG5Z4T0uXywkR/q+rrA01kpfd7FE0MnpQKtQjzhrQL6N9F/11yMz8TGE3F4b8sFvLv5PHI1WnSr54MNkzubdcL1/rDmmNIzAv97pDk2T+2G6X0boFNdb2niopjpK63GHQBGtw+Bh5MSsSk5+PwffXDi764PMI/F6juBTPj6CI7EpEoTYKf20k/K62eou958+ha0OgEHrybjfUOt7UMt9e8PmUyGNwboywN+PnIDMcnZ0OoEKfgWy6hiUnKg0wnYYyhv2HgiDjEp2cjML4RaKZfq9ZsFe+DZHnUhl+lLwkrTOsQTYbWckavRlqjpFYNSsTsLoK+N/+GZjni8U6j0XRd3J1e64hZ5PgG5Gi1CazmjlWHSHKA/QXBVOSAxMx/rDXMAjN9bYmB88kaaVAsqFFtGWJxwqTJc7fjl2E2M//ow0nI0CPF2KnHcnB0dsOyx1lKCozTGNd6j2tXGo21rS1cgBzYLkK427bmUJAW4xcsMAOO6WcPn31An6+umglymv/0X/1yVfvfahHoVtcCMz0R6jgYzN57GtrO3IQiCdHL0zpAmcHSQ42J8Jrr8bye2nomHUiHDyifaYunoVni1bwM0CnDD8w9EmIxHrJsVu/X0NtR8z950DgVa/VVLcQKgSCaT4TFDdjc6KRsFhTrojK4+NQ6o2G9BVbh70RMR9PVzcpl+kkpRmUFRNqdXI9MPglqpwLM9TD9Qxvo18ccvx25i+7kEqBwUyMgrRG0v/exaMQt4/NodXEvJgUymX52oODH4q+vjYjLZqfg4XulTerBaz88NC0oJtssyoGkA5m25iHjDJb8WwR7wcFaiSz0fHJ3VR9rPOMMgTlQzR4ghcPjv2h0UanX46ch1nLuVAVeVAwYbLv/1qO+Lo7F3EOLlJNWElkUul2Hjc12gFQSptlGsm21R2wO1vUwvLS8Z1Qqv9GkgZUg7162FrWf0EwPEpVXN6WRQHg9nJR5s5C/VHRqbObAxXvhRP5s8/C7BbIdwb8NKTPk4HZeORgFuiDT82A5pWbIUQNS9vg8WDG+GN349g092RSEpS9/zVamQoW2oFwLc1YjPyJM6GjQP9kC3ej4I8HCSuluUZkTb2tKlzpwCLWKSsxGTnI0Nx25i/eTOJu3C2tTxwrIdl+HnrpLqw1/qXR9htVwwrqPpYxhnWl2MgsuKMC47MObkqMCnY9vc9bb1/dykDhstgj3Lfc/dC6VCjt9f6IpCrSDVJTcL9sDvJ2+hU11vKWhZ/lhLJOfoJ86cu5WO87cycD01BwOaBeDFB+uXeiJTmpYhnlK/apE4cVEMlBr6u0lZyOJcVA54vFMdrNh9FXkafQnJtD4N8ObGM/jv2h3supiI+Iw8+Lg64o+p3UxOTAY0C8CSyMvYdyUZfZb8g4QMffu1vk38pZZXgL4rSK+Gvth9KQmLtl3E451CkZlXCDeVAwY2C8SK3VdxLSUb0cn6xR0A/aS0robWeM2CPEwysDMHNsb0vg2gcig92JfJZBjSIgif7o7CrouJeKSNPmBOzMiTsoj9ik2AFPm7qeHoIEdBoc5w2doZvxtq8B9uFWxywqRykKNnA19sOXNb+mw0MfoOb+DvBqVCJnVjebhVEDadvIV1R29gWp8GcHJUSFdPBrcIxMnraYhOzsbZOH2W8psnO5SaeS5Ph3BvrD18HY4OcjzUMhhqpQILhjfHhmM3MXtoE+w4n4DfT97Cr4bSiSAPdYkaegDo2dAP2HweR2NTkZGnkZ5j9/o+0OoEbDp5Cwv/ughAPwelWbA7tDrBsJhHPp757iiOxt7BppO38OUT+hIBtVKOrhE+GNA0AH8YJnnV9nLCnKFNpRKJF3vXx4uGbKqxLvVqwcNJifRcDd4e3AQj29XGgGX7cD01Bw6GbhalndAGeqjhpnZAZl4hopOz4KRUILtAC4VMQLhPydKk6sZglsqlVioQVssF0cnZ8HbRB7F1vJ3h7eIIbxdHqZenubrX94VaKUdcWi6+2KufhPXu0KZwUMilS68XpQlHriUmYAD6L8jICwl4/oF6pX4Qrc1N7YD+TQOw6aQ+A1h8QovIz02FcB8X3ErLNSnPKE/Phn5Q/XURJ66nYfr6U1Lbn9f6N5QuAw9vE4z1x25IPzLlKf5DPKJNbRyISpYunxlTFCtDEJ/f6ZvpUiup+mZ0MrgXg5oHoEOYN47EppYINIw5OsjRw/BjuON8Av6LTUVmvv6EqG0ZM9hFo9vXQVJmPj7afllqP9PA301/WdvPRTpZcVU5IKyWC+RyGd4c2Oiu9xnh64o/pnaDIAhIyS7A6Ztp+PbgNfxzOQkf/n1RuiTZ3JCp3vxiNzgbtYryc1Pjme51S9yvcTDbMMCt1HrrylTfz1V6H7Yu5YTS2op/zsd1DEVWfqHU0gvQB1xBnk4I8nQyqcu2lq4RPlIw27Fu6VlZ0YQuYVi1NwYFWh16N/KTOjBcuJ0hTZwZ0aZ2iQx7A383zHu4KZZEXpYu+3ar54PlY1qXOGF4fUAj7LmchL/Oxks14e3DvaVLzmk5GpMWYVn5hVKpTmmfobICWVGPBr74dHcU9kclQ6sToJDLpEv3rUI8y7y6JJfLEOLlhKtJ2biemgMnR/1ETUDfWaK4Pk38sMWo9rmJUXmWo4O+DORsXAYa+Lvio5Etcfz6HdxIzcWmk3F4rEMdqXtIi2APNA/2wNzN5+HhpMT3T3eUygUqqm8Tf/Rv6o9OdWtJQWq/pgFSr2uxzEZsh1bWlYpwHxfU83NFVGIWdl9MlILZFsEeeKChH87GpcPdSakvH2sRJB2TOt7OuGZoJwno2yOKk47FFpev9deXaHSOqIURbWqXqMMvjcpBga+fbI+bd3LwUMsgyGQyLBnVEpO/P4anuoWjXhlX3GQyGRr6u+G/a3dwKT5TGmegMyw6WahsDGbJLE2C3BGdnC1l6VxUDtg5vSfkclmFf2SdHBXoUd8X288nQBCAPo39pHrEsGIzq1uHlB6c1KnljD+mdrPgmVju0ba1pWC2WxnBrEwmw/rJnZGVXyhlsc0R7uOCjx9rjefWHpPanbUK8cTjnYqydbW9nLHv9QctHn89P1ezX7MIX1d8MqY1fjx8DYdjUuGokFtUM1kRMpkMX09sj7Nx6SaX/ErTu7H+x3Dr2dvSj8vUXvXMei9OfbA+vFwc8c7vZ6ETiiYjRfi64kCUviZR7MFZ0fH7uKrwYCN/NApwxwMf7pF+mMTgGDCvvRkAk8ChYQVLDKzBOCgoq81VZXJyLLqyotGU3k/V2jpH1MIXhgkvpdXLGvNzU2Ni1zCs2heN8YYyotpeTrh5Jxf/RovlAKVf1h7fOQwj2tTGz0dvICEjD6/0qV9qeUTjQHc80y0cXx+IhSAIcHZ0wOj2IXB2dIC/uwoJGfnYaFgBzkEuQ6FOQLThykILozppc7Wu4wlXlQPScvQTylqGeEolB8YlBqWp4+2Mq0nZuGboRKPVCWhZ28Okw4uoV0M/KOQyaHUC1Eo5wn1M9xnaIghXErLw9uAmUCrkGN8pDO9vvYA1B2Mxun2IydWOViGekEHfZaGswMwczo4O+OKJdmX+vaG/G5wdFVJngeKTv4wNaBqATxOjsO1svDTW5rU9EObjgp2vPlDm/YsT6Kb0jMDKf65KZRZiV6AQb2d8MqZ1hZ9b21Avk8U/2oV549g7fcu9XYMAfTB7OSETDnJ9ABvkfH+uEMZglswyo19D1Pdzk1ZoAUpm/iqiX9MAbD+fAJWDHHOGFk1A8nVTwUmpQK5hIk2bUE+LH8PaukT4oHUdT2TnF5p8MRRnPDO3IgY0C8CcIU3w7ubzcJDLsPCR5mZfNq0MD7UMwkMtg5CQkYdCnVBiglFlcFU5SCub3U2vhn6QyyD9cIfVcsaItuZlrAF91i/AXY01B2OlEgLjCUfGrc0sEeTphLEd62CNoU9lEwuCY7VSAR9XFZKz8k0mFlYV40z8/fQ5rEwdwr3h4qiARiuU2smguDcHNsKLvetL9dTtQr1w806u9N93yxK6qBzKnawGALMGN8GswU1KbA+t5YKEjKJZ8+M61sG3h4rqSlvd5epGWZQKObpE1MJ2w1LJgR5qqWd4/6Z3z4Trr6ol4XpqjjRRraz5DJ7OjugQ5o1D0SloGOBe4ntucs8IPN0tXMoAjmoXgmU7LuNifCbWHb2BhIx8yGRF7bye7Fr+63ivHBRytArxxEHD63G3GvIBzQLw6e4o7LigX8ZcLgOaBN79O6VHA32C5/kHIvD6gEZIycrHBsMk17JqtyubtFpefCbkhitJwS4MZsmGhfm44OU+JetxLDW0ZSDO3UpHp7q1TDoTyGQy1PF2lvoBljb5q7ooDHWolVnW8GTXcITWcoGb2sGqE27uhb972RPXqouXiyPahnpJmc9pfRtUuKazd2N/aTIiYJqJbHaPwSwAPP9ABH46ch35hTqLg+MWtT2w62JiidZzVaFxoBvah3khwMPJZAJlTebs6IAfJ3VCgVZn1gmpTCYzmRjYNswbvxuu3oxqX/ZkI2sIq+Vs0p7uhV71sPFEHDLzCuHprCx1MQxziEHV3itJSMjUn8i2C/UqNcNqTHy8fVeScO6WvqXgkJaltz0EgOGtg3EoOkWq8S3O+FK2h7MSj3cOxRf/REu9Wuv6uJTb69ja2oZ6mRXMNg1yl7L0gP67RewWUJZxHeugf9MA6X332oCGiLyQALlMVm0nkw2Mln6WQf+7F3T/lcsCYDBL1UTloDDJyBqrU0sfzLqpHUyyZfeDqqjPLT6hjkrXt4k/jsbeQUN/NwwtpQdsRRm/16wRzPq5q/FKnwZYGnkZg5rf/RJtWZaPaY34jLxq+RyoHBTYMKVLlT9udbtbvXZ5OhvqbN1UDhjcvOxAzhqMO37U9XGBn7savRv54feTt9DC0JLLEuJs/OPX06T+uWKt5t2IwazYbqtrPZ+7ngSNbFcbTYLcpQU6yjOpe118ezAW2YbL/Nb4jFaUWGqlVspNusMUJ5PJMKBpAFYbFgdqHuxZ7n3LZDKTEyg/NzW2v9IDAvQnWdVBnPdhvNR6MMsMiMwjtixqFeJZ5ZNeyHaM7xwGjVbA4OYl+wxbwt9dhUfaBKNQK0h9iO/Vcw9EYHKPuhaPz0V1/53QUdnq+bnhyyfaws9dXelZQ+NgSqxpfrZHBC4nZGGiYfVCS4R4OyPcx0WanNazgW+pvZuLK76S3LAySgxEMpmsQgGpj6sKj3cMNQoQqz6Y7RJRC0NaBKJ5sEe5JWADmxsHs5ZdZfOr5qti3i6O8HVTSX1y/d1VcFEWlnOr6sFglu47w1oHY9fFREzsGlbdQ6H7mFqpwAuG3p3WoJ/l28pq9yfiCZl96VfORClrMQ4exRr+JkHu2Ppy93u+7x71faRg1pysLGC6kI1aKUf/ZtZ/HZ7tURff/XsNBYW6asnMqhzKb2knah3ihSAPNW6l56FtaPXUvFpDQ383KZjV9ywvfQnl6sZglu47zYI9sGvGA9U9DCKi+1aoUWbW2jWVD7cOxvf/XsPItiFmB41qpULqsNC3SUCZi4zcCz93NZaPaY2LtzPL7XhS3eRyGb6Z2AHXUrJNVuCzNQ383aRVLxv5uwGF8eXconowmCUiIrIxrioHvNa/ITJy/9/encdEdYZrAH8GYUYQhsGCLDo4GhSLIioqxdYlZSK11mrTRGoIVWs0KN5I6lLbptU/akGbGpeqbdIo1jRSu6iNVVrKZjWKiKAgBDco3JalSIGhWkHmvX8Yz3UEt8owHnl+ySR4vnfm+848GfJyPOdMm3LVeVcZE+iFgg+mwqP3o7UIo4wG/Hy+FjH3+aatxxU93O+Btwl7UgT7eTzS/cafRMF+/3+a0zA/d+B/71PsQGxmiYiIVKgrT7O5W2ffbvUgG14Pw/+8eM0hpwCQfdx5j+thfh64wGaWiIiInlaebi7wdGMj+zQJ9vVAPw8ddC5OMD3jhguOXtA9sJklIiIiog5ctb1weNlEOGk0cO715F7MymaWiIiIiDrl7X7r/rfd9bXS/8WjfWUOEREREdEThM0sEREREakWm1kiIiIiUi02s0RERESkWmxmiYiIiEi12MwSERERkWqxmSUiIiIi1WIzS0RERESqxWaWiIiIiFSLzSwRERERqRabWSIiIiJSLTazRERERKRabGaJiIiISLXYzBIRERGRajk7egHdTUQAAM3NzXadp62tDdeuXUNzczNcXFzsOhfZD3NUP2aofsxQ/Zih+nV3hrf7tNt92/30uGbWYrEAAIxGo4NXQkRERET3Y7FY4Onped8ajTxMy/sUsVqt+PPPP+Hh4QGNRmO3eZqbm2E0GlFVVQW9Xm+3eci+mKP6MUP1Y4bqxwzVr7szFBFYLBYEBATAyen+Z8X2uCOzTk5OGDBgQLfNp9fr+cF9CjBH9WOG6scM1Y8Zql93ZvigI7K38QIwIiIiIlItNrNEREREpFpsZu1Ep9NhzZo10Ol0jl4KPQbmqH7MUP2YofoxQ/V7kjPscReAEREREdHTg0dmiYiIiEi12MwSERERkWqxmSUiIiIi1WIzS0RERESqxWbWTrZt2waTyYTevXsjIiICp06dcvSSeqyjR49ixowZCAgIgEajwYEDB2zGRQQffvgh/P394erqCrPZjIsXL9rUNDQ0IDY2Fnq9HgaDAQsWLEBLS4tNzblz5zBx4kT07t0bRqMRGzZssPeu9QhJSUkYN24cPDw80K9fP8yaNQtlZWU2Nf/++y8SEhLwzDPPwN3dHa+//jpqa2ttaiorKzF9+nS4ubmhX79+WLlyJW7evGlTk52djTFjxkCn0yEoKAgpKSn23r0eY8eOHRg5cqRyw/XIyEgcOXJEGWeG6pKcnAyNRoPExERlGzN88q1duxYajcbmMWzYMGVctRkKdbnU1FTRarWyc+dOOX/+vCxcuFAMBoPU1tY6emk90uHDh+X999+XH374QQDI/v37bcaTk5PF09NTDhw4IGfPnpVXX31VBg0aJNevX1dqXnrpJQkLC5OTJ0/Kb7/9JkFBQTJnzhxlvKmpSXx9fSU2NlaKi4tl79694urqKl988UV37eZTKzo6Wnbt2iXFxcVSWFgoL7/8sgQGBkpLS4tSEx8fL0ajUTIyMuT06dPy3HPPyYQJE5TxmzdvyogRI8RsNktBQYEcPnxYvL295d1331Vqrly5Im5ubvL2229LSUmJbN26VXr16iVpaWndur9Pqx9//FF++uknuXDhgpSVlcl7770nLi4uUlxcLCLMUE1OnTolJpNJRo4cKcuWLVO2M8Mn35o1a2T48OFSXV2tPP766y9lXK0Zspm1g/Hjx0tCQoLy7/b2dgkICJCkpCQHropEpEMza7Vaxc/PTz755BNlW2Njo+h0Otm7d6+IiJSUlAgAycvLU2qOHDkiGo1G/vjjDxER2b59u3h5ecmNGzeUmnfeeUeCg4PtvEc9T11dnQCQnJwcEbmVl4uLi3z77bdKTWlpqQCQEydOiMitP2icnJykpqZGqdmxY4fo9Xols1WrVsnw4cNt5oqJiZHo6Gh771KP5eXlJV9++SUzVBGLxSJDhgyR9PR0mTx5stLMMkN1WLNmjYSFhXU6puYMeZpBF2ttbUV+fj7MZrOyzcnJCWazGSdOnHDgyqgz5eXlqKmpscnL09MTERERSl4nTpyAwWDA2LFjlRqz2QwnJyfk5uYqNZMmTYJWq1VqoqOjUVZWhr///rub9qZnaGpqAgD07dsXAJCfn4+2tjabDIcNG4bAwECbDENDQ+Hr66vUREdHo7m5GefPn1dq7nyN2zX83Ha99vZ2pKam4p9//kFkZCQzVJGEhARMnz69w/vMDNXj4sWLCAgIwODBgxEbG4vKykoA6s6QzWwXq6+vR3t7u03QAODr64uamhoHrYru5XYm98urpqYG/fr1sxl3dnZG3759bWo6e40756DHZ7VakZiYiOeffx4jRowAcOv91Wq1MBgMNrV3Z/igfO5V09zcjOvXr9tjd3qcoqIiuLu7Q6fTIT4+Hvv370dISAgzVInU1FScOXMGSUlJHcaYoTpEREQgJSUFaWlp2LFjB8rLyzFx4kRYLBZVZ+hsl1clIrKDhIQEFBcX49ixY45eCv0HwcHBKCwsRFNTE7777jvMnTsXOTk5jl4WPYSqqiosW7YM6enp6N27t6OXQ//RtGnTlJ9HjhyJiIgIDBw4EPv27YOrq6sDV/Z4eGS2i3l7e6NXr14drv6rra2Fn5+fg1ZF93I7k/vl5efnh7q6OpvxmzdvoqGhwaams9e4cw56PEuXLsWhQ4eQlZWFAQMGKNv9/PzQ2tqKxsZGm/q7M3xQPveq0ev1qv4l/yTRarUICgpCeHg4kpKSEBYWhs2bNzNDFcjPz0ddXR3GjBkDZ2dnODs7IycnB1u2bIGzszN8fX2ZoQoZDAYMHToUly5dUvXnkM1sF9NqtQgPD0dGRoayzWq1IiMjA5GRkQ5cGXVm0KBB8PPzs8mrubkZubm5Sl6RkZFobGxEfn6+UpOZmQmr1YqIiAil5ujRo2hra1Nq0tPTERwcDC8vr27am6eTiGDp0qXYv38/MjMzMWjQIJvx8PBwuLi42GRYVlaGyspKmwyLiops/ihJT0+HXq9HSEiIUnPna9yu4efWfqxWK27cuMEMVSAqKgpFRUUoLCxUHmPHjkVsbKzyMzNUn5aWFly+fBn+/v7q/hza7dKyHiw1NVV0Op2kpKRISUmJLFq0SAwGg83Vf9R9LBaLFBQUSEFBgQCQjRs3SkFBgfz+++8icuvWXAaDQQ4ePCjnzp2TmTNndnprrtGjR0tubq4cO3ZMhgwZYnNrrsbGRvH19ZW4uDgpLi6W1NRUcXNz4625usDixYvF09NTsrOzbW4nc+3aNaUmPj5eAgMDJTMzU06fPi2RkZESGRmpjN++nczUqVOlsLBQ0tLSxMfHp9PbyaxcuVJKS0tl27ZtvCVQF1q9erXk5ORIeXm5nDt3TlavXi0ajUZ++eUXEWGGanTn3QxEmKEaLF++XLKzs6W8vFyOHz8uZrNZvL29pa6uTkTUmyGbWTvZunWrBAYGilarlfHjx8vJkycdvaQeKysrSwB0eMydO1dEbt2e64MPPhBfX1/R6XQSFRUlZWVlNq9x9epVmTNnjri7u4ter5f58+eLxWKxqTl79qy88MILotPppH///pKcnNxdu/hU6yw7ALJr1y6l5vr167JkyRLx8vISNzc3ee2116S6utrmdSoqKmTatGni6uoq3t7esnz5cmlra7OpycrKklGjRolWq5XBgwfbzEGP56233pKBAweKVqsVHx8fiYqKUhpZEWaoRnc3s8zwyRcTEyP+/v6i1Wqlf//+EhMTI5cuXVLG1ZqhRkTEfsd9iYiIiIjsh+fMEhEREZFqsZklIiIiItViM0tEREREqsVmloiIiIhUi80sEREREakWm1kiIiIiUi02s0RERESkWmxmiYiIiEi12MwSEfUQJpMJmzZtcvQyiIi6FJtZIiI7mDdvHmbNmgUAmDJlChITE7tt7pSUFBgMhg7b8/LysGjRom5bBxFRd3B29AKIiOjhtLa2QqvV/ufn+/j4dOFqiIieDDwyS0RkR/PmzUNOTg42b94MjUYDjUaDiooKAEBxcTGmTZsGd3d3+Pr6Ii4uDvX19cpzp0yZgqVLlyIxMRHe3t6Ijo4GAGzcuBGhoaHo06cPjEYjlixZgpaWFgBAdnY25s+fj6amJmW+tWvXAuh4mkFlZSVmzpwJd3d36PV6zJ49G7W1tcr42rVrMWrUKOzZswcmkwmenp544403YLFY7PumERE9AjazRER2tHnzZkRGRmLhwoWorq5GdXU1jEYjGhsb8eKLL2L06NE4ffo00tLSUFtbi9mzZ9s8f/fu3dBqtTh+/Dg+//xzAICTkxO2bNmC8+fPY/fu3cjMzMSqVasAABMmTMCmTZug1+uV+VasWNFhXVarFTNnzkRDQwNycnKQnp6OK1euICYmxqbu8uXLOHDgAA4dOoRDhw4hJycHycnJdnq3iIgeHU8zICKyI09PT2i1Wri5ucHPz0/Z/tlnn2H06NH4+OOPlW07d+6E0WjEhQsXMHToUADAkCFDsGHDBpvXvPP8W5PJhI8++gjx8fHYvn07tFotPD09odFobOa7W0ZGBoqKilBeXg6j0QgA+OqrrzB8+HDk5eVh3LhxAG41vSkpKfDw8AAAxMXFISMjA+vWrXu8N4aIqIvwyCwRkQOcPXsWWVlZcHd3Vx7Dhg0DcOto6G3h4eEdnvvrr78iKioK/fv3h4eHB+Li4nD16lVcu3btoecvLS2F0WhUGlkACAkJgcFgQGlpqbLNZDIpjSwA+Pv7o66u7pH2lYjInnhklojIAVpaWjBjxgysX7++w5i/v7/yc58+fWzGKioq8Morr2Dx4sVYt24d+vbti2PHjmHBggVobW2Fm5tbl67TxcXF5t8ajQZWq7VL5yAiehxsZomI7Eyr1aK9vd1m25gxY/D999/DZDLB2fnhfxXn5+fDarXi008/hZPTrf9c27dv3wPnu9uzzz6LqqoqVFVVKUdnS0pK0NjYiJCQkIdeDxGRo/E0AyIiOzOZTMjNzUVFRQXq6+thtVqRkJCAhoYGzJkzB3l5ebh8+TJ+/vlnzJ8//76NaFBQENra2rB161ZcuXIFe/bsUS4Mu3O+lpYWZGRkoL6+vtPTD8xmM0JDQxEbG4szZ87g1KlTePPNNzF58mSMHTu2y98DIiJ7YTNLRGRnK1asQK9evRASEgIfHx9UVlYiICAAx48fR3t7O6ZOnYrQ0FAkJibCYDAoR1w7ExYWho0bN2L9+vUYMWIEvv76ayQlJdnUTJgwAfHx8YiJiYGPj0+HC8iAW6cLHDx4EF5eXpg0aRLMZjMGDx6Mb775psv3n4jInjQiIo5eBBERERHRf8Ejs0RERESkWmxmiYiIiEi12MwSERERkWqxmSUiIiIi1WIzS0RERESqxWaWiIiIiFSLzSwRERERqRabWSIiIiJSLTazRERERKRabGaJiIiISLXYzBIRERGRav0f5/gbQ11791AAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: graphs/fast_rcnn_false_negative_over_iterations.png\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: graphs/lr_over_iterations.png\n",
            "Metrics at the latest iteration:\n",
            "                     Metric        Value\n",
            "0                 iteration  5000.000000\n",
            "1                   bbox/AP    30.223800\n",
            "2                 bbox/AP50    70.099381\n",
            "3                 bbox/AP75    19.400861\n",
            "4                total_loss          NaN\n",
            "5    fast_rcnn/cls_accuracy          NaN\n",
            "6  fast_rcnn/false_negative          NaN\n",
            "7                        lr          NaN\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the JSON file\n",
        "file_path = 'output/metrics.json'\n",
        "data = []\n",
        "with open(file_path, 'r') as f:\n",
        "    for line in f:\n",
        "        data.append(json.loads(line))\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Define the metrics to plot\n",
        "metrics_to_plot = [\n",
        "    'bbox/AP', 'bbox/AP50', 'bbox/AP75', 'total_loss', 'fast_rcnn/cls_accuracy',\n",
        "    'fast_rcnn/false_negative', 'lr'\n",
        "]\n",
        "\n",
        "output_dir = 'graphs/'  # Adjust this path if needed\n",
        "import os\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Plot each metric in a separate graph\n",
        "for metric in metrics_to_plot:\n",
        "    if metric in df.columns:  # Check if the metric exists in the DataFrame\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        plt.plot(df['iteration'], df[metric], label=metric)\n",
        "        plt.xlabel('Iteration')\n",
        "        plt.ylabel(metric)\n",
        "        plt.title(f'{metric} over Iterations')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "        \n",
        "        # Save the figure\n",
        "        file_name = f\"{metric.replace('/', '_')}_over_iterations.png\"\n",
        "        file_path = os.path.join(output_dir, file_name)\n",
        "        plt.savefig(file_path)\n",
        "        print(f\"Saved: {file_path}\")\n",
        "        plt.close()\n",
        "\n",
        "# Display the latest iteration as a table\n",
        "latest_metrics = df.iloc[-1][['iteration'] + metrics_to_plot]\n",
        "latest_metrics_df = pd.DataFrame(latest_metrics).reset_index()\n",
        "latest_metrics_df.columns = ['Metric', 'Value']\n",
        "\n",
        "# Print the table\n",
        "print(\"Metrics at the latest iteration:\")\n",
        "print(latest_metrics_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 61), started 1:39:41 ago. (Use '!kill 61' to kill it.)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-20a13f2d2b459ab6\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-20a13f2d2b459ab6\");\n",
              "          const url = new URL(\"http://localhost\");\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Look at training curves in tensorboard:\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kblA1IyFvWbT"
      },
      "source": [
        "We can also evaluate its performance using AP metric implemented in COCO API.\n",
        "This gives an AP of ~70. Not bad!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9tECBQCvMv3",
        "outputId": "c9f72ae9-d23c-44f7-8c52-8dd575400810"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'detectron2.evaluation'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdetectron2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m COCOEvaluator, inference_on_dataset\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdetectron2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_detection_test_loader\n\u001b[1;32m      3\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m COCOEvaluator(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweed_val\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./output\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'detectron2.evaluation'"
          ]
        }
      ],
      "source": [
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "evaluator = COCOEvaluator(\"weed_val\", output_dir=\"./output\")\n",
        "val_loader = build_detection_test_loader(cfg, \"weed_val\")\n",
        "print(inference_on_dataset(predictor.model, val_loader, evaluator))\n",
        "# another equivalent way to evaluate the model is to use `trainer.test`"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
