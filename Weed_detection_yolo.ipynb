{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weed Detection using Computer Vision\n",
    "\n",
    "This notebook focuses on developing a weed detection system using computer vision techniques. We are using the \"Weed Detection\" dataset from Kaggle, which contains images of weed species in agricultural settings.\n",
    "\n",
    "## Dataset Description\n",
    "- The dataset consists of annotated images of weed species in various growth stages.\n",
    "- Images are split into training and test sets\n",
    "- Annotations are provided in COCO format for object detection\n",
    "\n",
    "## Objective\n",
    "The goal is to build and train a model that can accurately detect and localize weed species in images. This has practical applications in precision agriculture and automated weed control systems.\n",
    "\n",
    "## Approach\n",
    "We will be using:\n",
    "- YOLO (You Only Look Once) for object detection\n",
    "- Data augmentation techniques to improve model robustness\n",
    "- TensorFlow/Keras for deep learning implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torchmetrics[detection]\n",
    "%pip install albumentations\n",
    "%pip install torchvision\n",
    "%pip install ultralytics\n",
    "%pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import albumentations as A\n",
    "import json\n",
    "import shutil\n",
    "import yaml\n",
    "import cv2\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import torchvision\n",
    "import torch\n",
    "from torchvision.models import resnet50\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /root/.cache/kagglehub/datasets/jaidalmotra/weed-detection/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"jaidalmotra/weed-detection\")\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO (You Only Look Once)\n",
    "In this section, we will:\n",
    "- Convert our COCO format annotations to YOLO format\n",
    "- Set up a YOLO model for weed detection \n",
    "- Train the model on our dataset\n",
    "- Evaluate model performance\n",
    "- Make predictions on test images\n",
    "\n",
    "YOLO is an efficient real-time object detection system that processes images in a single pass through a neural network, making it ideal for our weed detection task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for train and test data\n",
    "train_folder = os.path.join(path, 'train')\n",
    "test_folder = os.path.join(path, 'test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting COCO Format to YOLO Format\n",
    "\n",
    "In this step, we'll convert our dataset annotations from COCO format to YOLO format, which is required for training YOLO models.\n",
    "\n",
    "COCO format uses absolute pixel coordinates in the format:\n",
    "- `[x, y, width, height]` where (x,y) is the top-left corner\n",
    "- Separate JSON files containing image info and annotations\n",
    "\n",
    "YOLO format uses normalized coordinates in a simple text format:\n",
    "- `<class_id> <x_center> <y_center> <width> <height>`\n",
    "- All values are normalized between 0 and 1\n",
    "- One text file per image with the same name\n",
    "\n",
    "The conversion process will:\n",
    "1. Read the COCO JSON annotations\n",
    "2. For each annotation:\n",
    "   - Convert absolute coordinates to normalized values\n",
    "   - Convert top-left format to center point format\n",
    "   - Write in YOLO's text file format\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_yolo_format(coco_annotations, image_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Convert COCO annotations to YOLO format\n",
    "    YOLO format: <class> <x_center> <y_center> <width> <height>\n",
    "    Values are normalized between 0 and 1\n",
    "    \"\"\"\n",
    "    # Create output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Create mapping of image_id to file_name and dimensions\n",
    "    image_info = {}\n",
    "    for img in coco_annotations['images']:\n",
    "        image_info[img['id']] = {\n",
    "            'file_name': img['file_name'],\n",
    "            'width': img['width'],\n",
    "            'height': img['height']\n",
    "        }\n",
    "    \n",
    "    # Process each annotation\n",
    "    current_image_id = None\n",
    "    current_labels = []\n",
    "    \n",
    "    for ann in coco_annotations['annotations']:\n",
    "        img_id = ann['image_id']\n",
    "        img_data = image_info[img_id]\n",
    "        \n",
    "        # Get bbox coordinates\n",
    "        x, y, w, h = ann['bbox']\n",
    "        \n",
    "        # Convert to YOLO format (normalized)\n",
    "        x_center = (x + w/2) / img_data['width']\n",
    "        y_center = (y + h/2) / img_data['height']\n",
    "        width = w / img_data['width']\n",
    "        height = h / img_data['height']\n",
    "        \n",
    "        # Class ID (assuming category_id starts from 1, YOLO expects 0-based)\n",
    "        class_id = ann['category_id'] - 1\n",
    "        \n",
    "        # Create YOLO format line\n",
    "        yolo_line = f\"{class_id} {x_center} {y_center} {width} {height}\"\n",
    "        \n",
    "        # Write to file\n",
    "        label_file = os.path.splitext(img_data['file_name'])[0] + '.txt'\n",
    "        label_path = os.path.join(output_folder, label_file)\n",
    "        \n",
    "        with open(label_path, 'a') as f:\n",
    "            f.write(yolo_line + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function: visualize_yolo_dataset()\n",
    "\n",
    "This function visualizes images and their corresponding YOLO format annotations by:\n",
    "- Taking a directory path containing images and their annotation files\n",
    "- Randomly selecting a specified number of images (default 3) \n",
    "- For each selected image:\n",
    "  - Loading the image and its YOLO annotation file (.txt)\n",
    "  - Converting YOLO's normalized coordinates to pixel coordinates\n",
    "  - Drawing red bounding boxes around the annotated objects\n",
    "  - Displaying the image with annotations in a subplot\n",
    "\n",
    "Parameters:\n",
    "- data_path: Directory containing the image and annotation files\n",
    "- num_samples: Number of random images to visualize (default=3)\n",
    "\n",
    "The YOLO annotation format used is:\n",
    "class_id x_center y_center width height\n",
    "where all coordinates are normalized between 0 and 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_yolo_dataset(data_path, num_samples=3):\n",
    "    \"\"\"\n",
    "    Visualise les images et leurs annotations au format YOLO\n",
    "    \"\"\"\n",
    "    # Charger quelques images aléatoires\n",
    "    image_files = [f for f in os.listdir(data_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    selected_files = np.random.choice(image_files, min(num_samples, len(image_files)), replace=False)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5*num_samples))\n",
    "    \n",
    "    for idx, img_file in enumerate(selected_files):\n",
    "        # Charger l'image\n",
    "        img_path = os.path.join(data_path, img_file)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Charger les annotations YOLO correspondantes\n",
    "        txt_file = os.path.join(data_path, os.path.splitext(img_file)[0] + '.txt')\n",
    "        \n",
    "        plt.subplot(num_samples, 1, idx + 1)\n",
    "        plt.imshow(img)\n",
    "        \n",
    "        if os.path.exists(txt_file):\n",
    "            with open(txt_file, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                \n",
    "            img_height, img_width = img.shape[:2]\n",
    "            \n",
    "            for line in lines:\n",
    "                # Format YOLO: class x_center y_center width height\n",
    "                class_id, x_center, y_center, width, height = map(float, line.strip().split())\n",
    "                \n",
    "                # Convertir les coordonnées normalisées en pixels\n",
    "                x = int((x_center - width/2) * img_width)\n",
    "                y = int((y_center - height/2) * img_height)\n",
    "                w = int(width * img_width)\n",
    "                h = int(height * img_height)\n",
    "                \n",
    "                # Dessiner le rectangle\n",
    "                rect = patches.Rectangle(\n",
    "                    (x, y), w, h,\n",
    "                    linewidth=2,\n",
    "                    edgecolor='r',\n",
    "                    facecolor='none'\n",
    "                )\n",
    "                plt.gca().add_patch(rect)\n",
    "                \n",
    "                # Ajouter le label\n",
    "                plt.text(x, y-5, f'Weed {class_id}', color='r', fontsize=12)\n",
    "        \n",
    "        plt.title(f'Image: {img_file}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Documentation\n",
    "\n",
    "## clean_yolo_annotations(data_path)\n",
    "\n",
    "This function cleans YOLO annotation files by removing duplicate bounding box annotations.\n",
    "\n",
    "### Parameters:\n",
    "- `data_path`: Path to the directory containing YOLO annotation files (.txt)\n",
    "\n",
    "### Process:\n",
    "1. Finds all .txt annotation files in the specified directory\n",
    "2. For each file:\n",
    "   - Reads all annotation lines\n",
    "   - Converts annotations to tuples for deduplication\n",
    "   - Uses a set to identify and remove duplicates\n",
    "   - Rewrites the file with only unique annotations\n",
    "\n",
    "### Returns:\n",
    "- Number of duplicate annotations removed\n",
    "- Number of files that were cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_yolo_annotations(data_path):\n",
    "    \"\"\"\n",
    "    Nettoie les annotations YOLO en supprimant les doublons\n",
    "    \"\"\"\n",
    "    txt_files = [f for f in os.listdir(data_path) if f.endswith('.txt')]\n",
    "    duplicates_found = 0\n",
    "    files_cleaned = 0\n",
    "    \n",
    "    for txt_file in txt_files:\n",
    "        file_path = os.path.join(data_path, txt_file)\n",
    "        \n",
    "        # Lire les annotations\n",
    "        with open(file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        # Convertir chaque ligne en tuple pour pouvoir utiliser set()\n",
    "        unique_annotations = set()\n",
    "        for line in lines:\n",
    "            # Convertir la ligne en tuple de floats\n",
    "            values = tuple(map(float, line.strip().split()))\n",
    "            unique_annotations.add(values)\n",
    "        \n",
    "        # Si on a trouvé des doublons\n",
    "        if len(unique_annotations) < len(lines):\n",
    "            duplicates_found += len(lines) - len(unique_annotations)\n",
    "            files_cleaned += 1\n",
    "            \n",
    "            # Réécrire le fichier sans les doublons\n",
    "            with open(file_path, 'w') as f:\n",
    "                for annotation in unique_annotations:\n",
    "                    f.write(' '.join(map(str, annotation)) + '\\n')\n",
    "    \n",
    "    print(f\"Nettoyage terminé:\")\n",
    "    print(f\"- {duplicates_found} annotations dupliquées supprimées\")\n",
    "    print(f\"- {files_cleaned} fichiers nettoyés\")\n",
    "    \n",
    "    return duplicates_found, files_cleaned\n",
    "\n",
    "\n",
    "# Vérifier les annotations après nettoyage\n",
    "def verify_annotations_after_cleaning(data_path):\n",
    "    \"\"\"\n",
    "    Vérifie qu'il n'y a plus de doublons dans les annotations\n",
    "    \"\"\"\n",
    "    txt_files = [f for f in os.listdir(data_path) if f.endswith('.txt')]\n",
    "    \n",
    "    print(f\"\\nVérification des annotations dans {data_path}:\")\n",
    "    print(f\"Nombre total de fichiers d'annotation: {len(txt_files)}\")\n",
    "    \n",
    "    # Vérifier quelques exemples\n",
    "    sample_files = np.random.choice(txt_files, min(3, len(txt_files)), replace=False)\n",
    "    print(\"\\nExemples d'annotations nettoyées:\")\n",
    "    \n",
    "    for txt_file in sample_files:\n",
    "        with open(os.path.join(data_path, txt_file), 'r') as f:\n",
    "            content = f.readlines()\n",
    "        print(f\"\\n{txt_file} ({len(content)} annotations):\")\n",
    "        for line in content:\n",
    "            print(line.strip())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COCO Format Analysis\n",
    "The dataset uses the COCO (Common Objects in Context) format, which is a standard for object detection tasks. We analyze:\n",
    "- Total number of images in the dataset\n",
    "- Number of object annotations\n",
    "- Number of weed categories\n",
    "- Distribution of annotations per image\n",
    "- Basic image metadata (dimensions, file names)\n",
    "\n",
    "This analysis helps us understand:\n",
    "- The scale of our dataset\n",
    "- The complexity of our detection task\n",
    "- Potential class imbalance issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Information:\n",
      "Number of images: 1661\n",
      "Number of annotations: 4199\n",
      "Number of categories: 2\n",
      "\n",
      "Categories:\n",
      "ID: 0, Name: grass-weeds\n",
      "ID: 1, Name: 0 ridderzuring\n",
      "\n",
      "Annotation Statistics:\n",
      "Average annotations per image: 2.54\n",
      "Max annotations in a single image: 19\n",
      "Min annotations in a single image: 1\n",
      "\n",
      "Sample Image Information:\n",
      "ID: 0\n",
      "File name: Rumex-obtusifolius-L_1703_jpg.rf.00ce9f9ea755f686d01d88767ff162ee.jpg\n",
      "Width: 640, Height: 640\n",
      "\n",
      "ID: 1\n",
      "File name: ridderzuring_0981_jpg.rf.00938fc387bb7acbdd49a94a987fa58c.jpg\n",
      "Width: 640, Height: 640\n",
      "\n",
      "ID: 2\n",
      "File name: Rumex-obtusifolius-L_0194_jpg.rf.00065c173713e91bdc5e600a761fa880.jpg\n",
      "Width: 640, Height: 640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load and explore the COCO annotations\n",
    "import json\n",
    "\n",
    "# Load the COCO annotation file\n",
    "with open(path + '/train/_annotations.coco.json', 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Print basic dataset information\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"Number of images: {len(coco_data['images'])}\")\n",
    "print(f\"Number of annotations: {len(coco_data['annotations'])}\")\n",
    "print(f\"Number of categories: {len(coco_data['categories'])}\\n\")\n",
    "\n",
    "# Print category information\n",
    "print(\"Categories:\")\n",
    "for category in coco_data['categories']:\n",
    "    print(f\"ID: {category['id']}, Name: {category['name']}\")\n",
    "\n",
    "# Get some statistics about annotations per image\n",
    "annotations_per_image = {}\n",
    "for ann in coco_data['annotations']:\n",
    "    img_id = ann['image_id']\n",
    "    if img_id not in annotations_per_image:\n",
    "        annotations_per_image[img_id] = 0\n",
    "    annotations_per_image[img_id] += 1\n",
    "\n",
    "# Calculate and print annotation statistics\n",
    "num_annotations = list(annotations_per_image.values())\n",
    "print(f\"\\nAnnotation Statistics:\")\n",
    "print(f\"Average annotations per image: {sum(num_annotations)/len(num_annotations):.2f}\")\n",
    "print(f\"Max annotations in a single image: {max(num_annotations)}\")\n",
    "print(f\"Min annotations in a single image: {min(num_annotations)}\")\n",
    "\n",
    "# Print sample image information\n",
    "print(\"\\nSample Image Information:\")\n",
    "for img in coco_data['images'][:3]:  # Show first 3 images\n",
    "    print(f\"ID: {img['id']}\")\n",
    "    print(f\"File name: {img['file_name']}\")\n",
    "    print(f\"Width: {img['width']}, Height: {img['height']}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the COCO annotation file\n",
    "with open(path + '/train/_annotations.coco.json', 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Convert COCO annotations to YOLO format\n",
    "convert_to_yolo_format(coco_data, path + '/train', 'yolo_dataset/train')\n",
    "\n",
    "train_path = 'yolo_dataset/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert COCO annotations to YOLO format for test set\n",
    "with open(path + '/test/_annotations.coco.json', 'r') as f:\n",
    "    test_coco_data = json.load(f)\n",
    "\n",
    "# Convert test annotations to YOLO format \n",
    "convert_to_yolo_format(test_coco_data, path + '/test', 'yolo_dataset/test')\n",
    "\n",
    "test_path = 'yolo_dataset/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset train:\n",
      "- Nombre de fichiers d'annotations: 1655\n",
      "- Nombre de paires image-annotation: 1655\n",
      "\n",
      "Dataset test:\n",
      "- Nombre de fichiers d'annotations: 244\n",
      "- Nombre de paires image-annotation: 244\n"
     ]
    }
   ],
   "source": [
    "# Créer les dossiers YOLO s'ils n'existent pas\n",
    "for dataset in ['train', 'test']:\n",
    "    os.makedirs(f'yolo_dataset/{dataset}', exist_ok=True)\n",
    "\n",
    "# Copier les images et créer les annotations pour chaque dataset\n",
    "for dataset in ['train', 'test']:\n",
    "    source_folder = os.path.join(path, dataset)\n",
    "    target_folder = f'yolo_dataset/{dataset}'\n",
    "    \n",
    "    # Copier toutes les images\n",
    "    for img_file in os.listdir(source_folder):\n",
    "        if img_file.endswith(('.jpg', '.jpeg', '.png')):\n",
    "            src_path = os.path.join(source_folder, img_file)\n",
    "            dst_path = os.path.join(target_folder, img_file)\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "\n",
    "# Maintenant, on peut exécuter le code original pour data.yaml et la création des fichiers txt\n",
    "data_yaml = {\n",
    "    'train': train_path,\n",
    "    'val': test_path,\n",
    "    'nc': 1,\n",
    "    'names': ['weed']\n",
    "}\n",
    "\n",
    "with open('yolo_dataset/data.yaml', 'w') as f:\n",
    "    yaml.dump(data_yaml, f)\n",
    "\n",
    "# Pour chaque dataset (train et test)\n",
    "for dataset in ['train', 'test']:\n",
    "    img_path = f'yolo_dataset/{dataset}'\n",
    "    \n",
    "    # Lister tous les fichiers d'annotations (.txt)\n",
    "    txt_files = [f for f in os.listdir(img_path) if f.endswith('.txt')]\n",
    "    \n",
    "    # Créer les chemins correspondants pour les images\n",
    "    img_paths = []\n",
    "    label_paths = []\n",
    "    \n",
    "    for txt_file in txt_files:\n",
    "        base_name = os.path.splitext(txt_file)[0]\n",
    "        # Vérifier si l'image correspondante existe\n",
    "        for ext in ['.jpg', '.jpeg', '.png']:\n",
    "            img_file = base_name + ext\n",
    "            if os.path.exists(os.path.join(img_path, img_file)):\n",
    "                img_paths.append(os.path.join(img_path, img_file))\n",
    "                label_paths.append(os.path.join(img_path, txt_file))\n",
    "                break\n",
    "    \n",
    "    # Écrire les chemins dans les fichiers\n",
    "    with open(f'yolo_dataset/{dataset}_images.txt', 'w') as f:\n",
    "        f.write('\\n'.join(img_paths))\n",
    "    \n",
    "    with open(f'yolo_dataset/{dataset}_labels.txt', 'w') as f:\n",
    "        f.write('\\n'.join(label_paths))\n",
    "    \n",
    "    print(f\"\\nDataset {dataset}:\")\n",
    "    print(f\"- Nombre de fichiers d'annotations: {len(txt_files)}\")\n",
    "    print(f\"- Nombre de paires image-annotation: {len(img_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_yolo_dataset('yolo_dataset/train')\n",
    "visualize_yolo_dataset('yolo_dataset/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import albumentations as A\n",
    "import shutil\n",
    "\n",
    "# Define augmentation pipeline\n",
    "transform = A.Compose([\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.OneOf([\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "        A.RandomGamma(gamma_limit=(80, 120), p=0.5),\n",
    "    ], p=0.5),\n",
    "    A.OneOf([\n",
    "        A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
    "        A.GaussianBlur(blur_limit=(3, 7), p=0.5),\n",
    "    ], p=0.5),\n",
    "    A.OneOf([\n",
    "        A.RandomScale(scale_limit=0.2, p=0.5),\n",
    "        A.Resize(height=640, width=640, p=0.5),\n",
    "    ], p=0.5),\n",
    "], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
    "\n",
    "# Create augmented training directory\n",
    "augmented_train_folder = os.path.join('yolo_dataset/train_with_aug')\n",
    "os.makedirs(augmented_train_folder, exist_ok=True)\n",
    "\n",
    "# First, copy all original files to the new directory\n",
    "original_train_folder = os.path.join('yolo_dataset/train')\n",
    "for file in os.listdir(original_train_folder):\n",
    "    src = os.path.join(original_train_folder, file)\n",
    "    dst = os.path.join(augmented_train_folder, file)\n",
    "    shutil.copy2(src, dst)\n",
    "\n",
    "# Get list of training images\n",
    "image_files = [f for f in os.listdir(original_train_folder) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "num_augmentations = 3  # Number of augmented versions to create per image\n",
    "\n",
    "# Perform augmentation\n",
    "for img_file in image_files:\n",
    "    # Load image\n",
    "    img_path = os.path.join(original_train_folder, img_file)\n",
    "    image = cv2.imread(img_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Load corresponding label file\n",
    "    label_file = os.path.splitext(img_file)[0] + '.txt'\n",
    "    label_path = os.path.join(original_train_folder, label_file)\n",
    "    \n",
    "    if os.path.exists(label_path):\n",
    "        # Read boxes and class labels\n",
    "        boxes = []\n",
    "        class_labels = []\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                class_id, x_center, y_center, width, height = map(float, line.strip().split())\n",
    "                boxes.append([x_center, y_center, width, height])\n",
    "                class_labels.append(class_id)\n",
    "        \n",
    "        # Create augmented versions\n",
    "        for i in range(num_augmentations):\n",
    "            # Apply augmentation\n",
    "            transformed = transform(image=image, bboxes=boxes, class_labels=class_labels)\n",
    "            aug_image = transformed['image']\n",
    "            aug_boxes = transformed['bboxes']\n",
    "            \n",
    "            # Save augmented image\n",
    "            aug_img_file = f\"{os.path.splitext(img_file)[0]}_aug_{i}{os.path.splitext(img_file)[1]}\"\n",
    "            aug_img_path = os.path.join(augmented_train_folder, aug_img_file)\n",
    "            cv2.imwrite(aug_img_path, cv2.cvtColor(aug_image, cv2.COLOR_RGB2BGR))\n",
    "            \n",
    "            # Save augmented labels\n",
    "            aug_label_file = f\"{os.path.splitext(img_file)[0]}_aug_{i}.txt\"\n",
    "            aug_label_path = os.path.join(augmented_train_folder, aug_label_file)\n",
    "            \n",
    "            with open(aug_label_path, 'w') as f:\n",
    "                for box, class_id in zip(aug_boxes, class_labels):\n",
    "                    f.write(f\"{int(class_id)} {' '.join(map(str, box))}\\n\")\n",
    "\n",
    "# Print statistics\n",
    "original_images = len(image_files)\n",
    "total_images = len([f for f in os.listdir(augmented_train_folder) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "augmented_images = total_images - original_images\n",
    "\n",
    "print(f\"Original training images: {original_images}\")\n",
    "print(f\"Augmented images created: {augmented_images}\")\n",
    "print(f\"Total training images available: {total_images}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenir le chemin absolu du projet\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Créer data.yaml avec les chemins absolus, incluant les données augmentées\n",
    "data_yaml = {\n",
    "    'train': os.path.join(current_dir, 'yolo_dataset/train_with_aug'), # new combined directory\n",
    "\n",
    "        #os.path.join(current_dir, 'yolo_dataset/train'),          # données originales\n",
    "    \n",
    "    'val': os.path.join(current_dir, 'yolo_dataset/test'),        # chemin absolu vers test\n",
    "    'nc': 1,  # nombre de classes\n",
    "    'names': ['weed']  # nom des classes\n",
    "}\n",
    "\n",
    "# Écrire data.yaml\n",
    "with open('yolo_dataset/data.yaml', 'w') as f:\n",
    "    yaml.dump(data_yaml, f)\n",
    "\n",
    "# Vérifier le contenu du fichier\n",
    "with open('yolo_dataset/data.yaml', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier le contenu du fichier data.yaml\n",
    "with open('yolo_dataset/data.yaml', 'r') as f:\n",
    "    content = yaml.safe_load(f)\n",
    "    print(\"Configuration data.yaml:\")\n",
    "    print(f\"- Train path: {content['train']}\")\n",
    "    print(f\"- Val path: {content['val']}\")\n",
    "    print(f\"- Number of classes: {content['nc']}\")\n",
    "    print(f\"- Class names: {content['names']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Section\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA availability and GPU info\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "print(f\"Device name: {torch.cuda.get_device_name()}\")\n",
    "print(f\"Device memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Clear CUDA cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle YOLOv8\n",
    "model = YOLO(\"yolo11m.pt\")  \n",
    "\n",
    "# Entraîner le modèle\n",
    "results = model.train(\n",
    "    data='yolo_dataset/data.yaml',  # Chemin vers le fichier data.yaml\n",
    "    epochs=20,                     # Nombre d'époques\n",
    "    imgsz=640,                     # Taille des images\n",
    "    batch=16,                      # Taille du batch\n",
    "    name='weed_detection',\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the parent directory of the current working directory\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    "# Navigate to runs/detect/weed_detection8 folder\n",
    "target_dir = os.path.join('runs', 'detect', 'weed_detection')\n",
    "\n",
    "print(f\"Target directory: {target_dir}\")\n",
    "\n",
    "# List contents of weed_detection8 directory if it exists\n",
    "if os.path.exists(target_dir):\n",
    "    print(\"\\nContents of weed_detection8 directory:\")\n",
    "    for item in os.listdir(target_dir):\n",
    "        print(f\"- {item}\")\n",
    "else:\n",
    "    print(\"\\nDirectory not found. Please check if the path is correct.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Load and plot training results\n",
    "results = pd.read_csv('runs/detect/weed_detection/results.csv')\n",
    "\n",
    "# Plot training metrics\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Training Losses\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(results['epoch'], results['train/box_loss'], label='Box Loss')\n",
    "plt.plot(results['epoch'], results['train/cls_loss'], label='Class Loss')\n",
    "plt.plot(results['epoch'], results['train/dfl_loss'], label='DFL Loss')\n",
    "plt.title('Training Losses')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Precision, Recall, mAP\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(results['epoch'], results['metrics/precision(B)'], label='Precision')\n",
    "plt.plot(results['epoch'], results['metrics/recall(B)'], label='Recall')\n",
    "plt.plot(results['epoch'], results['metrics/mAP50(B)'], label='mAP50')\n",
    "plt.plot(results['epoch'], results['metrics/mAP50-95(B)'], label='mAP50-95')\n",
    "plt.title('Model Performance Metrics')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "\n",
    "# Validation Losses\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(results['epoch'], results['val/box_loss'], label='Val Box Loss')\n",
    "plt.plot(results['epoch'], results['val/cls_loss'], label='Val Class Loss')\n",
    "plt.plot(results['epoch'], results['val/dfl_loss'], label='Val DFL Loss')\n",
    "plt.title('Validation Losses')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Learning Rate\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(results['epoch'], results['lr/pg0'], label='Learning Rate')\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics summary\n",
    "print(\"\\nFinal Model Performance:\")\n",
    "print(f\"Precision: {results['metrics/precision(B)'].iloc[-1]:.3f}\")\n",
    "print(f\"Recall: {results['metrics/recall(B)'].iloc[-1]:.3f}\")\n",
    "print(f\"mAP50: {results['metrics/mAP50(B)'].iloc[-1]:.3f}\")\n",
    "print(f\"mAP50-95: {results['metrics/mAP50-95(B)'].iloc[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Strengths**\n",
    "\n",
    "1. **Precision (0.759):**\n",
    "   - The model has a relatively high precision, indicating that when it predicts a weed, it is correct about 75.9% of the time. This is beneficial in applications where false positives (incorrectly identifying non-weeds as weeds) need to be minimized, such as in automated weed control systems where unnecessary actions could be costly or damaging.\n",
    "\n",
    "2. **mAP50 (0.736):**\n",
    "   - The mean Average Precision at an IoU threshold of 50% is 73.6%, which suggests that the model is quite effective at detecting and localizing weeds with a reasonable degree of overlap between predicted and actual bounding boxes. This is a positive indicator of the model's ability to identify most weeds accurately.\n",
    "\n",
    "### **Weaknesses**\n",
    "\n",
    "1. **Recall (0.656):**\n",
    "   - The recall is lower than precision, at 65.6%, indicating that the model misses a significant number of actual weed instances. This suggests that the model could benefit from improvements in detecting all instances of weeds, especially smaller or less distinct ones.\n",
    "\n",
    "2. **mAP50-95 (0.363):**\n",
    "   - The mean Average Precision across a range of IoU thresholds (50% to 95%) is quite low at 36.3%. This indicates that while the model can detect weeds, the precision of the bounding boxes is not as high, especially under stricter IoU conditions. This suggests that the model struggles with accurately placing bounding boxes around weeds, which could be due to variability in weed shapes, sizes, or occlusions in the dataset.\n",
    "\n",
    "### **Overall Performance Summary**\n",
    "\n",
    "The model demonstrates a good balance between precision and recall, with a stronger emphasis on precision. However, the lower recall and mAP50-95 suggest that there is room for improvement in detecting all weed instances and refining the accuracy of bounding box placements. Potential areas for improvement could include:\n",
    "\n",
    "- **Data Augmentation:** Enhance the training dataset with more diverse augmentations to help the model generalize better to unseen data.\n",
    "- **Model Architecture:** Experiment with more complex architectures or fine-tuning pre-trained models to improve detection capabilities.\n",
    "- **Hyperparameter Tuning:** Adjust training parameters such as learning rate, batch size, and number of epochs to optimize model performance.\n",
    "- **Additional Data:** Incorporate more training data, especially for underrepresented weed types or challenging conditions, to improve recall and bounding box precision. \n",
    "\n",
    "Overall, while the model shows promise, particularly in precision, further refinements could enhance its ability to detect and accurately localize all weed instances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Training Results and Model Performance Metrics\n",
    "We will now plot various visualizations generated during model training, including:\n",
    "- Confusion matrices (normalized and raw) to evaluate classification performance\n",
    "- Validation batch predictions and ground truth labels\n",
    "- Training batch samples showing model's learning progress\n",
    "- Performance curves (P, R, F1, PR curves) to assess model metrics\n",
    "- Label statistics and correlations\n",
    "These plots help us analyze the model's behavior and validate its performance on our weed detection task.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "\n",
    "# List of image files to plot (excluding non-image files)\n",
    "image_files = [\n",
    "    'confusion_matrix.png',\n",
    "    'val_batch1_labels.jpg', \n",
    "    'results.png',\n",
    "    'P_curve.png',\n",
    "    'val_batch2_pred.jpg',\n",
    "    'labels_correlogram.jpg', \n",
    "    'train_batch4161.jpg',\n",
    "    'train_batch4160.jpg',\n",
    "    'train_batch4162.jpg',\n",
    "    'val_batch0_labels.jpg',\n",
    "    'F1_curve.png',\n",
    "    'train_batch0.jpg',\n",
    "    'train_batch1.jpg',\n",
    "    'confusion_matrix_normalized.png',\n",
    "    'train_batch2.jpg',\n",
    "    'val_batch1_pred.jpg',\n",
    "    'val_batch0_pred.jpg',\n",
    "    'R_curve.png',\n",
    "    'labels.jpg',\n",
    "    'val_batch2_labels.jpg',\n",
    "    'PR_curve.png'\n",
    "]\n",
    "\n",
    "# Create a figure with subplots\n",
    "n_images = len(image_files)\n",
    "n_cols = 3\n",
    "n_rows = (n_images + n_cols - 1) // n_cols\n",
    "\n",
    "plt.figure(figsize=(20, 5*n_rows))\n",
    "\n",
    "# Plot each image\n",
    "for i, img_file in enumerate(image_files):\n",
    "    img_path = os.path.join(target_dir, img_file)\n",
    "    if os.path.exists(img_path):\n",
    "        plt.subplot(n_rows, n_cols, i+1)\n",
    "        img = mpimg.imread(img_path)\n",
    "        plt.imshow(img)\n",
    "        plt.title(img_file)\n",
    "        plt.axis('off')\n",
    "    else:\n",
    "        print(f\"File not found: {img_path}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
